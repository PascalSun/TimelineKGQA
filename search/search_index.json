{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TimelineKGQA","text":"<p>A universal temporal question-answering pair generator for any temporal knowledge graph, revealing the landscape of Temporal Knowledge Graph Question Answering beyond the Great Dividing Range of Large Language Models.</p> <p>To download we generated two datasets based on ICEWS Actor and CronQuestions KG, please visit the following link: TimelineKGQA Datasets</p> <ul> <li>Motivation</li> <li>Timelines</li> <li>How human brain do the temporal question answering?<ul> <li>Information Indexing</li> <li>Information Retrieval</li> </ul> </li> <li>Temporal Questions Categorisation<ul> <li>Simple: Timeline and One Event Involved</li> <li>Medium: Timeline and Two Events Involved</li> <li>Complex: Timeline and Multiple Events Involved</li> <li>Other perspectives</li> </ul> </li> <li>TimelineKGQA Generator</li> <li>Temporal Question Answering Solutions<ul> <li>RAG</li> <li>TKGQA Embedding</li> <li>Text2SQL</li> <li>Finetuning</li> <li>Evaluation Metrics</li> <li>Evaluation Results<ul> <li>Systematic Comparison between RAG and TKGQA Embedding</li> <li>Hits@1 for Text2SQL</li> <li>Finetuning accuracy</li> </ul> </li> </ul> </li> <li>Development Setup<ul> <li>Install the package</li> <li>Folder Structure</li> </ul> </li> </ul>"},{"location":"#motivation","title":"Motivation","text":"<p>Since the release of ChatGPT in late 2022, one of the most successful applications of large language models (LLMs), the entire field of Question Answering (QA) research has undergone a significant transformation. Researchers in the QA field now face a crucial question:</p> <p>What unique value does your QA research offer when compared to LLMs?</p> <p>The underlying challenge is:</p> <p>If your research cannot surpass or effectively leverage LLMs, what is its purpose?</p> <p>These same questions are also pressing the Knowledge Graph QA research community.</p> <p>Knowledge graphs provide a simple, yet powerful and natural format to organize complex information. Performing QA over knowledge graphs is a natural extension of their use, especially when you want to fully exploit their potential. Temporal question answering over knowledge graphs allows us to retrieve information based on temporal constraints, enabling historical analysis, causal analysis, and making predictions\u2014an essential aspect of AI research.</p> <p>So we are wondering:</p> <p>What's the landscape of Temporal Knowledge Graph Question Answering beyond the Great Dividing Range of Large Language Models after 2022?</p> <p>The literature seems have not provided a clear answer to this question.</p>"},{"location":"#timelines","title":"Timelines","text":"<p>We will begin with question answering datasets, as they are fundamental to any progress in this field. Without datasets, we can't do anything. They are our climbing rope, guiding us to the other side of the Great Dividing Range.</p> <p>Current available datasets for the Temporal Knowledge Graph Question Answering are limited. For example, the most latest and popular TKGQA dataset: CronQuestions, containing limited types of questions, temporal relations, temporal granularity is only to year level.</p> <p>Our real world temporal questions is way more comphrehensive than this.</p> <p>We all know that we are living on top of the timeline, and it only goes forward, no way looking back. The questions we are asking are all related to the timeline, which is totally underesimated in current TKGQA research.</p> <p>If we view all the temporal questions from the timeline perspective, we have this following types of timelines:</p> <ul> <li>Straight Homogenous(Objective) Timeline:<ul> <li>Exact date when it happens, for example, [2023-05-01 10:00:00, 2023-05-01 10:30:00]</li> <li>This is normally asking question about the facts, and upon the facts, we can do the analysis.</li> <li>For example, crime analysis, historical analysis, etc.</li> <li>Under this timeline, human will focus more on Temporal Logic</li> </ul> </li> <li>Cycle Homogenous(Objective) Timeline:<ul> <li>Monday, First day of Month, Spring, 21st Century, etc.</li> <li>This is normally asking question about the patterns.</li> <li>Under this timeline, human will focus more on Temporal Pattern</li> </ul> </li> <li>Straight Homogenous(Subjective) Timeline:<ul> <li>If you sleep during night, it will be fast for you in the 8 hours, however, if someone is working overnight,   time will be slow for him.</li> <li>This is normally asking question about the perception of time.</li> <li>How is your recent life goes?</li> <li>Depending on the person, the perception of the meaning for the \"recent\" will be different.</li> <li>Under this timeline, human will focus more on Temporal Modifier</li> </ul> </li> <li>Cycle Heterogeneous(Subjective) Timeline:<ul> <li>History has its trend, however, it takes thousands years get the whole world into industrialization.</li> <li>And then it only takes 100 years to get the whole world into information age.</li> <li>So the spiaral speed of the timeline is not homogenous.</li> <li>Under this timeline, human will focus more on Temporal Modifier also, but more trying to understand the   development of human society, universe, etc.</li> </ul> </li> </ul> <p>We can not handle them all in a one go, and current TKGQA research is in front of the door of the Straight Homogenous( Objective) Timeline.</p> <p>We will try to advance the research in this area first, and then try to extend to the other areas.</p>"},{"location":"#how-human-brain-do-the-temporal-question-answering","title":"How human brain do the temporal question answering?","text":""},{"location":"#information-indexing","title":"Information Indexing","text":"<p>When we see something, for example, an accident happen near our home in today morning. We need to first index this event into our brain. As we live in a three dimension space together with a time dimension, when we want to store this in our memory, (we will treat our memory as a N dimension space)</p> <ol> <li>Index the spatial dimensions: is this close to my home or close to one of the point of interest in my mind</li> <li>Index the temporal dimension: Temporal have several aspects<ul> <li>Treat temporal as Straight Homogenous(Objective) Timeline:<ul> <li>Exact date when it happens, for example, [2023-05-01 10:00:00, 2023-05-01 10:30:00]</li> </ul> </li> <li>Treat temporal as Cycle Homogenous(Objective) Timeline:<ul> <li>Monday, First day of Month, Spring, 21st Century, etc.</li> <li>(You can aslo cycle the timeline based on your own requirement)</li> </ul> </li> <li>Treat temporal as Straight Homogenous(Subjective) Timeline:<ul> <li>If you sleep during night, it will be fast for you in the 8 hours, however, if someone is working overnight,   time will be slow for him.</li> </ul> </li> <li>Treat temporal as Cycle Heterogeneous(Subjective) Timeline:<ul> <li>Life has different turning points for everyone, until they reach the end of their life.</li> </ul> </li> </ul> </li> <li>Then index the information part: What happen, who is involved, what is the impact, etc.</li> </ol> <p>So in summary, we can say that in our mind, if we treat the event as embedding in our human mind:</p> <ul> <li>part of the embedding will represent the temporal dimension information,</li> <li>part of the embedding will represent the spatial dimension information,</li> <li>the rest of the embedding will represent the general information part.</li> </ul> <p>This will help us to retrieve the information when we need it.</p>"},{"location":"#information-retrieval","title":"Information Retrieval","text":"<p>So when we try to retrieval the information, espeically the temporal part of the information. Normally we have several types:</p> <ul> <li>Timeline Retrieval:<ul> <li>When Bush starts his term as president of US?<ul> <li>First: General Information Retrieval  =&gt; [(Bush, start, president of US), (Bush, term, president of US)]</li> <li>Second: Timeline Retrieval =&gt; [(Bush, start, president of US, 2000, 2000),   (Bush, term, president of US, 2000, 2008)]</li> <li>Third: Answer the question based on the timeline information</li> </ul> </li> </ul> </li> <li>Temporal Constrained Retrieval:<ul> <li>In 2009, who is the president of US?<ul> <li>First: General Information Retrieval  =&gt; [(Bush, president of US),   (Obama, president of US), (Trump, president of US)]</li> <li>Second: Temporal Constraint Retrieval =&gt; [(Obama, president of US, 2009, 2016)]</li> <li>Third: Answer the question based on the temporal constraint information</li> </ul> </li> </ul> </li> </ul> <p>Three key things here:</p> <ul> <li>General Information Retrieval: Retrieve the general information from the knowledge graph based on the question</li> <li>Temporal Constrained Retrieval: Filter on general information retrieval, apply the temporal constraint</li> <li>Timeline Retrieval: Based on general information retrieval, recover the timeline information</li> </ul> <p>Extend from this, it is retrieve the information for one fact, or you can name it event/truth, etc. If we have multiple facts, or events, or truths, etc, after the retrieval, we need to comparison: set operation, ranking, semantic extraction, etc.</p> <p>And whether the question is complex or not is depending on how much information our brain need to process, and the different capabilities of the brain needed to process the information.</p>"},{"location":"#temporal-questions-categorisation","title":"Temporal Questions Categorisation","text":"<p>So when we try to classify the temporal questions, especially from the difficulty perspective, we classify the level of difficulty based on how many events involved in the question.</p> <ul> <li>Simple: Timeline and One Event Involved</li> <li>Medium: Timeline and Two Events Involved</li> <li>Complex: Timeline and Multiple Events Involved</li> </ul>"},{"location":"#simple-timeline-and-one-event-involved","title":"Simple: Timeline and One Event Involved","text":"<ul> <li>Timeline Retrieval:<ul> <li>When Bush starts his term as president of US?<ul> <li>General Information Retrieval =&gt; Timeline Recovery =&gt; Answer the question</li> <li>Question Focus can be: Timestamp Start, Timestamp End, Duration, Timestamp Start and End</li> </ul> </li> </ul> </li> <li>Temporal Constrained Retrieval:<ul> <li>In 2009, who is the president of US?<ul> <li>General Information Retrieval =&gt; Temporal Constraint Retrieval =&gt; Answer the question</li> <li>Question Focus can be: Subject, Object, Predicate. Can be more complex if we want mask out more elements</li> </ul> </li> </ul> </li> </ul>"},{"location":"#medium-timeline-and-two-events-involved","title":"Medium: Timeline and Two Events Involved","text":"<ul> <li>Timeline Retrieval + Timeline Retrieval:<ul> <li>Is Bush president of US when 911 happen?<ul> <li>(General Information Retrieval =&gt; Timeline Recovery) And (General Information Retrieval =&gt; Timeline   Recovery) =&gt; Timeline Operation =&gt; Answer the question</li> <li>Question Focus can be:<ul> <li>A new Time Range</li> <li>A temporal relation (Before, After, During, etc.)</li> <li>A list of Time Range (Ranking)</li> <li>or Comparison of Duration</li> </ul> </li> <li>Key ability here is: Timeline Operation</li> </ul> </li> </ul> </li> <li>Timeline Retrieval + Temporal Constrained Retrieval:<ul> <li>When Bush is president of US, who is the president of China?<ul> <li>(General Information Retrieval =&gt; Timeline Retrieval) =&gt; Temporal Semantic Operation =&gt; Temporal   Constraint Retrieval =&gt; Answer the question</li> <li>This is same as above, Question Focus can be: Subject, Object</li> <li>Key ability here is: Temporal Semantic Operation</li> </ul> </li> </ul> </li> </ul>"},{"location":"#complex-timeline-and-multiple-events-involved","title":"Complex: Timeline and Multiple Events Involved","text":"<p>In general, question focus (answer type) will only be two types when we extend from Medium Level</p> <ul> <li>Timeline Operation</li> <li>(Subject, Predicate, Object)</li> </ul> <p>So if we say Complex is 3 or n events and Timeline.</p> <ul> <li>Timeline Retrieval * n</li> <li>Timeline Retrieval * (n -1) =&gt; Semantic Operation * (n - 1)? =&gt; Temporal Constrainted Retrieval</li> </ul>"},{"location":"#other-perspectives","title":"Other perspectives","text":"<p>And based on the Answer Type, we can classify them into:</p> <ul> <li>Factual</li> <li>Temporal</li> </ul> <p>Based on the Temporal Relations in the question, we can classify them into:</p> <ul> <li>Set Operation</li> <li>Allen Temporal Relations</li> <li>Ranking</li> <li>Duration</li> </ul> <p>Based on the Temporal Capabilities, we can classify them into:</p> <ul> <li>Temporal Constrained Retrieval: Filter on general information retrieval, apply the temporal constraint</li> <li>Timeline Retrieval: Based on general information retrieval, recover the timeline information</li> <li>Timeline Operation: From numeric to semantic</li> <li>Temporal Semantic Operation: From Semantic to Numeric</li> </ul> <p>To be able to answer the temporal question, we need to have the following key abilities:</p> <ul> <li>General Information Retrieval: Retrieve the general information from the knowledge graph based on the question,   you can call this semantic parsing, or semantic retrieval</li> </ul>"},{"location":"#timelinekgqa-generator","title":"TimelineKGQA Generator","text":"<p>With the above understanding, it will not be hard to programmatically generate the temporal question answering pairs for any temporal knowledge graph, as shown in the following figure:</p> <p></p> <p>And then we can follow the following steps to generate the question answering pairs:</p> <ul> <li>Unify the temporal knowledge graph into the above format</li> <li>Sampling the facts/events from the knowledge graph</li> <li>Generate the question answer pairs based on the facts/events</li> <li>Question paraphrasing via LLM</li> </ul> <p>Generating process is like:</p> <p></p>"},{"location":"#generated-question-answering-pairs-for-icews-actor-and-cronquestion-kg","title":"Generated Question Answering Pairs for ICEWS Actor and CronQuestion KG","text":"Source KG Train Val Test Temporal Capabilities Count ICEWS Actor Simple 17,982 5,994 5,994 Temporal Constrained Retrieval 34,498 Medium 15,990 5,330 5,330 Timeline Position Retrieval 79,382 Complex 19,652 6,550 6,550 Timeline Operation 34,894 Temporal Semantic Operation 24,508 Total 53,624 17,874 17,874 89,372 CronQuestion KG Simple 7,200 2,400 2,400 Temporal Constrained Retrieval 19,720 Medium 8,252 2,751 2,751 Timeline Position Retrieval 37,720 Complex 9,580 3,193 3,193 Timeline Arithmetic Operation 21,966 Temporal Semantic Operation 15,720 Total 25,032 8,344 8,344 41,720 Answers Detailed Types ICEWS Actor CronQuestions KG Subject 17,249 9,860 Object 17,249 9,860 Timestamp Start 4,995 2,000 Timestamp End 4,995 2,000 Timestamp Range 4,995 2,000 Duration 4,995 2,000 Relation Duration 9,971 4,000 Relation Ranking 4,981 2,000 Relation Union or Intersection 19,942 8,000 <p>For comparison, here is the statistics for the CronQuestions dataset:</p> Difficulty Template Count Question Count Question Categorization Count Simple 158 177,922 Simple.Factual 106,208 Medium 165 90,641 Simple.Temporal 71,714 Complex 331 141,437 Medium.Factual 90,641 Medium.Temporal 0 Complex.Factual 67,709 Complex.Temporal 73,728 Total 654 410,000 410,000"},{"location":"#temporal-question-answering-solutions","title":"Temporal Question Answering Solutions","text":""},{"location":"#rag","title":"RAG","text":"<p>The first hot spot is the Retrieval Augmented Generation(RAG), which will use the large language model embedding as the semantic index, to retrieve question relevant information from the knowledge graph, and then generate the answer based on the retrieved information.</p> <p>Really this will be the dominant solution in the future?</p>"},{"location":"#tkgqa-embedding","title":"TKGQA Embedding","text":"<p>On this side of the Great Dividing Range, people focused on the graph embedding ways to solve the question answering over the knowledge graph. However, due to the challenge from the LLMs, people are tend to ignore LLM in their research for this stream, or just give up this area. There is not much work released in the past years regarding this area.</p> <p>Are this way really out of dated?</p> <p>From the technical perspective, current temporal knowledge graph embedding ways will not fit with our proposed and generated dataset, because for the complex questions, the relevant fact will be 3, and they should have no difference between this three. If all three hit, then the Hits@1 is True.</p> <p>So we developed a contrastive learning based temporal knowledge graph embedding way to solve this problem.</p>"},{"location":"#text2sql","title":"Text2SQL","text":"<p>The third way may never think about being a competitor in the KGQA area, but the LLMs provide this potential, as the knowledge graph in theory is just one table with a lot of interconnections. So generate a sql to retrieve related information will be not that hard for the LLM.</p> <p>Will it really perform well in this area?</p>"},{"location":"#finetuning","title":"Finetuning","text":"<p>The last way in theory should be the easiest way for application if you have QA pairs, because if you want to fine tune the ChatGPT, they will do it for you, all you need to do is to provide the QA pairs. However, one of the main problem is lacking of QA pairs. Which we have solved the problem above.</p> <p>So what's the real performance of this way if you do have enough QA pairs?</p>"},{"location":"#evaluation-metrics","title":"Evaluation Metrics","text":"<ul> <li>Hits@K: The percentage of questions where the correct answer is within the top K retrieved answers.</li> <li>MRR: The mean reciprocal rank of the correct answer.</li> </ul> <p>The Hits@K metric, used to evaluate the accuracy of event retrieval, is defined by the following criteria in our scenario:</p> <pre><code>\\text{Hits@K} = \n\\begin{cases} \n  1 &amp; \\text{if } \\sum_{i=0}^{nN-1}r_i = n \\\\\n  0 &amp; \\text{otherwise},\n\\end{cases}\n</code></pre> <p>where $r_i$ is an indicator function described as:</p> <pre><code>r_i = \n\\begin{cases}\n  1 &amp; \\text{if the $i$-th retrieved triplet matches an event} \\\\\n  0 &amp; \\text{otherwise}.\n\\end{cases}\n</code></pre> <p>The $n$ represents the number of involved events for the question. In this framework, $r_i$ functions as an indicator that takes the value 1 if the $i$-th retrieved triplet corresponds to one of the designated events, and 0 otherwise. The indexing for $r_i$ begins at 0.</p> <p>The Mean Reciprocal Rank (MRR) is defined as follows:</p> <pre><code>\\text{MRR} = \\frac{1}{Q} \\sum_{q=1}^Q \\frac{1}{\\text{rank}_q + 1}\n</code></pre> <p>where $Q$ denotes the number of queries, and $\\text{rank}_q$ is defined as the position of the first relevant document, i.e., $\\text{rank}_q = \\min { i : r_i = 1 }$. In our scenario, the definition of $\\text{rank}_q$ needs to be adjusted to accommodate multiple relevance within the same set of results.</p> <p>It is defined as:</p> <pre><code>\\text{rank}_q = \\sum_{i=0}^{\\|\\mathcal{F}\\|} \\left\\lfloor \\frac{i}{n} \\right\\rfloor r_i\n</code></pre> <p>where $|\\mathcal{F}|$ is the number of facts.</p>"},{"location":"#evaluation-results","title":"Evaluation Results","text":""},{"location":"#systematic-comparison-between-rag-and-tkgqa-embedding","title":"Systematic Comparison between RAG and TKGQA Embedding","text":"<p>As these two are similar approaches, so we will evaluate them together.</p> Dataset Model MRR (Overall) MRR (Simple) MRR (Medium) MRR (Complex) Hits@1 (Overall) Hits@1 (Simple) Hits@1 (Medium) Hits@1 (Complex) Hits@3 (Overall) Hits@3 (Simple) Hits@3 (Medium) Hits@3 (Complex) ICEWS Actor RAG 0.365 0.726 0.274 0.106 0.265 0.660 0.128 0.011 0.391 0.776 0.331 0.086 RAG_semantic 0.427 0.794 0.337 0.162 0.301 0.723 0.164 0.022 0.484 0.852 0.424 0.195 TimelineKGQA 0.660 0.861 0.632 0.497 0.486 0.782 0.435 0.257 0.858 0.929 0.845 0.805 CronQuestions KG RAG 0.331 0.771 0.218 0.101 0.235 0.704 0.092 0.009 0.348 0.824 0.249 0.077 RAG_semantic 0.344 0.775 0.229 0.122 0.237 0.707 0.094 0.010 0.371 0.828 0.267 0.122 TimelineKGQA 0.522 0.788 0.510 0.347 0.319 0.676 0.283 0.103 0.758 0.759 0.667 0.834"},{"location":"#hits1-for-text2sql","title":"Hits@1 for Text2SQL","text":"Dataset Model Hits@1 (Overall) Hits@1 (Simple) Hits@1 (Medium) Hits@1 (Complex) ICEWS Actor GPT3.5_base 0.179 0.268 0.170 0.105 GPT3.5_semantic 0.358 0.537 0.311 0.232 GPT3.5_semantic.oneshot 0.432 0.611 0.354 0.328 GPT4o_semantic.oneshot 0.485 0.650 0.392 0.408 TimelineKGQA 0.486 0.782 0.435 0.257 CronQuestions KG GPT3.5_base 0.158 0.393 0.079 0.052 GPT3.5_semantic 0.236 0.573 0.130 0.076 GPT3.5_semantic.oneshot 0.281 0.583 0.179 0.143 GPT4o_semantic.oneshot 0.324 0.623 0.201 0.207 TimelineKGQA 0.319 0.676 0.283 0.103"},{"location":"#finetuning-accuracy","title":"Finetuning accuracy","text":"Model Rephrased Question_as_answer Simple_for_medium GPT-3.5-Turbo 0.60 0.18 0.36 GPT-4o-mini 0.62 0.00 0.25"},{"location":"#development-setup","title":"Development Setup","text":""},{"location":"#install-the-package","title":"Install the package","text":"<pre><code># cd to current directory\ncd TimelineKGQA\npython3 -m venv venv\npip install -r requirements.txt\n# if you are doing development\npip install -r requirements.dev.txt\n\n# and then install the package\npip install -e .\n</code></pre> <p>If you are doing development, you will also need a database to store the knowledge graph.</p> <pre><code># spin up the database\ndocker-compose up -d\n\n# After this we need to load the data\n\n# for icews_dict\nsource venv/bin/activate\nexport OPENAI_API_KEY=sk-proj-xxx\n# this will load the icews_dicts data into the database\npython3 -m TimelineKGQA.data_loader.load_icews --mode load_data --data_name icews_dicts\n# this will create the unified knowledge graph\npython3 -m TimelineKGQA.data_loader.load_icews --mode actor_unified_kg\n\n# this will generate the question answering pairs\npython3 -m TimelineKGQA.generator\n</code></pre>"},{"location":"#folder-structure","title":"Folder Structure","text":"<pre><code>TimelineKGQA/\n\u251c\u2500\u2500 TimelineKGQA/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 generator.py\n\u2502   \u251c\u2500\u2500 processor.py\n\u2502   \u2514\u2500\u2500 utils.py\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 test_generator.py\n\u2502   \u2514\u2500\u2500 test_processor.py\n\u251c\u2500\u2500 docs/\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 examples/\n\u2502   \u2514\u2500\u2500 basic_usage.py\n\u251c\u2500\u2500 setup.py\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 LICENSE\n</code></pre>"},{"location":"Code/","title":"Index","text":""},{"location":"Code/constants/","title":"Constants","text":""},{"location":"Code/generator/","title":"Generator","text":""},{"location":"Code/generator/#TimelineKGQA.generator.generator","title":"<code>generator = TKGQAGenerator(table_name=(args.table_name), host=(args.host), port=(args.port), user=(args.user), password=(args.password), db_name=(args.db_name), first_draw_size=1000, paraphrased=(args.paraphrased), bulk_sql_size=(args.bulk_sql_size))</code>  <code>module-attribute</code>","text":"<p>Question Types:</p> <ul> <li>Simple: Timeline and One Event Involved<ul> <li>Ask for the timeline<ul> <li>Timeline Position Retrieval</li> </ul> </li> <li>Ask for the event<ul> <li>Temporal Constrained Retrieval</li> </ul> </li> </ul> </li> <li>Medium: Timeline and Two Events Involved<ul> <li>Timeline Position Retrieval =&gt; Temporal Constrained Retrieval</li> <li>Timeline Position Retrieval + Timeline Position Retrieval</li> </ul> </li> <li>Complex: Timeline and Three Events Involved<ul> <li>Timeline Position Retrieval + Timeline Position Retrieval + Timeline Position Retrieval</li> <li>Timeline Position Retrieval + Timeline Position Retrieval + Timeline Position Retrieval</li> </ul> </li> </ul>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator","title":"<code>TKGQAGenerator</code>","text":"<p>How human handle the temporal information and answer the temporal questions?</p>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator--information-indexing","title":"Information Indexing","text":"<p>When we see something, for example, an accident happen near our home in this morning. We need to first index this event into our brain. As we live in a three dimension space together with a time dimension, when we want to store this in our memory, (we will treat our memory as an N dimension space) - Index the spatial dimensions: is this close to my home or close to one of the point of interest in my mind - Index the temporal dimension: Temporal have several aspects     - Treat temporal as Straight Homogenous(Objective) Timeline: Exact date when it happens, for example,         [2023-05-01 10:00:00, 2023-05-01 10:30:00]     - Treat temporal as Cycle Homogenous(Objective) sTimeline:         Monday, First day of Month, Spring, 21st Century, etc.         (You can also cycle the timeline based on your own requirement)     - Treat temporal as Straight Heterogeneous(Subjective) Timeline:         If you sleep during night, it will be fast for you in the 8 hours, however,         if someone is working overnight, time will be slow for him.     - Treat temporal as Cycle Heterogeneous(Subjective) Timeline: Life has different turning points for everyone,     until they reach the end of their life. - Then index the information part: What happen, who is involved, what is the impact, etc.</p> <p>So in summary, we can say that in our mind, if we treat the event as embedding, part of the embedding will represent the temporal dimension information, part of the embedding will represent the spatial dimension information, the rest of the embedding will represent the general information part. This will help us to retrieve the information when we need it.</p>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator--information-retrieval","title":"Information Retrieval","text":"<p>So when we try to retrieval the information, especially the temporal part of the information. Normally we have several types:</p> <ul> <li>Timeline Position Retrieval: When Bush starts his term as president of US?<ul> <li>First: General Information Retrieval [(Bush, start, president of US), (Bush, term, president of US)]</li> <li>Second: Timeline Position Retrieval [(Bush, start, president of US, 2000, 2000),                                            (Bush, term, president of US, 2000, 2008)]</li> <li>Third: Answer the question based on the timeline information</li> </ul> </li> <li>Temporal Constrained Retrieval: In 2009, who is the president of US?<ul> <li>First: General Information Retrieval [(Bush, president of US), (Obama, president of US),                                            (Trump, president of US)]</li> <li>Second: Temporal Constraint Retrieval [(Obama, president of US, 2009, 2016)]</li> <li>Third: Answer the question based on the temporal constraint information</li> </ul> </li> </ul> <p>Three key things here: - General Information Retrieval: Retrieve the general information from the knowledge graph based on the question - Temporal Constrained Retrieval: Filter on general information retrieval, apply the temporal constraint - Timeline Position Retrieval: Based on general information retrieval, recover the timeline information</p>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator--temporal-questions","title":"Temporal Questions","text":"<p>We can try to classify the temporal questions from quite a few perspectives: - Based on Answer: Entity, Temporal - Based on Temporal Relations in Question: Before, After, During , etc. or First, Last, etc. - Based on Temporal Representation Type: Point, Range, Duration, etc. - Based on Complexity of Question: Simple (direct retrieval), Complex     (Multiple hops with the three key things we mention above)</p> <p>There is still no agreement or clear classification here, most of them stays in the first two. However, it is obvious that they have overlaps,     so will not be the best way to advance the temporal embedding algorithms development.</p> <p>We are trying to decompose the question into the three key parts we mentioned above,     so we can evaluate the ability of the models for this three key capabilities.</p>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator--simple-timeline-and-one-event-involved","title":"Simple: Timeline and One Event Involved","text":"<ul> <li>Timeline Position Retrieval: When Bush starts his term as president of US?<ul> <li>General Information Retrieval =&gt; Timeline Position Retrieval =&gt; Answer the question</li> <li>Question Focus can be: Timestamp Start, Timestamp End, Duration, Timestamp Start and End</li> </ul> </li> <li>Temporal Constrained Retrieval: In 2009, who is the president of US?<ul> <li>General Information Retrieval =&gt; Temporal Constraint Retrieval =&gt; Answer the question</li> <li>Question Focus can be: Subject, Object, Predicate. Can be more complex if we want mask out more elements</li> </ul> </li> </ul>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator--medium-timeline-and-two-events-involved","title":"Medium: Timeline and Two Events Involved","text":"<ul> <li>Timeline Position Retrieval + Timeline Position Retrieval: Is Bush president of US when 911 happen?<ul> <li>(General Information Retrieval =&gt; Timeline Position Retrieval) And (General Information Retrieval     =&gt; Timeline Position Retrieval) =&gt; Timeline Operation =&gt; Answer the question</li> <li>Question Focus can be: A new Time Range, A temporal relation (Before, After, During, etc.),     A list of Time Range (Ranking), or Comparison of Duration</li> </ul> </li> <li>Timeline Position Retrieval + Temporal Constrained Retrieval:     When Bush is president of US, who is the president of China?<ul> <li>(General Information Retrieval =&gt; Timeline Position Retrieval) =&gt;     Temporal Constraint Retrieval =&gt; Answer the question</li> <li>This is same as above, Question Focus can be: Subject, Object</li> </ul> </li> </ul>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator--complex-timeline-and-multiple-events-involved","title":"Complex: Timeline and Multiple Events Involved","text":"<p>In general, question focus (answer type) will only be two types when we extend from Medium Level - Timeline Operation - (Subject, Predicate, Object)</p> <p>So if we say Complex is 3 events and Timeline.</p> <ul> <li>Timeline Position Retrieval + Timeline Position Retrieval + Timeline Position Retrieval:     When Bush is president of US and Putin is President of Russian, is Hu the president of China?<ul> <li>(General Information Retrieval =&gt; Timeline Position Retrieval) And (General Information Retrieval     =&gt; Timeline Position Retrival) And (General Information Retrieval =&gt; Timeline Position Retrival)         =&gt; Timeline Operation =&gt; Answer the question</li> </ul> </li> <li>Timeline Position Retrieval + Timeline Position Retrieval + Temporal Constrained Retrieval:     When Bush is president of US and Putin is President of Russian, who is the president of China?</li> </ul> <p>input will be a unified knowledge graph, it will be stored in a table     subject     subject_json     predicate     predicate_json     object     object_json     start_time     end_time</p> Source code in <code>TimelineKGQA/generator.py</code> <pre><code>class TKGQAGenerator:\n    \"\"\"\n    **How human handle the temporal information and answer the temporal questions?**\n\n    ## Information Indexing\n    When we see something, for example, an accident happen near our home in this morning.\n    We need to first index this event into our brain.\n    As we live in a three dimension space together with a time dimension,\n    when we want to store this in our memory, (we will treat our memory as an N dimension space)\n    - Index the spatial dimensions: is this close to my home or close to one of the point of interest in my mind\n    - Index the temporal dimension: Temporal have several aspects\n        - Treat temporal as Straight Homogenous(Objective) Timeline: Exact date when it happens, for example,\n            [2023-05-01 10:00:00, 2023-05-01 10:30:00]\n        - Treat temporal as Cycle Homogenous(Objective) sTimeline:\n            Monday, First day of Month, Spring, 21st Century, etc.\n            (You can also cycle the timeline based on your own requirement)\n        - Treat temporal as Straight Heterogeneous(Subjective) Timeline:\n            If you sleep during night, it will be fast for you in the 8 hours, however,\n            if someone is working overnight, time will be slow for him.\n        - Treat temporal as Cycle Heterogeneous(Subjective) Timeline: Life has different turning points for everyone,\n        until they reach the end of their life.\n    - Then index the information part: What happen, who is involved, what is the impact, etc.\n\n    So in summary, we can say that in our mind, if we treat the event as embedding,\n    part of the embedding will represent the temporal dimension information,\n    part of the embedding will represent the spatial dimension information,\n    the rest of the embedding will represent the general information part.\n    This will help us to retrieve the information when we need it.\n\n    ## Information Retrieval\n    So when we try to retrieval the information, especially the temporal part of the information.\n    Normally we have several types:\n\n    - Timeline Position Retrieval: When Bush starts his term as president of US?\n        - First: **General Information Retrieval** [(Bush, start, president of US), (Bush, term, president of US)]\n        - Second: **Timeline Position Retrieval** [(Bush, start, president of US, 2000, 2000),\n                                                   (Bush, term, president of US, 2000, 2008)]\n        - Third: Answer the question based on the timeline information\n    - Temporal Constrained Retrieval: In 2009, who is the president of US?\n        - First: **General Information Retrieval** [(Bush, president of US), (Obama, president of US),\n                                                   (Trump, president of US)]\n        - Second: **Temporal Constraint Retrieval** [(Obama, president of US, 2009, 2016)]\n        - Third: Answer the question based on the temporal constraint information\n\n    Three key things here:\n    - **General Information Retrieval**: Retrieve the general information from the knowledge graph based on the question\n    - **Temporal Constrained Retrieval**: Filter on general information retrieval, apply the temporal constraint\n    - **Timeline Position Retrieval**: Based on general information retrieval, recover the timeline information\n\n    ## Temporal Questions\n    We can try to classify the temporal questions from quite a few perspectives:\n    - Based on Answer: Entity, Temporal\n    - Based on Temporal Relations in Question: Before, After, During , etc. or First, Last, etc.\n    - Based on Temporal Representation Type: Point, Range, Duration, etc.\n    - Based on Complexity of Question: Simple (direct retrieval), Complex\n        (Multiple hops with the three key things we mention above)\n\n    There is still no agreement or clear classification here, most of them stays in the first two.\n    However, it is obvious that they have overlaps,\n        so will not be the best way to advance the temporal embedding algorithms development.\n\n    We are trying to decompose the question into the three key parts we mentioned above,\n        so we can evaluate the ability of the models for this three key capabilities.\n\n    ### Simple: Timeline and One Event Involved\n    - Timeline Position Retrieval: When Bush starts his term as president of US?\n        - General Information Retrieval =&gt; Timeline Position Retrieval =&gt; Answer the question\n        - Question Focus can be: Timestamp Start, Timestamp End, Duration, Timestamp Start and End\n    - Temporal Constrained Retrieval: In 2009, who is the president of US?\n        - General Information Retrieval =&gt; Temporal Constraint Retrieval =&gt; Answer the question\n        - Question Focus can be: Subject, Object, Predicate. Can be more complex if we want mask out more elements\n\n    ### Medium: Timeline and Two Events Involved\n    - Timeline Position Retrieval + Timeline Position Retrieval: Is Bush president of US when 911 happen?\n        - (General Information Retrieval =&gt; Timeline Position Retrieval) And (General Information Retrieval\n            =&gt; Timeline Position Retrieval) =&gt; Timeline Operation =&gt; Answer the question\n        - Question Focus can be: A new Time Range, A temporal relation (Before, After, During, etc.),\n            A list of Time Range (Ranking), or Comparison of Duration\n    - Timeline Position Retrieval + Temporal Constrained Retrieval:\n        When Bush is president of US, who is the president of China?\n        - (General Information Retrieval =&gt; Timeline Position Retrieval) =&gt;\n            Temporal Constraint Retrieval =&gt; Answer the question\n        - This is same as above, Question Focus can be: Subject, Object\n\n    ### Complex: Timeline and Multiple Events Involved\n    In general, question focus (answer type) will only be two types when we extend from Medium Level\n    - Timeline Operation\n    - (Subject, Predicate, Object)\n\n    So if we say Complex is 3 events and Timeline.\n\n    - Timeline Position Retrieval + Timeline Position Retrieval + Timeline Position Retrieval:\n        When Bush is president of US and Putin is President of Russian, is Hu the president of China?\n        - (General Information Retrieval =&gt; Timeline Position Retrieval) And (General Information Retrieval\n            =&gt; Timeline Position Retrival) And (General Information Retrieval =&gt; Timeline Position Retrival)\n                =&gt; Timeline Operation =&gt; Answer the question\n    - Timeline Position Retrieval + Timeline Position Retrieval + Temporal Constrained Retrieval:\n        When Bush is president of US and Putin is President of Russian, who is the president of China?\n\n    input will be a unified knowledge graph, it will be stored in a table\n        subject\n        subject_json\n        predicate\n        predicate_json\n        object\n        object_json\n        start_time\n        end_time\n    \"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        host: str,\n        port: int,\n        user: str,\n        password: str,\n        db_name: str = \"tkgqa\",\n        first_draw_size: int = 100,\n        paraphrased: bool = False,\n        bulk_sql_size: int = 100,\n    ):\n        # set up the db connection\n        self.host = host\n        self.port = port\n        self.user = user\n        self.password = password\n        self.db_name = db_name\n        self.connection = psycopg2.connect(\n            host=self.host,\n            port=self.port,\n            user=self.user,\n            password=self.password,\n            dbname=self.db_name,\n        )\n        self.unified_kg_table = table_name\n        # we also need to create a new table, we can call it\n        self.unified_kg_table_questions = f\"{self.unified_kg_table}_questions\"\n        self.cursor = self.connection.cursor()\n        self.bulk_sql_size = bulk_sql_size\n        self.first_draw_size = first_draw_size\n        # create a table to store retrieval questions\n        self.cursor.execute(\n            f\"\"\"\n            CREATE TABLE IF NOT EXISTS {self.unified_kg_table_questions} (\n                id SERIAL PRIMARY KEY,\n                source_kg_id BIGINT,\n                question VARCHAR(1024),\n                answer VARCHAR(1024),\n                paraphrased_question VARCHAR(1024),\n                events VARCHAR(1024)[],\n                question_level VARCHAR(1024),\n                question_type VARCHAR(1024),\n                answer_type VARCHAR(1024),\n                temporal_relation VARCHAR(1024) DEFAULT NULL\n            );\n        \"\"\"\n        )\n        self.cursor.connection.commit()\n        self.paraphrased = paraphrased\n        with timer(the_logger=logger, message=\"Getting the events from the database\"):\n            self.cursor.execute(\n                f\"SELECT * FROM {self.unified_kg_table} ORDER BY RANDOM() LIMIT {self.first_draw_size};\"\n            )\n            events_df = pd.DataFrame(self.cursor.fetchall())\n            # set the column names\n            columns = [desc[0] for desc in self.cursor.description]\n            if len(events_df) &gt; 0:\n                events_df.columns = columns\n            else:\n                events_df = pd.DataFrame(columns=columns)\n            self.events_df = events_df\n        self.sample_simple_events = []\n        self.sample_medium_events = []\n        self.sample_complex_events = []\n\n    def simple_question_generation(self):\n        \"\"\"\n        ## Types of Questions\n        This is used to generate the simple question, we will have two types of questions.\n\n        For each type of questions, based on the answer or the question focus, we can further divide them into\n        - Timeline Position Retrival\n            - Start TimePoint\n            - End TimePoint\n            - Time Range\n            - Duration\n        - Temporal Constrained Retrieval (Ignore predicate for now)\n            - Subject\n            - Object\n\n        Simple: Timeline and One Event Involved\n        - Timeline Position Retrival: When Bush starts his term as president of US?\n            - General Information Retrieval =&gt; Timeline Position Retrival =&gt; Answer the question\n            - Question Focus can be: Timestamp Start, Timestamp End, Duration, Timestamp Start and End\n        - Temporal Constrained Retrieval: In 2009, who is the president of US?\n            - General Information Retrieval =&gt; Temporal Constraint Retrieval =&gt; Answer the question\n            - Question Focus can be: Subject, Object, Predicate. Can be more complex if we want mask out more elements\n\n\n        ## Templates\n        To generate the questions, We can try to feed into the LLM, and generate the questions.\n        However, the diversity of the questions is not guaranteed, so we can use the template to generate the questions.\n        Then use LLM to paraphrase the questions.\n\n        Template examples:\n        - Timeline Position Retrival\n            - Start TimePoint: When did {subject} start the term as {object}?\n            - End TimePoint: When did {subject} end the term as {object}?\n            - Time Range: When did {subject} serve as {object}?\n            - Duration: How long did {subject} serve as {object}?\n        - Temporal Constrained Retrieval\n            - Subject:\n                - Who is affiliated to {subject} from {timestamp start} to {timestamp end}?\n                - Who is affiliated to {subject} in {timestamp}?\n            - Object:\n                - {subject} is affiliated to which organisation from {timestamp start} to {timestamp end}?\n                - {subject} is affiliated to which organisation during {temporal representation}?\n\n        ## Process\n        - Extract {subject}, {predicate}, {object}, {start_time}, {end_time} from the unified graph\n        - Generate the questions based on the template for each type\n        - Use LLM to paraphrase the questions\n\n        Output format will be:\n        - {question}\n        - {answer}\n        - {paraphrased_question}\n        - subject, predicate, object, start_time, end_time\n        - {question_level} =&gt; Simple\n        - {question_type} =&gt; Timeline Position Retrival, Temporal Constrained Retrieval\n        - {answer_type} =&gt; Subject, Object | Timestamp Start, Timestamp End, Duration, Timestamp Start and End\n        \"\"\"\n        # get records not yet generated questions\n\n        insert_values_list = []\n        bulk_sql_pointer = 0\n        for item in self.sample_simple_events:\n            event = self.events_df.iloc[item]\n            questions = self.simple_question_generation_individual(\n                subject=event[\"subject\"],\n                predicate=event[\"predicate\"],\n                tail_object=event[\"object\"],\n                start_time=event[\"start_time\"],\n                end_time=event[\"end_time\"],\n                template_based=True,\n                paraphrased=self.paraphrased,\n            )\n\n            # insert each qa into the table, have a flat table\n            for question_obj in questions:\n                question_obj[\"source_kg_id\"] = int(event[\"id\"])\n                # get dict to tuple, sequence should be the same as the sql command\n                data = (\n                    question_obj[\"source_kg_id\"],\n                    question_obj[\"question\"],\n                    question_obj[\"answer\"],\n                    question_obj[\"paraphrased_question\"],\n                    question_obj[\"events\"],\n                    question_obj[\"question_level\"],\n                    question_obj[\"question_type\"],\n                    question_obj[\"answer_type\"],\n                    \"timeline\",\n                )\n                insert_values_list.append(data)\n                bulk_sql_pointer += 1\n                if bulk_sql_pointer % self.bulk_sql_size == 0:\n                    self.bulk_insert(values=insert_values_list)\n                    insert_values_list = []\n\n        self.bulk_insert(values=insert_values_list)\n\n    @staticmethod\n    def simple_question_generation_individual(\n        subject: str,\n        predicate: str,\n        tail_object: str,\n        start_time: str,\n        end_time: str,\n        template_based: bool = False,\n        paraphrased: bool = False,\n    ) -&gt; List[dict]:\n        \"\"\"\n        This will try to generate four questions belong to RE type\n\n        The questions will be:\n        - ? p o during the time range from start_time to end_time?\n        - s p ? during the time range from start_time to end_time?\n        - s p o from ? to end_time?\n        - s p o from start_time to ?\n        - s p o from ? to ?\n        - [How long/What's the duration, etc.] ? for the statement s p o\n\n        Args:\n            subject (str): The subject\n            predicate (str): The predicate\n            tail_object (str): The tail_object\n            start_time (str): The start time\n            end_time (str): The end time\n            template_based (bool): Whether you use the template based question generation\n            paraphrased (bool): Whether you do the paraphrase for the question, if set to False,\n                    then the paraphrased_question will be the same as the question\n\n        Returns:\n            dict: The generated questions\n                - question\n                - answer\n                - paraphrased_question\n                - events\n                - question_level: Simple\n                - question_type: The type of the question\n                - answer_type: The type of the answer\n        \"\"\"\n\n        questions = [\n            {\n                \"question\": f\"??? {predicate} {tail_object} during the time range from {start_time} to {end_time}?\",\n                \"answer\": f\"{subject}\",\n                \"paraphrased_question\": None,\n                \"events\": [\n                    f\"{subject}|{predicate}|{tail_object}|{start_time}|{end_time}\"\n                ],\n                \"question_level\": \"simple\",\n                \"question_type\": \"temporal_constrained_retrieval\",\n                \"answer_type\": \"subject\",\n            },\n            {\n                \"question\": f\"{subject} {predicate} ??? during the time range from {start_time} to {end_time}?\",\n                \"answer\": f\"{tail_object}\",\n                \"paraphrased_question\": None,\n                \"events\": [\n                    f\"{subject}|{predicate}|{tail_object}|{start_time}|{end_time}\"\n                ],\n                \"question_level\": \"simple\",\n                \"question_type\": \"temporal_constrained_retrieval\",\n                \"answer_type\": \"object\",\n            },\n            {\n                \"question\": f\"{subject} {predicate} {tail_object} from ??? to {end_time}?\",\n                \"answer\": f\"{start_time}\",\n                \"paraphrased_question\": None,\n                \"events\": [\n                    f\"{subject}|{predicate}|{tail_object}|{start_time}|{end_time}\"\n                ],\n                \"question_level\": \"simple\",\n                \"question_type\": \"timeline_position_retrieval\",\n                \"answer_type\": \"timestamp_start\",\n            },\n            {\n                \"question\": f\"{subject} {predicate} {tail_object} from {start_time} to ???\",\n                \"answer\": f\"{end_time}\",\n                \"paraphrased_question\": None,\n                \"events\": [\n                    f\"{subject}|{predicate}|{tail_object}|{start_time}|{end_time}\"\n                ],\n                \"question_level\": \"simple\",\n                \"question_type\": \"timeline_position_retrieval\",\n                \"answer_type\": \"timestamp_end\",\n            },\n            {\n                \"question\": f\"{subject} {predicate} {tail_object} from ??? to ???\",\n                \"answer\": f\"{start_time} and {end_time}\",\n                \"paraphrased_question\": None,\n                \"events\": [\n                    f\"{subject}|{predicate}|{tail_object}|{start_time}|{end_time}\"\n                ],\n                \"question_level\": \"simple\",\n                \"question_type\": \"timeline_position_retrieval\",\n                \"answer_type\": \"timestamp_range\",\n            },\n            {\n                \"question\": f\"[How long/What's the duration/etc] ??? for the statement \"\n                f\"{subject} {predicate} {tail_object}\",\n                \"answer\": f\"{end_time} - {start_time}\",\n                \"paraphrased_question\": None,\n                \"events\": [\n                    f\"{subject}|{predicate}|{tail_object}|{start_time}|{end_time}\"\n                ],\n                \"question_level\": \"simple\",\n                \"question_type\": \"timeline_position_retrieval\",\n                \"answer_type\": \"duration\",\n            },\n        ]\n        if template_based:\n            # we will random pick one from the template\n            for question_draft in questions:\n                this_type_templates = QUESTION_TEMPLATES[\n                    question_draft[\"question_level\"]\n                ][question_draft[\"question_type\"]][question_draft[\"answer_type\"]]\n                logger.debug(f\"this_type_templates: {this_type_templates}\")\n                random_pick_template = random.choice(this_type_templates)\n                # replace {subject}, {predicate}, {tail_object}, {start_time}, {end_time} with the real value\n                question_draft[\"question\"] = random_pick_template.format(\n                    subject=subject,\n                    predicate=predicate,\n                    tail_object=tail_object,\n                    start_time=start_time,\n                    end_time=end_time,\n                )\n\n        if paraphrased:\n            for question_obj in questions:\n                paraphrased_question = paraphrase_simple_question(\n                    question=question_obj[\"question\"]\n                )\n                logger.info(f\"paraphrased_question: {paraphrased_question}\")\n                question_obj[\"paraphrased_question\"] = paraphrased_question\n\n        return questions\n\n    def medium_question_generation(self):\n        \"\"\"\n        This will involve mainly two types of questions\n\n        - **Timeline Position Retrival =&gt; Temporal Constrained Retrieval**\n        - **Timeline Position Retrival + Timeline Position Retrival**\n\n        ---\n\n        - question_level: medium\n        - question_type:\n            - timeline_recovery_temporal_constrained_retrieval\n            - timeline_recovery_timeline_recovery\n        - answer_type:\n            - entity:\n                - subject:\n                - object\n            - temporal related\n                - Infer a new time range: Union/Intersection\n                - Infer a temporal relation: Allen\n                - Infer duration, and then compare\n                - Note: Ranking will be the same as Allen, so it will be in **Complex** level\n\n        \"\"\"\n\n        insert_values_list = []\n        bulk_sql_pointer = 0\n\n        for item in self.sample_medium_events:\n            first_event = self.events_df.iloc[item[0]]\n            second_event = self.events_df.iloc[item[1]]\n\n            source_kg_id = first_event[\"id\"] * 1000000 + second_event[\"id\"]\n            logger.debug(f\"Generating question for source_kg_id: {source_kg_id}\")\n            questions = self.medium_question_generation_individual(\n                first_event=first_event.to_dict(),\n                second_event=second_event.to_dict(),\n                template_based=True,\n                paraphrased=self.paraphrased,\n            )\n\n            for question_obj in questions:\n\n                question_obj[\"source_kg_id\"] = int(source_kg_id)\n                # get dict to tuple, sequence should be the same as the sql command\n                data = (\n                    question_obj[\"source_kg_id\"],\n                    question_obj[\"question\"],\n                    question_obj[\"answer\"],\n                    question_obj[\"paraphrased_question\"],\n                    question_obj[\"events\"],\n                    question_obj[\"question_level\"],\n                    question_obj[\"question_type\"],\n                    question_obj[\"answer_type\"],\n                    question_obj[\"temporal_relation\"],\n                )\n                insert_values_list.append(data)\n                bulk_sql_pointer += 1\n                if bulk_sql_pointer % self.bulk_sql_size == 0:\n                    self.bulk_insert(values=insert_values_list)\n                    insert_values_list = []\n        self.bulk_insert(values=insert_values_list)\n\n    def medium_question_generation_individual(\n        self,\n        first_event: dict,\n        second_event: dict,\n        template_based: bool = True,\n        paraphrased: bool = False,\n    ) -&gt; List[dict]:\n        \"\"\"\n\n        Args:\n            first_event (dict): The first event\n            second_event (dict): The second event\n            template_based (bool): Whether you use the template based question generation\n            paraphrased (bool): Whether you do the paraphrase for the question, if set to False,\n                    then the paraphrased_question will be the same as the question\n\n        Returns:\n            dict: The generated questions\n                - question\n                - answer\n                - paraphrased_question\n                - events\n                - question_level: Medium\n                - question_type: The type of the question\n                - answer_type: The type of the answer\n\n        - question_type:\n            - timeline_position_retrieval_temporal_constrained_retrieval\n                - For this one, the logic/reasoning/math part will be like: **TimeRange** +\n                    Temporal Semantic Operation =&gt; **TimeRange**\n                - Then the interesting part will be the Timeline Operation,\n                    we have mentioned several types of operations below.\n                    - There are mostly from numeric to semantic perspective\n                    - Here is the reverse process: name it Temporal Semantic Operation\n                    - So this is trying to convert the temporal semantic representation to\n                        a numeric operation and then get a new operation.\n            - timeline_position_retrieval_timeline_position_retrieval\n                - For the logic/reasoning/math side, it actually is **TimeRange** vs **TimeRange** =&gt; Timeline Operation\n                    - Get a way to ask about this comparison relations.\n                    - So the question will mainly be about whether this relation is True, or which relation it is.\n                    - For duration, we can ask about the duration of the two events, and then compare\n                    - Or we can compare the event ranking based on the time range\n            - there is another types: Three years before 2019,  who is the president of China? =&gt;\n                It is a valid question, but nobody will in this way.\n                - It will be normally classified into **simple**: in 2016, who is the president of China?\n                - Or it will be something like: Three years before bush end the term, who is the president of China?\n                    =&gt; This will be classified into **Medium**,\n                        and belong to the timeline_position_retrieval_temporal_constrained_retrieval\n        - answer_type:\n            - subject, object for timeline_position_retrieval_temporal_constrained_retrieval\n                - subject\n                - object\n                - only focus on the first one, as the second will always become the first later\n            - temporal related for timeline_position_retrieval_timeline_position_retrieval\n                - Infer a new time range: Union/Intersection\n                - Infer a temporal relation: Allen\n                - Infer a list of time ranges: Ranking\n                - Infer duration, and then compare\n\n        Process:\n\n        The quality of the question is not guaranteed by LLM directly if we just mask out the answer.\n        So we will use the template to generate the questions, then use LLM to paraphrase the questions.\n\n        \"\"\"\n        first_event_subject = first_event[\"subject\"]\n        first_event_predicate = first_event[\"predicate\"]\n        first_event_object = first_event[\"object\"]\n        first_event_start_time = first_event[\"start_time\"]\n        first_event_end_time = first_event[\"end_time\"]\n\n        second_event_subject = second_event[\"subject\"]\n        second_event_predicate = second_event[\"predicate\"]\n        second_event_object = second_event[\"object\"]\n        second_event_start_time = second_event[\"start_time\"]\n        second_event_end_time = second_event[\"end_time\"]\n\n        first_event_start_time_dt, first_event_end_time_dt = self.util_str_to_datetime(\n            [first_event_start_time, first_event_end_time]\n        )\n        second_event_start_time_dt, second_event_end_time_dt = (\n            self.util_str_to_datetime([second_event_start_time, second_event_end_time])\n        )\n\n        # first generate\n        # timeline_position_retrieval =&gt; timeline_position_retrieval\n        # this will ask for the subject or object in one of the event\n\n        medium_type_1_a_questions = []\n        questions = []\n        \"\"\"\n        Timeline Position Retrieval =&gt; Temporal Constrained Retrieval Questions\n        \"\"\"\n        # NOTES: question here actually is not used, because we will replace it with the template.\n        # It is putting there to get the idea about the types of questions we are generating\n        \"\"\"\n        The key part of this type is:\n        We need to cover as many temporal semantic operations as possible\n        - Before, After, During, this is the most common one and shown in the literature\n        - Starts from the same time, Ends at the same time, Meets, Overlap, this is another way to add the\n            temporal condition (inspired by the allen logic)\n        - Above are from allen temporal logic and intersection/union\n        - We can also add the ranking ones, however, before/after is the same as first/last, under this category\n        - Then the rest is the one for duration, question like 3 years before, 3 years after, etc.\n\n        So we have main two types of questions here:\n        - Relation: Before, After, During \uff5c Starts from the same time, Ends at the same time, Meets, Overlap\n            - calculate the relation first, then generated based on template\n            - Before: Who is the president of US before the end of Bush's term?\n            - After: Who is the president of US after the start of Bush's term?\n            - Starts from the same time: Who and Bush start their term as father and\n                President of US respectively at the same time?\n            - Ends at the same time: Who and Bush end their term as father and\n                President of US respectively at the same time?\n            - Meets: ?\n            - During: Who is the president of US during Bush's term?\n            - Overlap: Bush as the president of US meets who when the guy become the father?\n        - Duration: 3 years before, 3 years after, 3 years after the end, etc.\n            - calculate the duration first, then generated based on template\n            - 3 years before: Who is the president of US 3 years before the end of Bush's term?\n            - 3 years after: Who is the president of US 3 years after the start of Bush's term?\n            - 3 years after the end: Who is the president of US 3 years after the end of Bush's term?\n            - 3 years after the start: Who is the president of US 3 years after the start of Bush's term?\n            - meets/during/overlap hard to get a time point, so not considered here.\n        \"\"\"\n        # ask for first subject\n        medium_type_1_a_questions.append(\n            {\n                \"question\": f\"??? {first_event_predicate} {first_event_object} [Timeline Operation on \"\n                f\"({first_event_start_time}, {first_event_end_time}) vs ({second_event_start_time}, \"\n                f\"{second_event_end_time})] {second_event_subject}\"\n                f\"{second_event_predicate} {second_event_object}?\",\n                \"answer\": f\"{first_event_subject}\",\n                \"paraphrased_question\": None,\n                \"events\": [\n                    f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                    f\"{first_event_start_time}|{first_event_end_time}\",\n                    f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                    f\"{second_event_start_time}|{second_event_end_time}\",\n                ],\n                \"question_level\": \"medium\",\n                \"question_type\": \"timeline_position_retrieval_temporal_constrained_retrieval\",\n                \"answer_type\": \"subject\",\n                \"temporal_relation\": None,\n            }\n        )\n\n        # ask for first object\n        medium_type_1_a_questions.append(\n            {\n                \"question\": f\"{first_event_subject} {first_event_predicate} ??? [Timeline Operation on \"\n                f\"({first_event_start_time}, {first_event_end_time}) vs ({second_event_start_time},\"\n                f\"{second_event_end_time})] {second_event_subject}\"\n                f\" {second_event_predicate} {second_event_object}?\",\n                \"answer\": f\"{first_event_object}\",\n                \"paraphrased_question\": None,\n                \"events\": [\n                    f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                    f\"{first_event_start_time}|{first_event_end_time}\",\n                    f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                    f\"{second_event_start_time}|{second_event_end_time}\",\n                ],\n                \"question_level\": \"medium\",\n                \"question_type\": \"timeline_position_retrieval_temporal_constrained_retrieval\",\n                \"answer_type\": \"object\",\n                \"temporal_relation\": None,\n            }\n        )\n        questions += medium_type_1_a_questions\n\n        \"\"\"\n        For duration before, duration after type  question\n        \"\"\"\n        medium_type_1_b_questions = []\n        # this will be added later when we process the questions with template\n\n        \"\"\"\n        Timeline Position Retrieval + Timeline Position Retrieval Questions\n\n        This one is mainly from numeric to temporal semantic\n\n        - Infer a new time range: Union/Intersection\n        - Infer a temporal relation: Allen\n        - Infer a list of time ranges: Ranking (not considered here)\n        - Infer duration, and then compare\n        \"\"\"\n        # Timeline Position Retrieval + Timeline Position Retrieval\n\n        # ask for union/intersection of the time range\n\n        medium_type_2_questions = [\n            {\n                \"question\": f\"{first_event_subject} {first_event_predicate} {first_event_object} ???\"\n                f\"[Timeline Operation on ({first_event_start_time}, {first_event_end_time}) vs \"\n                f\"({second_event_start_time}, {second_event_end_time})]??? \"\n                f\"{second_event_subject} {second_event_predicate} {second_event_object}?\",\n                \"answer\": \"Union/Intersection of the time range\",\n                \"paraphrased_question\": None,\n                \"events\": [\n                    f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                    f\"{first_event_start_time}|{first_event_end_time}\",\n                    f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                    f\"{second_event_start_time}|{second_event_end_time}\",\n                ],\n                \"question_level\": \"medium\",\n                \"question_type\": \"timeline_position_retrieval_timeline_position_retrieval\",\n                \"answer_type\": \"relation_union_or_intersection\",\n                \"temporal_relation\": \"intersection\",\n            },\n            {\n                \"question\": f\"{first_event_subject} {first_event_predicate} {first_event_object} \"\n                f\"???[Timeline Operation on ({first_event_start_time}, {first_event_end_time}) \"\n                f\"vs ({second_event_start_time}, {second_event_end_time})]??? \"\n                f\"{second_event_subject} {second_event_predicate} {second_event_object}?\",\n                \"answer\": \"Union/Intersection of the time range\",\n                \"paraphrased_question\": None,\n                \"events\": [\n                    f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                    f\"{first_event_start_time}|{first_event_end_time}\",\n                    f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                    f\"{second_event_start_time}|{second_event_end_time}\",\n                ],\n                \"question_level\": \"medium\",\n                \"question_type\": \"timeline_position_retrieval_timeline_position_retrieval\",\n                \"answer_type\": \"relation_union_or_intersection\",\n                \"temporal_relation\": \"union\",\n            },\n            {\n                \"question\": f\"{first_event_subject} {first_event_predicate} {first_event_object} \"\n                f\"???[Timeline Operation on ({first_event_start_time}, \"\n                f\"{first_event_end_time}) vs ({second_event_start_time}, \"\n                f\"{second_event_end_time})]??? {second_event_subject} \"\n                f\"{second_event_predicate} {second_event_object}?\",\n                \"answer\": \"Duration\",\n                \"paraphrased_question\": None,\n                \"events\": [\n                    f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                    f\"{first_event_start_time}|{first_event_end_time}\",\n                    f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                    f\"{second_event_start_time}|{second_event_end_time}\",\n                ],\n                \"question_level\": \"medium\",\n                \"question_type\": \"timeline_position_retrieval_timeline_position_retrieval\",\n                \"answer_type\": \"relation_duration\",\n                \"temporal_relation\": None,\n            },\n        ]\n        questions += medium_type_2_questions\n        temporal_answer = None\n        if template_based:\n            for question_draft in questions:\n                this_type_templates = QUESTION_TEMPLATES[\n                    question_draft[\"question_level\"]\n                ][question_draft[\"question_type\"]][question_draft[\"answer_type\"]]\n\n                if (\n                    question_draft[\"answer_type\"] == \"subject\"\n                    or question_draft[\"answer_type\"] == \"object\"\n                ):\n                    \"\"\"\n                    Handle the Medium Type 1 Questions here: Both a and b\n                    First calculate the relations, then based on relations to select the template\n                    \"\"\"\n                    temporal_relation = self.relation_allen_time_range(\n                        time_range_a=[\n                            first_event_start_time_dt,\n                            first_event_end_time_dt,\n                        ],\n                        time_range_b=[\n                            second_event_start_time_dt,\n                            second_event_end_time_dt,\n                        ],\n                    )\n                    temporal_relation_semantic = temporal_relation.get(\"semantic\")\n                    question_draft[\"temporal_relation\"] = temporal_relation[\"relation\"]\n                    random_pick_template = random.choice(\n                        this_type_templates[temporal_relation_semantic]\n                    )\n\n                    question_draft[\"question\"] = random_pick_template.format(\n                        first_event_subject=first_event_subject,\n                        first_event_predicate=first_event_predicate,\n                        first_event_object=first_event_object,\n                        temporal_relation=temporal_relation,\n                        second_event_subject=second_event_subject,\n                        second_event_predicate=second_event_predicate,\n                        second_event_object=second_event_object,\n                    )\n                    # this will generate the basic temporal relation questions.\n                    # TODO: we also need to generate the one duration_before, duration_after\n                    # If relation is before or after, then we can generate the duration_before, duration_after\n                    # Add it to variable medium_type_1_b_questions\n                    if temporal_relation_semantic in [\"before\", \"after\"]:\n                        random_pick_template = random.choice(\n                            this_type_templates[\n                                f\"duration_{temporal_relation_semantic}\"\n                            ]\n                        )\n                        # get the duration year\n                        # Example: 3 years before Bush as the president of US, who is the president of China?\n                        # The duration is calculated based on first_end_time - second_start time\n                        # NOTE: It can be extended further later to calculate first_start - second_start\n                        duration = self.relation_duration_calculation(\n                            time_range_a=[\n                                first_event_start_time_dt,\n                                first_event_end_time_dt,\n                            ],\n                            time_range_b=[\n                                second_event_start_time_dt,\n                                second_event_end_time_dt,\n                            ],\n                            temporal_operator=f\"duration_{temporal_relation_semantic}\",\n                        )\n                        # copy a new question draft\n                        duration_question_draft = copy.deepcopy(question_draft)\n                        duration_question_draft[\"question\"] = (\n                            random_pick_template.format(\n                                first_event_subject=first_event_subject,\n                                first_event_predicate=first_event_predicate,\n                                first_event_object=first_event_object,\n                                temporal_relation=f\"{duration} {temporal_relation}\",\n                                second_event_subject=second_event_subject,\n                                second_event_predicate=second_event_predicate,\n                                second_event_object=second_event_object,\n                            )\n                        )\n                        duration_question_draft[\"temporal_relation\"] = (\n                            f\"duration_{temporal_relation_semantic}\"\n                        )\n                        medium_type_1_b_questions.append(duration_question_draft)\n                else:\n                    \"\"\"\n                    Handle in theory four types of questions here\n                    \"\"\"\n                    if (\n                        question_draft[\"answer_type\"]\n                        == \"relation_union_or_intersection\"\n                    ):\n                        temporal_relation = question_draft[\"temporal_relation\"]\n                        random_pick_template = random.choice(\n                            this_type_templates[temporal_relation]\n                        )\n                        temporal_answer = self.relation_union_or_intersection(\n                            time_ranges=[\n                                (first_event_start_time_dt, first_event_end_time_dt),\n                                (second_event_start_time_dt, second_event_end_time_dt),\n                            ],\n                            temporal_operator=temporal_relation,\n                        )\n\n                        question_draft[\"question\"] = random_pick_template.format(\n                            first_event_subject=first_event_subject,\n                            first_event_predicate=first_event_predicate,\n                            first_event_object=first_event_object,\n                            second_event_subject=second_event_subject,\n                            second_event_predicate=second_event_predicate,\n                            second_event_object=second_event_object,\n                        )\n                        if temporal_answer is None:\n                            temporal_answer = \"No Answer\"\n                        question_draft[\"answer\"] = temporal_answer\n                    elif question_draft[\"answer_type\"] == \"relation_allen\":\n                        temporal_allen_relation = self.relation_allen_time_range(\n                            time_range_a=[\n                                first_event_start_time_dt,\n                                first_event_end_time_dt,\n                            ],\n                            time_range_b=[\n                                second_event_start_time_dt,\n                                second_event_end_time_dt,\n                            ],\n                        )\n                        question_draft[\"temporal_relation\"] = temporal_allen_relation[\n                            \"relation\"\n                        ]\n                        # random select from [choices, true_false]\n                        question_format = random.choice([\"choice\", \"true_false\"])\n                        if question_format == \"choice\":\n                            random_pick_template = random.choice(\n                                this_type_templates[\"choice\"]\n                            )\n                            temporal_answer = temporal_allen_relation[\"relation\"]\n                            question_draft[\"question\"] = random_pick_template.format(\n                                first_event_subject=first_event_subject,\n                                first_event_predicate=first_event_predicate,\n                                first_event_object=first_event_object,\n                                second_event_subject=second_event_subject,\n                                second_event_predicate=second_event_predicate,\n                                second_event_object=second_event_object,\n                            )\n                            question_draft[\"answer\"] = temporal_answer\n\n                        else:\n                            random_pick_template = random.choice(\n                                this_type_templates[\"true_false\"]\n                            )\n                            random_yes_no_answer = random.choice([\"True\", \"False\"])\n                            if random_yes_no_answer == \"True\":\n                                temporal_relation = temporal_allen_relation[\"relation\"]\n                            else:\n                                temporal_relation = random.choice(\n                                    list(\n                                        set(self.allen_relations)\n                                        - {temporal_allen_relation[\"relation\"]}\n                                    )\n                                )\n                            question_draft[\"question\"] = random_pick_template.format(\n                                first_event_subject=first_event_subject,\n                                first_event_predicate=first_event_predicate,\n                                first_event_object=first_event_object,\n                                second_event_subject=second_event_subject,\n                                second_event_predicate=second_event_predicate,\n                                second_event_object=second_event_object,\n                                temporal_relation=temporal_relation,\n                            )\n                            question_draft[\"answer\"] = random_yes_no_answer\n                    elif question_draft[\"answer_type\"] == \"relation_duration\":\n                        \"\"\"There are four types in this category\n                        - duration =&gt; which is the intersection of the two time range\n                        - duration_compare =&gt; longer shorter equal\n                        - sum =&gt; total duration of the two time range, which is actually the union\n                        - average =&gt; average duration of the two time range\n                        \"\"\"\n                        temporal_relation = random.choice(\n                            [\n                                \"duration\",\n                                \"duration_compare\",\n                                \"sum\",\n                                \"average\",\n                            ]\n                        )\n                        random_pick_template = random.choice(\n                            this_type_templates[temporal_relation]\n                        )\n                        question_draft[\"temporal_relation\"] = temporal_relation\n                        if temporal_relation == \"duration\":\n                            temporal_answer = self.relation_union_or_intersection(\n                                time_ranges=[\n                                    (\n                                        first_event_start_time_dt,\n                                        first_event_end_time_dt,\n                                    ),\n                                    (\n                                        second_event_start_time_dt,\n                                        second_event_end_time_dt,\n                                    ),\n                                ],\n                                temporal_operator=\"intersection\",\n                            )\n                            question_draft[\"question\"] = random_pick_template.format(\n                                first_event_subject=first_event_subject,\n                                first_event_predicate=first_event_predicate,\n                                first_event_object=first_event_object,\n                                second_event_subject=second_event_subject,\n                                second_event_predicate=second_event_predicate,\n                                second_event_object=second_event_object,\n                            )\n                        elif temporal_relation == \"duration_compare\":\n                            temporal_relation_duration = (\n                                self.relation_allen_time_duration(\n                                    time_range_a=[\n                                        first_event_start_time_dt,\n                                        first_event_end_time_dt,\n                                    ],\n                                    time_range_b=[\n                                        second_event_start_time_dt,\n                                        second_event_end_time_dt,\n                                    ],\n                                )\n                            )\n                            temporal_answer = temporal_relation_duration[\"semantic\"]\n                            question_draft[\"question\"] = random_pick_template.format(\n                                first_event_subject=first_event_subject,\n                                first_event_predicate=first_event_predicate,\n                                first_event_object=first_event_object,\n                                temporal_relation=temporal_answer,\n                                second_event_subject=second_event_subject,\n                                second_event_predicate=second_event_predicate,\n                                second_event_object=second_event_object,\n                            )\n                        elif temporal_relation == \"sum\":\n                            temporal_answer = self.util_average_duration_calculation(\n                                time_ranges=[\n                                    [\n                                        first_event_start_time_dt,\n                                        first_event_end_time_dt,\n                                    ],\n                                    [\n                                        second_event_start_time_dt,\n                                        second_event_end_time_dt,\n                                    ],\n                                ],\n                                temporal_operator=\"sum\",\n                            )\n                            question_draft[\"question\"] = random_pick_template.format(\n                                first_event_subject=first_event_subject,\n                                first_event_predicate=first_event_predicate,\n                                first_event_object=first_event_object,\n                                second_event_subject=second_event_subject,\n                                second_event_predicate=second_event_predicate,\n                                second_event_object=second_event_object,\n                            )\n                        elif temporal_relation == \"average\":\n                            temporal_answer = self.util_average_duration_calculation(\n                                time_ranges=[\n                                    [\n                                        first_event_start_time_dt,\n                                        first_event_end_time_dt,\n                                    ],\n                                    [\n                                        second_event_start_time_dt,\n                                        second_event_end_time_dt,\n                                    ],\n                                ],\n                                temporal_operator=\"average\",\n                            )\n                            question_draft[\"question\"] = random_pick_template.format(\n                                first_event_subject=first_event_subject,\n                                first_event_predicate=first_event_predicate,\n                                first_event_object=first_event_object,\n                                second_event_subject=second_event_subject,\n                                second_event_predicate=second_event_predicate,\n                                second_event_object=second_event_object,\n                            )\n                        question_draft[\"answer\"] = temporal_answer\n                        question_draft[\"temporal_relation\"] = temporal_relation\n\n        questions += medium_type_1_b_questions\n        if paraphrased:\n            for question_obj in questions:\n                paraphrased_question = paraphrase_medium_question(\n                    question=question_obj[\"question\"],\n                )\n                logger.info(f\"paraphrased_question: {paraphrased_question}\")\n                question_obj[\"paraphrased_question\"] = paraphrased_question\n\n        return questions\n\n    def complex_question_generation(self):\n        \"\"\"\n        This is to generate the complex question, which will involve three events and timeline\n\n        - Type 1: Before Bush, after Kennedy, who is the president of US?\n        - Type 2: Who is the first president of US among Bush, Kennedy, and Obama?\n        \"\"\"\n\n        # next step is to construct three events\n        insert_values_list = []\n        bulk_sql_pointer = 0\n\n        for item in self.sample_complex_events:\n            first_event = self.events_df.iloc[item[0]]\n            second_event = self.events_df.iloc[item[1]]\n            third_event = self.events_df.iloc[item[2]]\n            source_kg_id = (\n                first_event[\"id\"] * 1000000 * 1000000\n                + second_event[\"id\"] * 1000000\n                + third_event[\"id\"]\n            )\n\n            questions = self.complex_question_generation_individual(\n                first_event=first_event.to_dict(),\n                second_event=second_event.to_dict(),\n                third_event=third_event.to_dict(),\n                template_based=True,\n                paraphrased=self.paraphrased,\n            )\n            for question_obj in questions:\n                question_obj[\"source_kg_id\"] = int(source_kg_id)\n                # get dict to tuple, sequence should be the same as the sql command\n                data = (\n                    question_obj[\"source_kg_id\"],\n                    question_obj[\"question\"],\n                    question_obj[\"answer\"],\n                    question_obj[\"paraphrased_question\"],\n                    question_obj[\"events\"],\n                    question_obj[\"question_level\"],\n                    question_obj[\"question_type\"],\n                    question_obj[\"answer_type\"],\n                    question_obj[\"temporal_relation\"],\n                )\n                insert_values_list.append(data)\n                bulk_sql_pointer += 1\n                if bulk_sql_pointer % self.bulk_sql_size == 0:\n                    self.bulk_insert(insert_values_list)\n                    insert_values_list = []\n\n        self.bulk_insert(insert_values_list)\n\n    def complex_question_generation_individual(\n        self,\n        first_event: dict,\n        second_event: dict,\n        third_event: dict,\n        template_based: bool = True,\n        paraphrased: bool = True,\n    ) -&gt; List[dict]:\n        \"\"\"\n        Args:\n            first_event (dict): The first event\n            second_event (dict): The second event\n            third_event (dict): The third event\n            template_based (bool): Whether you use the template based question generation\n            paraphrased (bool): Whether you do the paraphrase for the question, if set to False,\n                    then the paraphrased_question will be the same as the question\n\n        Returns:\n            dict: The generated questions\n                - question\n                - answer\n                - paraphrased_question\n                - events\n                - question_level: Complex\n                - question_type: The type of the question\n                - answer_type: The type of the answer\n\n\n        - question_type:\n            - timeline_position_retrieval *2 + temporal constrained retrieval\n            - timeline_position_retrieval *3\n        - answer_type:\n            - type1:\n                - subject\n                    - trel b, trel c, ? predicate object\n                - object\n                    - trel b, trel c, subject predicate ?\n            - type2:\n                - Infer a new time range: Union/Intersection\n                    - trel b, trel c, from when to when the subject predicate object? (intersection)\n                    - within (trel b, trel c), who is the subject predicate object? (union)\n                - Infer a temporal relation: Allen\n                    - ? More making sense one is ranking\n                    - hard to justify the question that\n                    - If we ask for choice question, it will be between two events\n                    - If we ask for true/false, event a,b,c; ab, ac, bc;  Question, ab+ac =&gt; is bc relation True\n                - Infer a list of time ranges: Ranking\n                    - Who is the {} among a,b,c? =&gt; an\n                - Infer duration, and then compare\n                    - Who is the president of US for the longest time among a, b, c? =&gt; a\n\n        \"\"\"\n\n        first_event_subject = first_event[\"subject\"]\n        first_event_predicate = first_event[\"predicate\"]\n        first_event_object = first_event[\"object\"]\n        first_event_start_time = first_event[\"start_time\"]\n        first_event_end_time = first_event[\"end_time\"]\n\n        second_event_subject = second_event[\"subject\"]\n        second_event_predicate = second_event[\"predicate\"]\n        second_event_object = second_event[\"object\"]\n        second_event_start_time = second_event[\"start_time\"]\n        second_event_end_time = second_event[\"end_time\"]\n\n        third_event_subject = third_event[\"subject\"]\n        third_event_predicate = third_event[\"predicate\"]\n        third_event_object = third_event[\"object\"]\n        third_event_start_time = third_event[\"start_time\"]\n        third_event_end_time = third_event[\"end_time\"]\n\n        first_event_start_time_dt, first_event_end_time_dt = self.util_str_to_datetime(\n            [first_event_start_time, first_event_end_time]\n        )\n        second_event_start_time_dt, second_event_end_time_dt = (\n            self.util_str_to_datetime([second_event_start_time, second_event_end_time])\n        )\n        third_event_start_time_dt, third_event_end_time_dt = self.util_str_to_datetime(\n            [third_event_start_time, third_event_end_time]\n        )\n\n        # first generate\n        complex_type_1_a_questions = []\n        questions = []\n\n        # timeline_position_retrieval *2 + temporal constrained retrieval\n        # ask for the first subject\n        complex_type_1_a_questions.append(\n            {\n                \"question\": f\"??? {first_event_predicate} {first_event_object} {second_event_predicate} \"\n                f\"{second_event_object} {third_event_predicate} {third_event_object}?\",\n                \"answer\": f\"{first_event_subject}\",\n                \"paraphrased_question\": None,\n                \"events\": [\n                    f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                    f\"{first_event_start_time}|{first_event_end_time}\",\n                    f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                    f\"{second_event_start_time}|{second_event_end_time}\",\n                    f\"{third_event_subject}|{third_event_predicate}|{third_event_object}|\"\n                    f\"{third_event_start_time}|{third_event_end_time}\",\n                ],\n                \"question_level\": \"complex\",\n                \"question_type\": \"timeline_position_retrieval*2+temporal_constrained_retrieval\",\n                \"answer_type\": \"subject\",\n                \"temporal_relation\": None,\n            }\n        )\n        # ask for first object\n        complex_type_1_a_questions.append(\n            {\n                \"question\": f\"{first_event_subject} {first_event_predicate} ??? {second_event_predicate} \"\n                f\"{second_event_object} {third_event_predicate} {third_event_object}?\",\n                \"answer\": f\"{first_event_object}\",\n                \"paraphrased_question\": None,\n                \"events\": [\n                    f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                    f\"{first_event_start_time}|{first_event_end_time}\",\n                    f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                    f\"{second_event_start_time}|{second_event_end_time}\",\n                    f\"{third_event_subject}|{third_event_predicate}|{third_event_object}|\"\n                    f\"{third_event_start_time}|{third_event_end_time}\",\n                ],\n                \"question_level\": \"complex\",\n                \"question_type\": \"timeline_position_retrieval*2+temporal_constrained_retrieval\",\n                \"answer_type\": \"object\",\n                \"temporal_relation\": None,\n            }\n        )\n\n        questions += complex_type_1_a_questions\n\n        \"\"\"\n        For duration before, duration after type question\n        \"\"\"\n\n        complex_type_1_b_questions = []\n        # this will be added later when we process the questions with template\n\n        \"\"\"\n        Timeline Position Retrieval + Timeline Position Retrieval + Timeline Position Retrieval\n        \"\"\"\n\n        complex_type_2_questions = [\n            {\n                \"question\": f\"{first_event_subject} {first_event_predicate} {first_event_object} \"\n                f\"{second_event_predicate} {second_event_object} \"\n                f\"{third_event_predicate} {third_event_object}?\",\n                \"answer\": \"Union/Intersection of the time range\",\n                \"paraphrased_question\": None,\n                \"events\": [\n                    f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                    f\"{first_event_start_time}|{first_event_end_time}\",\n                    f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                    f\"{second_event_start_time}|{second_event_end_time}\",\n                    f\"{third_event_subject}|{third_event_predicate}|{third_event_object}|\"\n                    f\"{third_event_start_time}|{third_event_end_time}\",\n                ],\n                \"question_level\": \"complex\",\n                \"question_type\": \"timeline_position_retrieval*3\",\n                \"answer_type\": \"relation_union_or_intersection\",\n                \"temporal_relation\": \"intersection\",\n            },\n            {\n                \"question\": f\"{first_event_subject} {first_event_predicate} {first_event_object} \"\n                f\"{second_event_predicate} {second_event_object} \"\n                f\"{third_event_predicate} {third_event_object}?\",\n                \"answer\": \"Union/Intersection of the time range\",\n                \"paraphrased_question\": None,\n                \"events\": [\n                    f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                    f\"{first_event_start_time}|{first_event_end_time}\",\n                    f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                    f\"{second_event_start_time}|{second_event_end_time}\",\n                    f\"{third_event_subject}|{third_event_predicate}|{third_event_object}|\"\n                    f\"{third_event_start_time}|{third_event_end_time}\",\n                ],\n                \"question_level\": \"complex\",\n                \"question_type\": \"timeline_position_retrieval*3\",\n                \"answer_type\": \"relation_union_or_intersection\",\n                \"temporal_relation\": \"union\",\n            },\n            {\n                \"question\": f\"{first_event_subject} {first_event_predicate} {first_event_object} \"\n                f\"{second_event_predicate} {second_event_object} \"\n                f\"{third_event_predicate} {third_event_object}?\",\n                \"answer\": \"Duration\",\n                \"paraphrased_question\": None,\n                \"events\": [\n                    f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                    f\"{first_event_start_time}|{first_event_end_time}\",\n                    f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                    f\"{second_event_start_time}|{second_event_end_time}\",\n                    f\"{third_event_subject}|{third_event_predicate}|{third_event_object}|\"\n                    f\"{third_event_start_time}|{third_event_end_time}\",\n                ],\n                \"question_level\": \"complex\",\n                \"question_type\": \"timeline_position_retrieval*3\",\n                \"answer_type\": \"relation_duration\",\n                \"temporal_relation\": None,\n            },\n            # add ranking one\n            {\n                \"question\": f\"Who is the xxx among {first_event_subject}, {second_event_subject}, \"\n                f\"and {third_event_subject}?\",\n                \"answer\": \"Ranking\",\n                \"paraphrased_question\": None,\n                \"events\": [\n                    f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                    f\"{first_event_start_time}|{first_event_end_time}\",\n                    f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                    f\"{second_event_start_time}|{second_event_end_time}\",\n                    f\"{third_event_subject}|{third_event_predicate}|{third_event_object}|\"\n                    f\"{third_event_start_time}|{third_event_end_time}\",\n                ],\n                \"question_level\": \"complex\",\n                \"question_type\": \"timeline_position_retrieval*3\",\n                \"answer_type\": \"relation_ranking\",\n                \"temporal_relation\": \"ranking\",\n            },\n        ]\n\n        questions += complex_type_2_questions\n        temporal_answer = None\n\n        if template_based:\n            for question_draft in questions:\n                this_type_templates = QUESTION_TEMPLATES[\n                    question_draft[\"question_level\"]\n                ][question_draft[\"question_type\"]][question_draft[\"answer_type\"]]\n\n                if (\n                    question_draft[\"answer_type\"] == \"subject\"\n                    or question_draft[\"answer_type\"] == \"object\"\n                ):\n                    \"\"\"\n                    Handle the Complex Type 1 Questions here:\n                    a,b,c, will ask for a information.\n                    Get temporal_relation_12, temporal_relation_13, then generate the question\n                    \"\"\"\n                    temporal_relation_12 = self.relation_allen_time_range(\n                        time_range_a=[\n                            first_event_start_time_dt,\n                            first_event_end_time_dt,\n                        ],\n                        time_range_b=[\n                            second_event_start_time_dt,\n                            second_event_end_time_dt,\n                        ],\n                    )\n                    temporal_relation_13 = self.relation_allen_time_range(\n                        time_range_a=[\n                            first_event_start_time_dt,\n                            first_event_end_time_dt,\n                        ],\n                        time_range_b=[\n                            third_event_start_time_dt,\n                            third_event_end_time_dt,\n                        ],\n                    )\n\n                    temporal_relation_12_semantic = temporal_relation_12.get(\"semantic\")\n                    temporal_relation_13_semantic = temporal_relation_13.get(\"semantic\")\n                    question_draft[\"temporal_relation\"] = (\n                        f\"{temporal_relation_12['relation']}&amp;{temporal_relation_13['relation']}\"\n                    )\n                    random_pick_template = random.choice(this_type_templates)\n\n                    question_draft[\"question\"] = random_pick_template.format(\n                        first_event_subject=first_event_subject,\n                        first_event_predicate=first_event_predicate,\n                        first_event_object=first_event_object,\n                        temporal_relation_12=temporal_relation_12_semantic,\n                        second_event_subject=second_event_subject,\n                        second_event_predicate=second_event_predicate,\n                        second_event_object=second_event_object,\n                        temporal_relation_13=temporal_relation_13_semantic,\n                        third_event_subject=third_event_subject,\n                        third_event_predicate=third_event_predicate,\n                        third_event_object=third_event_object,\n                    )\n\n                    # this will generate the basic temporal relation questions.\n                    # then we will want to generate the duration_before, duration_after\n                    can_generate_duration_question = False\n                    if temporal_relation_12_semantic in [\"before\", \"after\"]:\n                        duration = self.relation_duration_calculation(\n                            time_range_a=[\n                                first_event_start_time_dt,\n                                first_event_end_time_dt,\n                            ],\n                            time_range_b=[\n                                second_event_start_time_dt,\n                                second_event_end_time_dt,\n                            ],\n                            temporal_operator=f\"duration_{temporal_relation_12_semantic}\",\n                        )\n                        temporal_relation_12_semantic = (\n                            f\"{duration} {temporal_relation_12_semantic}\"\n                        )\n                        logger.debug(temporal_relation_12_semantic)\n                        can_generate_duration_question = True\n                    if temporal_relation_13_semantic in [\"before\", \"after\"]:\n                        duration = self.relation_duration_calculation(\n                            time_range_a=[\n                                first_event_start_time_dt,\n                                first_event_end_time_dt,\n                            ],\n                            time_range_b=[\n                                third_event_start_time_dt,\n                                third_event_end_time_dt,\n                            ],\n                            temporal_operator=f\"duration_{temporal_relation_13_semantic}\",\n                        )\n                        temporal_relation_13_semantic = (\n                            f\"{duration} {temporal_relation_13_semantic}\"\n                        )\n                        logger.debug(temporal_relation_13_semantic)\n                        can_generate_duration_question = True\n                    if can_generate_duration_question:\n                        # copy a new question draft\n                        duration_question_draft = copy.deepcopy(question_draft)\n                        duration_question_draft[\"question\"] = (\n                            random_pick_template.format(\n                                first_event_subject=first_event_subject,\n                                first_event_predicate=first_event_predicate,\n                                first_event_object=first_event_object,\n                                temporal_relation_12=temporal_relation_12_semantic,\n                                second_event_subject=second_event_subject,\n                                second_event_predicate=second_event_predicate,\n                                second_event_object=second_event_object,\n                                temporal_relation_13=temporal_relation_13_semantic,\n                                third_event_subject=third_event_subject,\n                                third_event_predicate=third_event_predicate,\n                                third_event_object=third_event_object,\n                            )\n                        )\n                        duration_question_draft[\"temporal_relation\"] = (\n                            f\"duration_{temporal_relation_12_semantic}&amp;duration_{temporal_relation_13_semantic}\"\n                        )\n                        complex_type_1_b_questions.append(duration_question_draft)\n                else:\n                    # handle the Timeline Position Retrieval + Timeline Position Retrieval + Timeline Position Retrieval\n                    if (\n                        question_draft[\"answer_type\"]\n                        == \"relation_union_or_intersection\"\n                    ):\n                        temporal_relation = question_draft[\"temporal_relation\"]\n                        random_pick_template = random.choice(\n                            this_type_templates[temporal_relation]\n                        )\n                        temporal_answer = self.relation_union_or_intersection(\n                            time_ranges=[\n                                (first_event_start_time_dt, first_event_end_time_dt),\n                                (second_event_start_time_dt, second_event_end_time_dt),\n                                (third_event_start_time_dt, third_event_end_time_dt),\n                            ],\n                            temporal_operator=temporal_relation,\n                        )\n\n                        question_draft[\"question\"] = random_pick_template.format(\n                            first_event_subject=first_event_subject,\n                            first_event_predicate=first_event_predicate,\n                            first_event_object=first_event_object,\n                            second_event_subject=second_event_subject,\n                            second_event_predicate=second_event_predicate,\n                            second_event_object=second_event_object,\n                            third_event_subject=third_event_subject,\n                            third_event_predicate=third_event_predicate,\n                            third_event_object=third_event_object,\n                        )\n                        logger.debug(question_draft[\"question\"])\n                        logger.debug(temporal_answer)\n                        if temporal_answer is None:\n                            temporal_answer = \"No Answer\"\n                        question_draft[\"answer\"] = temporal_answer\n                    elif question_draft[\"answer_type\"] == \"relation_duration\":\n                        \"\"\"\n                        There are four types in this category\n                        - duration =&gt; which is the intersection of the two time range\n                        - duration_compare =&gt; longer shorter equal\n                        - sum =&gt; total duration of the two time range, which is actually the union\n                        - average =&gt; average duration of the two time range\n                        \"\"\"\n                        temporal_relation = random.choice(\n                            [\n                                \"duration\",\n                                \"duration_compare\",\n                                \"sum\",\n                                \"average\",\n                            ]\n                        )\n                        random_pick_template = random.choice(\n                            this_type_templates[temporal_relation]\n                        )\n                        question_draft[\"temporal_relation\"] = temporal_relation\n                        if temporal_relation == \"duration\":\n                            temporal_answer = self.relation_union_or_intersection(\n                                time_ranges=[\n                                    (\n                                        first_event_start_time_dt,\n                                        first_event_end_time_dt,\n                                    ),\n                                    (\n                                        second_event_start_time_dt,\n                                        second_event_end_time_dt,\n                                    ),\n                                    (\n                                        third_event_start_time_dt,\n                                        third_event_end_time_dt,\n                                    ),\n                                ],\n                                temporal_operator=\"intersection\",\n                            )\n                            question_draft[\"question\"] = random_pick_template.format(\n                                first_event_subject=first_event_subject,\n                                first_event_predicate=first_event_predicate,\n                                first_event_object=first_event_object,\n                                second_event_subject=second_event_subject,\n                                second_event_predicate=second_event_predicate,\n                                second_event_object=second_event_object,\n                                third_event_subject=third_event_subject,\n                                third_event_predicate=third_event_predicate,\n                                third_event_object=third_event_object,\n                            )\n                        elif temporal_relation == \"duration_compare\":\n                            # we do duration ranking here\n                            duration_rank_by_index = self.relation_duration(\n                                time_ranges=[\n                                    [\n                                        first_event_start_time_dt,\n                                        first_event_end_time_dt,\n                                    ],\n                                    [\n                                        second_event_start_time_dt,\n                                        second_event_end_time_dt,\n                                    ],\n                                    [\n                                        third_event_start_time_dt,\n                                        third_event_end_time_dt,\n                                    ],\n                                ],\n                                agg_temporal_operator=\"ranking\",\n                            )\n                            logger.debug(duration_rank_by_index)\n                            temporal_answer = duration_rank_by_index[0] + 1\n                            question_draft[\"question\"] = random_pick_template.format(\n                                first_event_subject=first_event_subject,\n                                first_event_predicate=first_event_predicate,\n                                first_event_object=first_event_object,\n                                second_event_subject=second_event_subject,\n                                second_event_predicate=second_event_predicate,\n                                second_event_object=second_event_object,\n                                third_event_subject=third_event_subject,\n                                third_event_predicate=third_event_predicate,\n                                third_event_object=third_event_object,\n                                temporal_duration_rank=temporal_answer,\n                            )\n                        elif temporal_relation == \"sum\":\n                            temporal_answer = self.util_average_duration_calculation(\n                                time_ranges=[\n                                    [\n                                        first_event_start_time_dt,\n                                        first_event_end_time_dt,\n                                    ],\n                                    [\n                                        second_event_start_time_dt,\n                                        second_event_end_time_dt,\n                                    ],\n                                    [\n                                        third_event_start_time_dt,\n                                        third_event_end_time_dt,\n                                    ],\n                                ],\n                                temporal_operator=\"sum\",\n                            )\n                            question_draft[\"question\"] = random_pick_template.format(\n                                first_event_subject=first_event_subject,\n                                first_event_predicate=first_event_predicate,\n                                first_event_object=first_event_object,\n                                second_event_subject=second_event_subject,\n                                second_event_predicate=second_event_predicate,\n                                second_event_object=second_event_object,\n                                third_event_subject=third_event_subject,\n                                third_event_predicate=third_event_predicate,\n                                third_event_object=third_event_object,\n                            )\n                        elif temporal_relation == \"average\":\n                            temporal_answer = self.util_average_duration_calculation(\n                                time_ranges=[\n                                    [\n                                        first_event_start_time_dt,\n                                        first_event_end_time_dt,\n                                    ],\n                                    [\n                                        second_event_start_time_dt,\n                                        second_event_end_time_dt,\n                                    ],\n                                    [\n                                        third_event_start_time_dt,\n                                        third_event_end_time_dt,\n                                    ],\n                                ],\n                                temporal_operator=\"average\",\n                            )\n                            logger.debug(temporal_answer)\n                            question_draft[\"question\"] = random_pick_template.format(\n                                first_event_subject=first_event_subject,\n                                first_event_predicate=first_event_predicate,\n                                first_event_object=first_event_object,\n                                second_event_subject=second_event_subject,\n                                second_event_predicate=second_event_predicate,\n                                second_event_object=second_event_object,\n                                third_event_subject=third_event_subject,\n                                third_event_predicate=third_event_predicate,\n                                third_event_object=third_event_object,\n                            )\n                        question_draft[\"answer\"] = temporal_answer\n                        question_draft[\"temporal_relation\"] = temporal_relation\n                    elif question_draft[\"answer_type\"] == \"relation_ranking\":\n                        # random select, ranking based on start time or end time\n                        rank_by_what = random.choice(\n                            [\"rank_start_time\", \"rank_end_time\"]\n                        )\n                        rank_by_index = self.relation_ordinal_time_range(\n                            time_ranges=[\n                                [first_event_start_time_dt, first_event_end_time_dt],\n                                [second_event_start_time_dt, second_event_end_time_dt],\n                                [third_event_start_time_dt, third_event_end_time_dt],\n                            ],\n                            agg_temporal_operator=rank_by_what,\n                        )\n\n                        random_pick_template = random.choice(\n                            this_type_templates[rank_by_what]\n                        )\n                        question_draft[\"question\"] = random_pick_template.format(\n                            first_event_subject=first_event_subject,\n                            first_event_predicate=first_event_predicate,\n                            first_event_object=first_event_object,\n                            second_event_subject=second_event_subject,\n                            second_event_predicate=second_event_predicate,\n                            second_event_object=second_event_object,\n                            third_event_subject=third_event_subject,\n                            third_event_predicate=third_event_predicate,\n                            third_event_object=third_event_object,\n                        )\n                        temporal_answer = rank_by_index[0] + 1\n                        question_draft[\"answer\"] = temporal_answer\n                        question_draft[\"temporal_relation\"] = rank_by_what\n\n        questions += complex_type_1_b_questions\n        if paraphrased:\n            for question_obj in questions:\n                paraphrased_question = paraphrase_medium_question(\n                    question=question_obj[\"question\"],\n                )\n                logger.info(f\"paraphrased_question: {paraphrased_question}\")\n                question_obj[\"paraphrased_question\"] = paraphrased_question\n\n        return questions\n\n    @staticmethod\n    def relation_allen_time_range(time_range_a: list, time_range_b: list) -&gt; dict:\n        \"\"\"\n        This function will return the allen temporal relation between two time ranges\n\n        We have the 26 possible relations\n        - 13 for time range operation\n        - 10 for time point and time range operation\n        - 3 for time point operation\n\n        We will need to extract them from the quantitatively time range\n\n        We will have \"beginning of time\" or \"end of time\" to represent the infinite time range\n        We will need to convert it to a numerical value in np.inf\n        Others will be converted to a numerical value in the timestamp\n\n        Args:\n            time_range_a (list[datetime, datetime]): The first time range\n            time_range_b (list[datetime, datetime]): The second time range\n\n        Returns:\n            dict: The allen temporal relation between the two time ranges\n        \"\"\"\n        start_time1, end_time1 = time_range_a\n        start_time2, end_time2 = time_range_b\n\n        logger.debug(\n            f\"start_time1: {start_time1}, end_time1: {end_time1}, start_time2: {start_time2}, end_time2: {end_time2}\"\n        )\n\n        # 13 for time range operation\n        time_range_a_datetime = [start_time1, end_time1]\n        time_range_b_datetime = [start_time2, end_time2]\n        # then we will do the operation for the time range, get the allen temporal relation\n        \"\"\"\n        x_start &lt;= x_end\n        y_start &lt;= y_end\n        allen_operator = [\n            x_start - x_end,  # 0, or -1, which means a is a point or a range\n            y_start - y_end, # 0, or -1\n            x_start - y_start,\n            x_start - y_end,\n            x_end - y_start,\n            x_end - y_end,\n        ]\n\n        After this do a operation for the allen_operator, if value = 0, keep it, &lt; 0, set it to -1, &gt; 0, set it to 1\n\n        Then we will have:\n        13 for time range operation, which means x_start &lt; x_end, y_start &lt; y_end\n            - X &lt;  Y =&gt; [-1, -1, -1, -1, -1, -1]\n            - X m  Y =&gt; [-1, -1, -1, -1,  0, -1]\n            - X o  Y =&gt; [-1, -1, -1, -1,  1, -1]\n            - X fi Y =&gt; [-1, -1, -1, -1,  1,  0]\n            - X di Y =&gt; [-1, -1, -1, -1,  1,  1]\n            - X s  Y =&gt; [-1, -1,  0, -1,  1, -1]\n            - X =  Y =&gt; [-1, -1,  0, -1,  1,  0]\n            - X si Y =&gt; [-1, -1,  0, -1,  1,  1]\n            - X d  Y =&gt; [-1, -1,  1, -1,  1, -1]\n            - X f  Y =&gt; [-1, -1,  1, -1,  1,  0]\n            - X oi Y =&gt; [-1, -1,  1, -1,  1,  1]\n            - X mi Y =&gt; [-1, -1,  1,  0,  1,  1]\n            - X &gt;  Y =&gt; [-1, -1,  1,  1,  1,  1]\n\n        10 for time point and time range operation\n        Among the 10, 5 for X is a point, 5 for Y is a point\n        5 for X is a point, Y is a range, which means x_start = x_end, y_start &lt; y_end\n            - X &lt;  Y =&gt; [0, -1, -1, -1, -1, -1]\n            - X s  Y =&gt; [0, -1,  0, -1,  0, -1]\n            - X d  Y =&gt; [0, -1,  1, -1,  1, -1]\n            - X f  Y =&gt; [0, -1,  1,  0,  1,  0]\n            - X &gt;  Y =&gt; [0, -1,  1,  1,  1,  1]\n        5 for X is a range, Y is a point, which means x_start &lt; x_end, y_start = y_end\n            - X &lt;  Y =&gt; [-1, 0, -1, -1, -1, -1]\n            - X fi Y =&gt; [-1, 0, -1\uff0c-1,  0,  0]\n            - X di Y =&gt; [-1, 0, -1, -1,  1,  1]\n            - X si Y =&gt; [-1, 0,  0,  0,  1,  1]\n            - X &gt;  Y =&gt; [-1, 0,  1,  1,  1,  1]\n\n        3 for time point operation, which means x_start = x_end, y_start = y_end\n            - X &lt; Y =&gt; [0, 0, -1, -1, -1, -1]\n            - X = Y =&gt; [0, 0,  0,  0,  0,  0]\n            - X &gt; Y =&gt; [0, 0,  1,  1,  1,  1]\n        \"\"\"\n\n        allen_operator = [\n            time_range_a_datetime[0] - time_range_a_datetime[1],\n            time_range_b_datetime[0] - time_range_b_datetime[1],\n            time_range_a_datetime[0] - time_range_b_datetime[0],\n            time_range_a_datetime[0] - time_range_b_datetime[1],\n            time_range_a_datetime[1] - time_range_b_datetime[0],\n            time_range_a_datetime[1] - time_range_b_datetime[1],\n        ]\n\n        # do the operation for the allen_operator\n        for index, value in enumerate(allen_operator):\n            if value == 0:\n                allen_operator[index] = 0\n            elif value &lt; 0:\n                allen_operator[index] = -1\n            else:\n                allen_operator[index] = 1\n\n        # logger.critical(f\"allen_operator: {allen_operator}\")\n        # get it to be a tuple\n        allen_operator = tuple(allen_operator)\n        logger.debug(f\"allen_operator: {allen_operator}\")\n        try:\n            logger.debug(f\"ALLEN_OPERATOR_DICT: {ALLEN_OPERATOR_DICT[allen_operator]}\")\n            return ALLEN_OPERATOR_DICT[allen_operator]\n        except KeyError:\n            logger.info(f\"allen_operator: {allen_operator}\")\n            logger.info(f\"time_range_a: {time_range_a}\")\n            logger.info(f\"time_range_b: {time_range_b}\")\n            raise ValueError(\"The allen operator is not found\")\n\n    @staticmethod\n    def relation_allen_time_duration(time_range_a: list, time_range_b: list) -&gt; dict:\n        \"\"\"\n\n        Args:\n            time_range_a (list[datetime, datetime]): The first time range\n            time_range_b (list[datetime, datetime]): The second time range\n\n        Returns:\n            dict: The allen temporal relation between the two time ranges\n        \"\"\"\n        duration_a = abs(time_range_a[1] - time_range_a[0])\n        duration_b = abs(time_range_b[1] - time_range_b[0])\n        if duration_a &lt; duration_b:\n            return {\n                \"relation\": \"X &lt; Y\",\n                \"description\": \"X is shorter Y\",\n                \"category\": \"td\",\n                \"code\": \"td-1\",\n                \"semantic\": \"shorter\",\n            }\n        elif duration_a == duration_b:\n            return {\n                \"relation\": \"X = Y\",\n                \"description\": \"X equals Y\",\n                \"category\": \"td\",\n                \"code\": \"td-2\",\n                \"semantic\": \"equals\",\n            }\n        else:\n            return {\n                \"relation\": \"X &gt; Y\",\n                \"description\": \"X is longer Y\",\n                \"category\": \"td\",\n                \"code\": \"td-3\",\n                \"semantic\": \"longer\",\n            }\n\n    @staticmethod\n    def relation_union_or_intersection(\n        time_ranges: List[Tuple[np.datetime64, np.datetime64]],\n        temporal_operator: str = \"intersection\",\n    ) -&gt; Optional[str]:\n        \"\"\"\n        This function will return the temporal operator between multiple time ranges\n        The temporal operator can be:\n            - 'intersection'\n            - 'union'\n\n        Args:\n            time_ranges (List[Tuple[datetime, datetime]]): A list of time ranges\n            temporal_operator (str): The temporal operator\n\n        Returns:\n            str: A string representation of the new time range, or None if no valid range exists.\n\n        \"\"\"\n        if temporal_operator not in [\"intersection\", \"union\"]:\n            raise ValueError(\n                \"temporal_operator should be either 'intersection' or 'union'\"\n            )\n\n        if not time_ranges:\n            return None\n\n        # Start with the first time range\n        result = time_ranges[0]\n\n        for current in time_ranges[1:]:\n            if temporal_operator == \"intersection\":\n                # Find the latest start time and earliest end time\n                start = max(result[0], current[0])\n                end = min(result[1], current[1])\n                if start &gt;= end:\n                    return None  # No intersection\n                result = (start, end)\n            elif temporal_operator == \"union\":\n                # Find the earliest start time and latest end time\n                start = min(result[0], current[0])\n                end = max(result[1], current[1])\n                # Check if there is a gap between the ranges\n                if result[1] &lt; current[0] or current[1] &lt; result[0]:\n                    return None  # No continuous union possible\n                result = (start, end)\n\n        return f\"({result[0]}, {result[1]})\"\n\n    @staticmethod\n    def relation_ordinal_time_range(\n        time_ranges: list[[datetime, datetime]], agg_temporal_operator: str = None\n    ) -&gt; list:\n        \"\"\"\n        For the time range, it will do the rank operation, sort it\n\n        Aggregation operator can be:\n        - ranking(min, max)\n            - ranking_start\n            - ranking_end\n\n        Args:\n            time_ranges (list): The list of time ranges\n            agg_temporal_operator (str): The aggregation temporal operator\n\n        Returns:\n            list: the list of sorted index for the time range\n\n        For example, we have the time range:\n\n        ```\n        time_ranges = [\n            (datetime(2023, 5, 1, 12, 0), datetime(2023, 5, 1, 15, 0)),  # 3 hours\n            (datetime(2023, 5, 1, 9, 30), datetime(2023, 5, 1, 14, 0)),  # 4.5 hours\n            (datetime(2023, 5, 1, 8, 0), datetime(2023, 5, 1, 11, 30)),  # 3.5 hours\n            (datetime(2023, 5, 2, 9, 30), datetime(2023, 5, 2, 12, 0)),  # 2.5 hours\n            (datetime(2023, 5, 1, 10, 30), datetime(2023, 5, 1, 13, 0))  # 2.5 hours\n        ]\n\n        result_start = TKGQAGenerator.aggregate_tr_temporal_operator(time_ranges, \"ranking_start\")\n        [3,1,0,4,2]\n\n        result_end = TKGQAGenerator.aggregate_tr_temporal_operator(time_ranges, \"ranking_end\")\n        [2,4,3,0,1]\n        ```\n        \"\"\"\n\n        # Create a list of indices paired with time ranges\n        indexed_time_ranges = list(enumerate(time_ranges))\n\n        if agg_temporal_operator == \"rank_start_time\":\n            # Sort by start time, but maintain original indices\n            indexed_time_ranges.sort(key=lambda x: x[1][0])\n        elif agg_temporal_operator == \"rank_end_time\":\n            # Sort by end time, but maintain original indices\n            indexed_time_ranges.sort(key=lambda x: x[1][1])\n        else:\n            raise ValueError(\n                \"Unsupported aggregation temporal operator. Please use 'rank_start_time' or 'rank_end_time'.\"\n            )\n\n        # After sorting, create a new list that maps the original index to its new rank\n        rank_by_index = [0] * len(time_ranges)  # Pre-initialize a list of zeros\n        for rank, (original_index, _) in enumerate(indexed_time_ranges):\n            rank_by_index[original_index] = rank\n\n        return rank_by_index\n\n    @staticmethod\n    def relation_duration(\n        time_ranges: list[[datetime, datetime]], agg_temporal_operator: str = None\n    ):\n        \"\"\"\n        For the time range, it will do the rank operation, sort it\n\n        First calculate the duration of the time range, then do the rank operation based on the duration\n\n        Args:\n            time_ranges (list): The list of time ranges\n            agg_temporal_operator (str): The aggregation temporal operator\n\n        Returns:\n            list: the list of sorted index for the time range\n\n\n        Example:\n        ```\n        time_ranges = [\n            (datetime(2023, 5, 1, 12, 0), datetime(2023, 5, 1, 15, 0)),  # 3 hours\n            (datetime(2023, 5, 1, 9, 30), datetime(2023, 5, 1, 14, 0)),  # 4.5 hours\n            (datetime(2023, 5, 1, 8, 0), datetime(2023, 5, 1, 11, 30)),  # 3.5 hours\n            (datetime(2023, 5, 2, 9, 30), datetime(2023, 5, 2, 12, 0)),  # 2.5 hours\n            (datetime(2023, 5, 1, 10, 30), datetime(2023, 5, 1, 13, 0))  # 2.5 hours\n        ]\n        ```\n\n        The output will be:\n        ```\n        [2, 4, 3, 0, 1]\n        ```\n        \"\"\"\n        # Create a list of indices paired with time ranges\n        if agg_temporal_operator == \"ranking\":\n            indexed_time_ranges = list(enumerate(time_ranges))\n\n            indexed_time_ranges.sort(key=lambda x: abs(x[1][1] - x[1][0]))\n            rank_by_index = [0] * len(time_ranges)  # Pre-initialize a list of zeros\n            for index, (original_index, _) in enumerate(indexed_time_ranges):\n                rank_by_index[original_index] = index\n            return rank_by_index\n        if agg_temporal_operator == \"sum\":\n            # total value of the time range\n            durations = [\n                abs(time_range[1] - time_range[0]) for time_range in time_ranges\n            ]\n            return sum(durations)\n        if agg_temporal_operator == \"average\":\n            # average value of the time range\n            durations = [\n                abs(time_range[1] - time_range[0]) for time_range in time_ranges\n            ]\n            return sum(durations) / len(durations)\n        raise ValueError(\n            \"Unsupported aggregation temporal operator. Please use 'ranking', 'sum' or 'average'.\"\n        )\n\n    @staticmethod\n    def relation_duration_calculation(\n        time_range_a: list,\n        time_range_b: list,\n        temporal_operator: str = None,\n    ) -&gt; Optional[timedelta]:\n        \"\"\"\n        We will calculate the time difference between two time ranges\n\n        However, there are several combination we can do here\n\n        - duration_before =&gt; abs(time_range_a[1] - time_range_b[0])\n        - duration_after =&gt; abs(time_range_b[1] - time_range_a[0])\n        We also have other combinations, but we will not consider them here\n\n        Args:\n            time_range_a (list[datetime, datetime]): The first time range\n            time_range_b (list[datetime, datetime]): The second time range\n            temporal_operator (str): The temporal operator\n\n        Returns:\n            timedelta: The time difference between two time ranges\n\n        \"\"\"\n        if temporal_operator is None or temporal_operator not in [\n            \"duration_before\",\n            \"duration_after\",\n        ]:\n            raise ValueError(\n                \"temporal_operator should be one of the following: duration_before, duration_after\"\n            )\n        if temporal_operator == \"duration_before\":\n            return abs(time_range_a[1] - time_range_b[0])\n        if temporal_operator == \"duration_after\":\n            return abs(time_range_b[1] - time_range_a[0])\n        return None\n\n    @staticmethod\n    def util_str_to_datetime(time_range: list) -&gt; Tuple[np.datetime64, np.datetime64]:\n        \"\"\"\n        Convert the string to datetime\n\n        Args:\n            time_range (list[str, str]): The time range in string format\n\n        Returns:\n            list[datetime, datetime]: The time range in datetime format\n\n        \"\"\"\n        start_time, end_time = time_range\n        if start_time == \"beginning of time\":\n            start_time = datetime.min.replace(year=1)\n        if end_time == \"end of time\":\n            end_time = datetime.max.replace(year=9999)\n\n        # convert the time to numerical value, format is like this: 1939-04-25\n        start_time = np.datetime64(start_time)\n        end_time = np.datetime64(end_time)\n\n        return start_time, end_time\n\n    def util_average_duration_calculation(\n        self, time_ranges: list[[datetime, datetime]], temporal_operator: str = None\n    ):\n        try:\n            if temporal_operator == \"average\":\n                durations = [\n                    abs(time_range[1] - time_range[0]) for time_range in time_ranges\n                ]\n                average_d = sum(durations) / len(durations)\n                return self.utils_format_np_datetime(average_d)\n\n            if temporal_operator == \"sum\":\n                durations = [\n                    abs(time_range[1] - time_range[0]) for time_range in time_ranges\n                ]\n                total = sum(durations)\n                return self.utils_format_np_datetime(total)\n            return None\n        except Exception as e:\n            logger.error(f\"Error in util_average_duration_calculation: {e}\")\n            return None\n\n    @staticmethod\n    def utils_format_np_datetime(np_date_delta):\n        td = timedelta(seconds=np_date_delta / (np.timedelta64(1, \"s\")))\n        days = td.days\n        seconds = td.seconds\n        # Compute hours, minutes, and remaining seconds\n        hours, remainder = divmod(seconds, 3600)\n        minutes, seconds = divmod(remainder, 60)\n\n        # also get how many years\n        years = days // 365\n        if years &gt; 2000:\n            return \"forever\"\n        months = (days % 365) // 30\n        days = (days % 365) % 30\n        human_readable_format = (\n            f\"{years} years, {months} months, \"\n            f\"{days} days, {hours} hours, {minutes} minutes, {seconds} seconds\"\n        )\n        return human_readable_format\n\n    @property\n    def allen_relations(self):\n        return [\n            \"X &lt; Y\",\n            \"X m Y\",\n            \"X o Y\",\n            \"X fi Y\",\n            \"X di Y\",\n            \"X s Y\",\n            \"X = Y\",\n            \"X si Y\",\n            \"X d Y\",\n            \"X f Y\",\n            \"X oi Y\",\n            \"X mi Y\",\n            \"X &gt; Y\",\n        ]\n\n    def bulk_insert(self, values: List[Tuple]):\n        \"\"\"\n        This function will insert the values into the table\n\n        \"\"\"\n        values_str = \",\\n\".join([\"(%s, %s, %s, %s, %s, %s, %s, %s, %s)\"] * len(values))\n        # Flatten the list of values tuples into a single tuple for execution\n        flat_values = [item for sublist in values for item in sublist]\n\n        bulk_insert_query = f\"\"\"\n        INSERT INTO {self.unified_kg_table_questions} (\n                                                    source_kg_id,\n                                                      question,\n                                                      answer,\n                                                      paraphrased_question,\n                                                      events,\n                                                      question_level,\n                                                      question_type,\n                                                      answer_type,\n                                                      temporal_relation\n                                                      )\n        VALUES {values_str}\n        \"\"\"\n        if len(values) == 0:\n            return\n        # Execute the bulk insert command\n        try:\n            self.cursor.execute(bulk_insert_query, flat_values)\n            self.connection.commit()\n            logger.info(f\"Successfully inserted {len(values)} rows into the table.\")\n        except Exception as e:\n            logger.exception(f\"Error: {e}\")\n            logger.info(flat_values)\n\n    def sampling_events(\n        self,\n        sample_percentage: Union[float, dict, int] = 0.1,\n        sample_strategy: str = \"temporal_close\",\n    ):\n        \"\"\"\n        This function will sample the events from the list\n\n        Args:\n            sample_percentage (float): The sample percentage\n            sample_strategy (str): The sample strategy, can be random, temporal_close, degree_high, both\n\n        Returns:\n            List[Tuple]: The list Tuples (event1_id, event2_id, event3_id)\n\n        \"\"\"\n\n        if sample_strategy not in (\"random\", \"temporal_close\", \"degree_high\", \"both\"):\n            raise ValueError(\n                \"sample_strategy should be random, temporal_close, degree_high, or both\"\n            )\n\n        \"\"\"\n        Do matrix sampling based on the element value\n        If the dimension is 2, we will have a nxn matrix\n            - value of the matrix is 1 for random\n            - value will be calculated based on the temporal information if it is temporal_close\n            - value will be calculated based on the degree information if it is degree_high\n            - value will be calculated based on both temporal and degree information if it is both\n\n        And then use the value as weight to do the sampling over the matrix\n        \"\"\"\n\n        num_events = len(self.events_df)\n        # for dimension 1 generate, first construct a matrix with len(event_df), using numpy,\n        # and it is always sampling randomly\n        if sample_strategy == \"degree_high\" or sample_strategy == \"both\":\n            degree_scores = self.calculate_degree_scores(self.events_df)\n        else:\n            degree_scores = None\n\n        with timer(the_logger=logger, message=\"Generating the matrix D1\"):\n            dimension_1_matrix = np.ones(num_events)\n\n            # make sure every element in the matrix sum to 1\n            dimension_1_matrix = dimension_1_matrix / dimension_1_matrix.sum()\n\n        with timer(the_logger=logger, message=\"Generating the matrix D2\"):\n            # for dimension 2 generate, first construct a matrix with len(event_df), using numpy\n            dimension_2_matrix = np.zeros((num_events, num_events))\n\n            if sample_strategy == \"random\":\n                dimension_2_matrix = np.ones((num_events, num_events))\n            elif sample_strategy == \"temporal_close\":\n                start_times = self.events_df[\"start_time\"].values\n                end_times = self.events_df[\"end_time\"].values\n\n                for x in range(num_events):\n                    for y in range(x + 1, num_events):  # y &gt; x to avoid redundancy\n                        score = self.temporal_close_score(\n                            time_ranges=[\n                                [start_times[x], end_times[x]],\n                                [start_times[y], end_times[y]],\n                            ]\n                        )\n                        dimension_2_matrix[x, y] = score\n                        dimension_2_matrix[y, x] = score  # Leverage symmetry\n\n            elif sample_strategy == \"degree_high\":\n\n                for x in range(num_events):\n                    for y in range(x + 1, num_events):\n                        score = degree_scores[x] = degree_scores[y]\n                        dimension_2_matrix[x, y] = score\n                        dimension_2_matrix[y, x] = score  # Leverage symmetry\n\n            elif sample_strategy == \"both\":\n                # park here\n                start_times = self.events_df[\"start_time\"].values\n                end_times = self.events_df[\"end_time\"].values\n                for x in tqdm(range(num_events), desc=\"Processing events\"):\n                    for y in range(x + 1, num_events):\n                        degree_score = degree_scores[x] + degree_scores[y]\n                        temporal_score = self.temporal_close_score(\n                            time_ranges=[\n                                [start_times[x], end_times[x]],\n                                [start_times[y], end_times[y]],\n                            ]\n                        )\n                        score = degree_score + temporal_score\n                        dimension_2_matrix[x, y] = score\n                        dimension_2_matrix[y, x] = score\n\n            # make sure every element in the matrix sum to 1\n            dimension_2_matrix = dimension_2_matrix / dimension_2_matrix.sum()\n\n        with timer(the_logger=logger, message=\"Generating the matrix D3\"):\n            # for dimension 3 generate, first construct a matrix with len(event_df), using numpy\n            dimension_3_matrix = np.zeros((num_events, num_events, num_events))\n            if sample_strategy == \"random\":\n                dimension_3_matrix = np.ones((num_events, num_events, num_events))\n            elif sample_strategy == \"temporal_close\":\n                start_times = self.events_df[\"start_time\"].values\n                end_times = self.events_df[\"end_time\"].values\n\n                for x in tqdm(range(num_events), desc=\"Processing 3D events\"):\n                    for y in range(x + 1, num_events):  # y &gt; x to avoid redundancy\n                        for z in range(y + 1, num_events):  # z &gt; y to avoid redundancy\n                            score = self.temporal_close_score(\n                                time_ranges=[\n                                    [start_times[x], end_times[x]],\n                                    [start_times[y], end_times[y]],\n                                    [start_times[z], end_times[z]],\n                                ]\n                            )\n                            dimension_3_matrix[x, y, z] = score\n                            dimension_3_matrix[x, z, y] = score\n                            dimension_3_matrix[y, x, z] = score\n                            dimension_3_matrix[y, z, x] = score\n                            dimension_3_matrix[z, x, y] = score\n                            dimension_3_matrix[z, y, x] = score\n            elif sample_strategy == \"degree_high\":\n                for x in range(num_events):\n                    for y in range(x + 1, num_events):\n                        for z in range(y + 1, num_events):\n                            score = (\n                                degree_scores[x] + degree_scores[y] + degree_scores[z]\n                            )\n                            dimension_3_matrix[x, y, z] = score\n                            dimension_3_matrix[x, z, y] = score\n                            dimension_3_matrix[y, x, z] = score\n                            dimension_3_matrix[y, z, x] = score\n                            dimension_3_matrix[z, x, y] = score\n                            dimension_3_matrix[z, y, x] = score\n            elif sample_strategy == \"both\":\n                # park here\n                start_times = self.events_df[\"start_time\"].values\n                end_times = self.events_df[\"end_time\"].values\n                for x in tqdm(range(num_events), desc=\"Processing 3D events\"):\n                    for y in range(x + 1, num_events):\n                        for z in range(y + 1, num_events):\n                            degree_score = (\n                                degree_scores[x] + degree_scores[y] + degree_scores[z]\n                            )\n                            temporal_score = self.temporal_close_score(\n                                time_ranges=[\n                                    [start_times[x], end_times[x]],\n                                    [start_times[y], end_times[y]],\n                                    [start_times[z], end_times[z]],\n                                ]\n                            )\n                            score = degree_score + temporal_score\n                            dimension_3_matrix[x, y, z] = score\n                            dimension_3_matrix[x, z, y] = score\n                            dimension_3_matrix[y, x, z] = score\n                            dimension_3_matrix[y, z, x] = score\n                            dimension_3_matrix[z, x, y] = score\n                            dimension_3_matrix[z, y, x] = score\n\n            # make sure every element in the matrix sum to 1\n            dimension_3_matrix = dimension_3_matrix / dimension_3_matrix.sum()\n        # do the sampling based on the matrix\n        # if sampling is a float value, then it means all three dimensions will be sampled based on the rate\n        # if samping is an int value, then it means all three dimension will have that many questions\n        # if sampling is a dict value, then it means the sampling rate for each dimension\n\n        with timer(the_logger=logger, message=\"Sampling the events\"):\n            if isinstance(sample_percentage, float):\n                # sample based on the rate, and the value (weight) is the matrix value\n                dimension_1_samples = np.random.choice(\n                    num_events,\n                    int(num_events * sample_percentage),\n                    p=dimension_1_matrix,\n                )\n                # sample it from dimension 2 matrix\n                dimension_2_samples = self.random_selection(\n                    dimension_2_matrix,\n                    int(dimension_2_matrix.size * sample_percentage),\n                )\n                # sample it from dimension 3 matrix\n                dimension_3_samples = self.random_selection(\n                    dimension_3_matrix,\n                    int(dimension_3_matrix.size * sample_percentage),\n                )\n\n            elif isinstance(sample_percentage, int):\n                dimension_1_samples = np.random.choice(\n                    num_events, sample_percentage, p=dimension_1_matrix\n                )\n                dimension_2_samples = self.random_selection(\n                    dimension_2_matrix,\n                    sample_percentage,\n                )\n                dimension_3_samples = self.random_selection(\n                    dimension_3_matrix,\n                    sample_percentage,\n                )\n\n            elif isinstance(sample_percentage, dict):\n                dimension_1_samples = sample_percentage.get(\"dimension_1\", 0)\n                dimension_2_samples = sample_percentage.get(\"dimension_2\", 0)\n                dimension_3_samples = sample_percentage.get(\"dimension_3\", 0)\n                if (\n                    dimension_1_samples == 0\n                    or dimension_2_samples == 0\n                    or dimension_3_samples == 0\n                ):\n                    raise ValueError(\n                        \"The sample_percentage should have all three dimensions\"\n                    )\n                # if all types of\n                # dimension_1_sample_percentage,\n                # dimension_2_sample_percentage,\n                # dimension_3_sample_percentage are float\n                # then we will sample based on the rate\n                if all(\n                    isinstance(i, float)\n                    for i in [\n                        dimension_1_samples,\n                        dimension_2_samples,\n                        dimension_3_samples,\n                    ]\n                ):\n                    dimension_1_samples = np.random.choice(\n                        len(self.events_df),\n                        int(len(self.events_df) * dimension_1_samples),\n                        p=dimension_1_matrix,\n                    )\n                    dimension_2_samples = self.random_selection(\n                        dimension_2_matrix,\n                        int(dimension_2_matrix.size * dimension_2_samples),\n                    )\n                    dimension_3_samples = self.random_selection(\n                        dimension_3_matrix,\n                        int(dimension_3_matrix.size * dimension_3_samples),\n                    )\n                # if all types of\n                # dimension_1_sample_percentage,\n                # dimension_2_sample_percentage,\n                # dimension_3_sample_percentage are int\n                # then we will sample based on the number\n                elif all(\n                    isinstance(i, int)\n                    for i in [\n                        dimension_1_samples,\n                        dimension_2_samples,\n                        dimension_3_samples,\n                    ]\n                ):\n                    dimension_1_samples = np.random.choice(\n                        len(self.events_df), dimension_1_samples, p=dimension_1_matrix\n                    )\n                    dimension_2_samples = self.random_selection(\n                        dimension_2_matrix,\n                        dimension_2_samples,\n                    )\n                    dimension_3_samples = self.random_selection(\n                        dimension_3_matrix,\n                        dimension_3_samples,\n                    )\n                else:\n                    raise ValueError(\n                        \"The sample_percentage should have all three dimensions\"\n                    )\n            else:\n                raise ValueError(\n                    \"The sample_percentage should be either float, int, or dict\"\n                )\n\n        logger.info(len(dimension_1_samples))\n        logger.info(len(dimension_2_samples))\n        logger.info(len(dimension_3_samples))\n\n        \"\"\"\n        Examples of the output:\n\n        ```\n        [  0  10  20  30  40  50  60  70  80  90 100]\n        [(0, 10), (20, 30), (40, 50), (60, 70), (80, 90), (100, 110)]\n        [(0, 10, 20), (30, 40, 50), (60, 70, 80), (90, 100, 110)]\n        ```\n        \"\"\"\n        self.sample_simple_events = dimension_1_samples\n        self.sample_medium_events = dimension_2_samples\n        self.sample_complex_events = dimension_3_samples\n        return dimension_1_samples, dimension_2_samples, dimension_3_samples\n\n    def temporal_close_score(self, time_ranges: List) -&gt; float:\n        \"\"\"\n        This function will calculate the temporal close score between two/three time ranges.\n\n        range_a = [start_time_a, end_time_a]\n        range_b = [start_time_b, end_time_b]\n        range_c = [start_time_c, end_time_c] (if three ranges are provided)\n\n        score = 1 / ((start_time_b - start_time_a)**2 + (end_time_b - end_time_a)**2)\n        (for two ranges)\n\n        score = 1 / ((start_time_b - start_time_a)**2 + (end_time_b - end_time_a)**2 +\n                     (start_time_c - start_time_a)**2 + (end_time_c - end_time_a)**2)\n        (for three ranges)\n\n        Args:\n            time_ranges (List[Tuple[datetime, datetime]]): The list of time ranges\n\n        Returns:\n            temporal_close_score (float): The temporal close score between two/three time ranges, if it is close\n            to 1, it means the time ranges are close to each other, if it is close to 0, it means the time ranges\n            are far from each other\n        \"\"\"\n        if len(time_ranges) not in [2, 3]:\n            raise ValueError(\"The function only supports two or three time ranges\")\n\n        # Utility function to convert string to datetime\n        start_time_a_dt, end_time_a_dt = self.util_str_to_datetime(time_ranges[0])\n        start_time_b_dt, end_time_b_dt = self.util_str_to_datetime(time_ranges[1])\n\n        # Calculate the differences in days\n        start_diff_days = (start_time_b_dt - start_time_a_dt) / np.timedelta64(1, \"D\")\n        end_diff_days = (end_time_b_dt - end_time_a_dt) / np.timedelta64(1, \"D\")\n\n        # Calculate the score using days difference\n        score = start_diff_days**2 + end_diff_days**2\n\n        if len(time_ranges) == 3:\n            start_time_c_dt, end_time_c_dt = self.util_str_to_datetime(time_ranges[2])\n            start_diff_days_c = (start_time_c_dt - start_time_a_dt) / np.timedelta64(\n                1, \"D\"\n            )\n            end_diff_days_c = (end_time_c_dt - end_time_a_dt) / np.timedelta64(1, \"D\")\n            score += start_diff_days_c**2 + end_diff_days_c**2\n        if score == 0:\n            return 0\n        return 1 / score\n\n    @staticmethod\n    def calculate_degree_scores(event_df: pd.DataFrame):\n        \"\"\"\n        group by subject, each subject will have a degree\n        group by object, each object will have a degree\n        merge subject_degree and object_degree back to the event_df, it will have extract two columns\n        - subject_degree\n        - object_degree\n        then we will calculate the degree score based on the two columns\n\n        In this way, we have the degree score for each event\n\n        Args:\n            event_df (pd.DataFrame): The dataframe of the events\n\n        Returns:\n            Tuple: The tuple of the degree scores\n        \"\"\"\n\n        subject_degree_df = (\n            event_df.groupby(\"subject\").size().reset_index(name=\"subject_degree\")\n        )\n        object_degree_df = (\n            event_df.groupby(\"object\").size().reset_index(name=\"object_degree\")\n        )\n\n        event_df = event_df.merge(subject_degree_df, on=\"subject\", how=\"left\")\n        event_df = event_df.merge(object_degree_df, on=\"object\", how=\"left\")\n\n        event_df[\"degree_score\"] = (\n            event_df[\"subject_degree\"] + event_df[\"object_degree\"]\n        )\n        # show the summary of the degree score\n        logger.debug(event_df[\"degree_score\"].describe())\n\n        # flat the score to 1,2,3,4 based on the quartile\n        event_df[\"degree_score\"] += np.random.normal(\n            0, 1e-5, size=event_df[\"degree_score\"].shape\n        )\n        event_df[\"degree_score\"] = pd.qcut(event_df[\"degree_score\"], q=4, labels=False)\n        event_df[\"degree_score\"] = event_df[\"degree_score\"] + 1\n\n        # normalize the score to 0-1\n        event_df[\"degree_score\"] = event_df[\"degree_score\"] / 4\n\n        return event_df[\"degree_score\"].values\n\n    @staticmethod\n    def random_selection(matrix, sample_num):\n\n        if sample_num &gt; matrix.size:\n            raise ValueError(\n                \"The sample number should be less than the length of the matrix\"\n            )\n\n        if matrix.shape[0] != matrix.shape[1]:\n            raise ValueError(\"The matrix should be a square matrix\")\n        # if it is one dimension, then we will sample based on the rate\n        if matrix.shape[0] == 1:\n            raise ValueError(\"The matrix should be at least two dimensions\")\n\n        flatten_matrix = matrix.flatten()\n        sample_indices = np.random.choice(\n            np.arange(len(flatten_matrix)),\n            size=sample_num,\n            replace=False,\n            p=flatten_matrix,\n        )\n        samples = [np.unravel_index(i, matrix.shape) for i in sample_indices]\n        return samples\n</code></pre>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.bulk_insert","title":"<code>bulk_insert(values)</code>","text":"<p>This function will insert the values into the table</p> Source code in <code>TimelineKGQA/generator.py</code> <pre><code>def bulk_insert(self, values: List[Tuple]):\n    \"\"\"\n    This function will insert the values into the table\n\n    \"\"\"\n    values_str = \",\\n\".join([\"(%s, %s, %s, %s, %s, %s, %s, %s, %s)\"] * len(values))\n    # Flatten the list of values tuples into a single tuple for execution\n    flat_values = [item for sublist in values for item in sublist]\n\n    bulk_insert_query = f\"\"\"\n    INSERT INTO {self.unified_kg_table_questions} (\n                                                source_kg_id,\n                                                  question,\n                                                  answer,\n                                                  paraphrased_question,\n                                                  events,\n                                                  question_level,\n                                                  question_type,\n                                                  answer_type,\n                                                  temporal_relation\n                                                  )\n    VALUES {values_str}\n    \"\"\"\n    if len(values) == 0:\n        return\n    # Execute the bulk insert command\n    try:\n        self.cursor.execute(bulk_insert_query, flat_values)\n        self.connection.commit()\n        logger.info(f\"Successfully inserted {len(values)} rows into the table.\")\n    except Exception as e:\n        logger.exception(f\"Error: {e}\")\n        logger.info(flat_values)\n</code></pre>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.calculate_degree_scores","title":"<code>calculate_degree_scores(event_df)</code>  <code>staticmethod</code>","text":"<p>group by subject, each subject will have a degree group by object, each object will have a degree merge subject_degree and object_degree back to the event_df, it will have extract two columns - subject_degree - object_degree then we will calculate the degree score based on the two columns</p> <p>In this way, we have the degree score for each event</p> <p>Parameters:</p> Name Type Description Default <code>event_df</code> <code>DataFrame</code> <p>The dataframe of the events</p> required <p>Returns:</p> Name Type Description <code>Tuple</code> <p>The tuple of the degree scores</p> Source code in <code>TimelineKGQA/generator.py</code> <pre><code>@staticmethod\ndef calculate_degree_scores(event_df: pd.DataFrame):\n    \"\"\"\n    group by subject, each subject will have a degree\n    group by object, each object will have a degree\n    merge subject_degree and object_degree back to the event_df, it will have extract two columns\n    - subject_degree\n    - object_degree\n    then we will calculate the degree score based on the two columns\n\n    In this way, we have the degree score for each event\n\n    Args:\n        event_df (pd.DataFrame): The dataframe of the events\n\n    Returns:\n        Tuple: The tuple of the degree scores\n    \"\"\"\n\n    subject_degree_df = (\n        event_df.groupby(\"subject\").size().reset_index(name=\"subject_degree\")\n    )\n    object_degree_df = (\n        event_df.groupby(\"object\").size().reset_index(name=\"object_degree\")\n    )\n\n    event_df = event_df.merge(subject_degree_df, on=\"subject\", how=\"left\")\n    event_df = event_df.merge(object_degree_df, on=\"object\", how=\"left\")\n\n    event_df[\"degree_score\"] = (\n        event_df[\"subject_degree\"] + event_df[\"object_degree\"]\n    )\n    # show the summary of the degree score\n    logger.debug(event_df[\"degree_score\"].describe())\n\n    # flat the score to 1,2,3,4 based on the quartile\n    event_df[\"degree_score\"] += np.random.normal(\n        0, 1e-5, size=event_df[\"degree_score\"].shape\n    )\n    event_df[\"degree_score\"] = pd.qcut(event_df[\"degree_score\"], q=4, labels=False)\n    event_df[\"degree_score\"] = event_df[\"degree_score\"] + 1\n\n    # normalize the score to 0-1\n    event_df[\"degree_score\"] = event_df[\"degree_score\"] / 4\n\n    return event_df[\"degree_score\"].values\n</code></pre>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.complex_question_generation","title":"<code>complex_question_generation()</code>","text":"<p>This is to generate the complex question, which will involve three events and timeline</p> <ul> <li>Type 1: Before Bush, after Kennedy, who is the president of US?</li> <li>Type 2: Who is the first president of US among Bush, Kennedy, and Obama?</li> </ul> Source code in <code>TimelineKGQA/generator.py</code> <pre><code>def complex_question_generation(self):\n    \"\"\"\n    This is to generate the complex question, which will involve three events and timeline\n\n    - Type 1: Before Bush, after Kennedy, who is the president of US?\n    - Type 2: Who is the first president of US among Bush, Kennedy, and Obama?\n    \"\"\"\n\n    # next step is to construct three events\n    insert_values_list = []\n    bulk_sql_pointer = 0\n\n    for item in self.sample_complex_events:\n        first_event = self.events_df.iloc[item[0]]\n        second_event = self.events_df.iloc[item[1]]\n        third_event = self.events_df.iloc[item[2]]\n        source_kg_id = (\n            first_event[\"id\"] * 1000000 * 1000000\n            + second_event[\"id\"] * 1000000\n            + third_event[\"id\"]\n        )\n\n        questions = self.complex_question_generation_individual(\n            first_event=first_event.to_dict(),\n            second_event=second_event.to_dict(),\n            third_event=third_event.to_dict(),\n            template_based=True,\n            paraphrased=self.paraphrased,\n        )\n        for question_obj in questions:\n            question_obj[\"source_kg_id\"] = int(source_kg_id)\n            # get dict to tuple, sequence should be the same as the sql command\n            data = (\n                question_obj[\"source_kg_id\"],\n                question_obj[\"question\"],\n                question_obj[\"answer\"],\n                question_obj[\"paraphrased_question\"],\n                question_obj[\"events\"],\n                question_obj[\"question_level\"],\n                question_obj[\"question_type\"],\n                question_obj[\"answer_type\"],\n                question_obj[\"temporal_relation\"],\n            )\n            insert_values_list.append(data)\n            bulk_sql_pointer += 1\n            if bulk_sql_pointer % self.bulk_sql_size == 0:\n                self.bulk_insert(insert_values_list)\n                insert_values_list = []\n\n    self.bulk_insert(insert_values_list)\n</code></pre>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.complex_question_generation_individual","title":"<code>complex_question_generation_individual(first_event, second_event, third_event, template_based=True, paraphrased=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>first_event</code> <code>dict</code> <p>The first event</p> required <code>second_event</code> <code>dict</code> <p>The second event</p> required <code>third_event</code> <code>dict</code> <p>The third event</p> required <code>template_based</code> <code>bool</code> <p>Whether you use the template based question generation</p> <code>True</code> <code>paraphrased</code> <code>bool</code> <p>Whether you do the paraphrase for the question, if set to False,     then the paraphrased_question will be the same as the question</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>List[dict]</code> <p>The generated questions - question - answer - paraphrased_question - events - question_level: Complex - question_type: The type of the question - answer_type: The type of the answer</p> <ul> <li>question_type:<ul> <li>timeline_position_retrieval *2 + temporal constrained retrieval</li> <li>timeline_position_retrieval *3</li> </ul> </li> <li>answer_type:<ul> <li>type1:<ul> <li>subject<ul> <li>trel b, trel c, ? predicate object</li> </ul> </li> <li>object<ul> <li>trel b, trel c, subject predicate ?</li> </ul> </li> </ul> </li> <li>type2:<ul> <li>Infer a new time range: Union/Intersection<ul> <li>trel b, trel c, from when to when the subject predicate object? (intersection)</li> <li>within (trel b, trel c), who is the subject predicate object? (union)</li> </ul> </li> <li>Infer a temporal relation: Allen<ul> <li>? More making sense one is ranking</li> <li>hard to justify the question that</li> <li>If we ask for choice question, it will be between two events</li> <li>If we ask for true/false, event a,b,c; ab, ac, bc;  Question, ab+ac =&gt; is bc relation True</li> </ul> </li> <li>Infer a list of time ranges: Ranking<ul> <li>Who is the {} among a,b,c? =&gt; an</li> </ul> </li> <li>Infer duration, and then compare<ul> <li>Who is the president of US for the longest time among a, b, c? =&gt; a</li> </ul> </li> </ul> </li> </ul> </li> </ul> Source code in <code>TimelineKGQA/generator.py</code> <pre><code>def complex_question_generation_individual(\n    self,\n    first_event: dict,\n    second_event: dict,\n    third_event: dict,\n    template_based: bool = True,\n    paraphrased: bool = True,\n) -&gt; List[dict]:\n    \"\"\"\n    Args:\n        first_event (dict): The first event\n        second_event (dict): The second event\n        third_event (dict): The third event\n        template_based (bool): Whether you use the template based question generation\n        paraphrased (bool): Whether you do the paraphrase for the question, if set to False,\n                then the paraphrased_question will be the same as the question\n\n    Returns:\n        dict: The generated questions\n            - question\n            - answer\n            - paraphrased_question\n            - events\n            - question_level: Complex\n            - question_type: The type of the question\n            - answer_type: The type of the answer\n\n\n    - question_type:\n        - timeline_position_retrieval *2 + temporal constrained retrieval\n        - timeline_position_retrieval *3\n    - answer_type:\n        - type1:\n            - subject\n                - trel b, trel c, ? predicate object\n            - object\n                - trel b, trel c, subject predicate ?\n        - type2:\n            - Infer a new time range: Union/Intersection\n                - trel b, trel c, from when to when the subject predicate object? (intersection)\n                - within (trel b, trel c), who is the subject predicate object? (union)\n            - Infer a temporal relation: Allen\n                - ? More making sense one is ranking\n                - hard to justify the question that\n                - If we ask for choice question, it will be between two events\n                - If we ask for true/false, event a,b,c; ab, ac, bc;  Question, ab+ac =&gt; is bc relation True\n            - Infer a list of time ranges: Ranking\n                - Who is the {} among a,b,c? =&gt; an\n            - Infer duration, and then compare\n                - Who is the president of US for the longest time among a, b, c? =&gt; a\n\n    \"\"\"\n\n    first_event_subject = first_event[\"subject\"]\n    first_event_predicate = first_event[\"predicate\"]\n    first_event_object = first_event[\"object\"]\n    first_event_start_time = first_event[\"start_time\"]\n    first_event_end_time = first_event[\"end_time\"]\n\n    second_event_subject = second_event[\"subject\"]\n    second_event_predicate = second_event[\"predicate\"]\n    second_event_object = second_event[\"object\"]\n    second_event_start_time = second_event[\"start_time\"]\n    second_event_end_time = second_event[\"end_time\"]\n\n    third_event_subject = third_event[\"subject\"]\n    third_event_predicate = third_event[\"predicate\"]\n    third_event_object = third_event[\"object\"]\n    third_event_start_time = third_event[\"start_time\"]\n    third_event_end_time = third_event[\"end_time\"]\n\n    first_event_start_time_dt, first_event_end_time_dt = self.util_str_to_datetime(\n        [first_event_start_time, first_event_end_time]\n    )\n    second_event_start_time_dt, second_event_end_time_dt = (\n        self.util_str_to_datetime([second_event_start_time, second_event_end_time])\n    )\n    third_event_start_time_dt, third_event_end_time_dt = self.util_str_to_datetime(\n        [third_event_start_time, third_event_end_time]\n    )\n\n    # first generate\n    complex_type_1_a_questions = []\n    questions = []\n\n    # timeline_position_retrieval *2 + temporal constrained retrieval\n    # ask for the first subject\n    complex_type_1_a_questions.append(\n        {\n            \"question\": f\"??? {first_event_predicate} {first_event_object} {second_event_predicate} \"\n            f\"{second_event_object} {third_event_predicate} {third_event_object}?\",\n            \"answer\": f\"{first_event_subject}\",\n            \"paraphrased_question\": None,\n            \"events\": [\n                f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                f\"{first_event_start_time}|{first_event_end_time}\",\n                f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                f\"{second_event_start_time}|{second_event_end_time}\",\n                f\"{third_event_subject}|{third_event_predicate}|{third_event_object}|\"\n                f\"{third_event_start_time}|{third_event_end_time}\",\n            ],\n            \"question_level\": \"complex\",\n            \"question_type\": \"timeline_position_retrieval*2+temporal_constrained_retrieval\",\n            \"answer_type\": \"subject\",\n            \"temporal_relation\": None,\n        }\n    )\n    # ask for first object\n    complex_type_1_a_questions.append(\n        {\n            \"question\": f\"{first_event_subject} {first_event_predicate} ??? {second_event_predicate} \"\n            f\"{second_event_object} {third_event_predicate} {third_event_object}?\",\n            \"answer\": f\"{first_event_object}\",\n            \"paraphrased_question\": None,\n            \"events\": [\n                f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                f\"{first_event_start_time}|{first_event_end_time}\",\n                f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                f\"{second_event_start_time}|{second_event_end_time}\",\n                f\"{third_event_subject}|{third_event_predicate}|{third_event_object}|\"\n                f\"{third_event_start_time}|{third_event_end_time}\",\n            ],\n            \"question_level\": \"complex\",\n            \"question_type\": \"timeline_position_retrieval*2+temporal_constrained_retrieval\",\n            \"answer_type\": \"object\",\n            \"temporal_relation\": None,\n        }\n    )\n\n    questions += complex_type_1_a_questions\n\n    \"\"\"\n    For duration before, duration after type question\n    \"\"\"\n\n    complex_type_1_b_questions = []\n    # this will be added later when we process the questions with template\n\n    \"\"\"\n    Timeline Position Retrieval + Timeline Position Retrieval + Timeline Position Retrieval\n    \"\"\"\n\n    complex_type_2_questions = [\n        {\n            \"question\": f\"{first_event_subject} {first_event_predicate} {first_event_object} \"\n            f\"{second_event_predicate} {second_event_object} \"\n            f\"{third_event_predicate} {third_event_object}?\",\n            \"answer\": \"Union/Intersection of the time range\",\n            \"paraphrased_question\": None,\n            \"events\": [\n                f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                f\"{first_event_start_time}|{first_event_end_time}\",\n                f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                f\"{second_event_start_time}|{second_event_end_time}\",\n                f\"{third_event_subject}|{third_event_predicate}|{third_event_object}|\"\n                f\"{third_event_start_time}|{third_event_end_time}\",\n            ],\n            \"question_level\": \"complex\",\n            \"question_type\": \"timeline_position_retrieval*3\",\n            \"answer_type\": \"relation_union_or_intersection\",\n            \"temporal_relation\": \"intersection\",\n        },\n        {\n            \"question\": f\"{first_event_subject} {first_event_predicate} {first_event_object} \"\n            f\"{second_event_predicate} {second_event_object} \"\n            f\"{third_event_predicate} {third_event_object}?\",\n            \"answer\": \"Union/Intersection of the time range\",\n            \"paraphrased_question\": None,\n            \"events\": [\n                f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                f\"{first_event_start_time}|{first_event_end_time}\",\n                f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                f\"{second_event_start_time}|{second_event_end_time}\",\n                f\"{third_event_subject}|{third_event_predicate}|{third_event_object}|\"\n                f\"{third_event_start_time}|{third_event_end_time}\",\n            ],\n            \"question_level\": \"complex\",\n            \"question_type\": \"timeline_position_retrieval*3\",\n            \"answer_type\": \"relation_union_or_intersection\",\n            \"temporal_relation\": \"union\",\n        },\n        {\n            \"question\": f\"{first_event_subject} {first_event_predicate} {first_event_object} \"\n            f\"{second_event_predicate} {second_event_object} \"\n            f\"{third_event_predicate} {third_event_object}?\",\n            \"answer\": \"Duration\",\n            \"paraphrased_question\": None,\n            \"events\": [\n                f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                f\"{first_event_start_time}|{first_event_end_time}\",\n                f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                f\"{second_event_start_time}|{second_event_end_time}\",\n                f\"{third_event_subject}|{third_event_predicate}|{third_event_object}|\"\n                f\"{third_event_start_time}|{third_event_end_time}\",\n            ],\n            \"question_level\": \"complex\",\n            \"question_type\": \"timeline_position_retrieval*3\",\n            \"answer_type\": \"relation_duration\",\n            \"temporal_relation\": None,\n        },\n        # add ranking one\n        {\n            \"question\": f\"Who is the xxx among {first_event_subject}, {second_event_subject}, \"\n            f\"and {third_event_subject}?\",\n            \"answer\": \"Ranking\",\n            \"paraphrased_question\": None,\n            \"events\": [\n                f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                f\"{first_event_start_time}|{first_event_end_time}\",\n                f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                f\"{second_event_start_time}|{second_event_end_time}\",\n                f\"{third_event_subject}|{third_event_predicate}|{third_event_object}|\"\n                f\"{third_event_start_time}|{third_event_end_time}\",\n            ],\n            \"question_level\": \"complex\",\n            \"question_type\": \"timeline_position_retrieval*3\",\n            \"answer_type\": \"relation_ranking\",\n            \"temporal_relation\": \"ranking\",\n        },\n    ]\n\n    questions += complex_type_2_questions\n    temporal_answer = None\n\n    if template_based:\n        for question_draft in questions:\n            this_type_templates = QUESTION_TEMPLATES[\n                question_draft[\"question_level\"]\n            ][question_draft[\"question_type\"]][question_draft[\"answer_type\"]]\n\n            if (\n                question_draft[\"answer_type\"] == \"subject\"\n                or question_draft[\"answer_type\"] == \"object\"\n            ):\n                \"\"\"\n                Handle the Complex Type 1 Questions here:\n                a,b,c, will ask for a information.\n                Get temporal_relation_12, temporal_relation_13, then generate the question\n                \"\"\"\n                temporal_relation_12 = self.relation_allen_time_range(\n                    time_range_a=[\n                        first_event_start_time_dt,\n                        first_event_end_time_dt,\n                    ],\n                    time_range_b=[\n                        second_event_start_time_dt,\n                        second_event_end_time_dt,\n                    ],\n                )\n                temporal_relation_13 = self.relation_allen_time_range(\n                    time_range_a=[\n                        first_event_start_time_dt,\n                        first_event_end_time_dt,\n                    ],\n                    time_range_b=[\n                        third_event_start_time_dt,\n                        third_event_end_time_dt,\n                    ],\n                )\n\n                temporal_relation_12_semantic = temporal_relation_12.get(\"semantic\")\n                temporal_relation_13_semantic = temporal_relation_13.get(\"semantic\")\n                question_draft[\"temporal_relation\"] = (\n                    f\"{temporal_relation_12['relation']}&amp;{temporal_relation_13['relation']}\"\n                )\n                random_pick_template = random.choice(this_type_templates)\n\n                question_draft[\"question\"] = random_pick_template.format(\n                    first_event_subject=first_event_subject,\n                    first_event_predicate=first_event_predicate,\n                    first_event_object=first_event_object,\n                    temporal_relation_12=temporal_relation_12_semantic,\n                    second_event_subject=second_event_subject,\n                    second_event_predicate=second_event_predicate,\n                    second_event_object=second_event_object,\n                    temporal_relation_13=temporal_relation_13_semantic,\n                    third_event_subject=third_event_subject,\n                    third_event_predicate=third_event_predicate,\n                    third_event_object=third_event_object,\n                )\n\n                # this will generate the basic temporal relation questions.\n                # then we will want to generate the duration_before, duration_after\n                can_generate_duration_question = False\n                if temporal_relation_12_semantic in [\"before\", \"after\"]:\n                    duration = self.relation_duration_calculation(\n                        time_range_a=[\n                            first_event_start_time_dt,\n                            first_event_end_time_dt,\n                        ],\n                        time_range_b=[\n                            second_event_start_time_dt,\n                            second_event_end_time_dt,\n                        ],\n                        temporal_operator=f\"duration_{temporal_relation_12_semantic}\",\n                    )\n                    temporal_relation_12_semantic = (\n                        f\"{duration} {temporal_relation_12_semantic}\"\n                    )\n                    logger.debug(temporal_relation_12_semantic)\n                    can_generate_duration_question = True\n                if temporal_relation_13_semantic in [\"before\", \"after\"]:\n                    duration = self.relation_duration_calculation(\n                        time_range_a=[\n                            first_event_start_time_dt,\n                            first_event_end_time_dt,\n                        ],\n                        time_range_b=[\n                            third_event_start_time_dt,\n                            third_event_end_time_dt,\n                        ],\n                        temporal_operator=f\"duration_{temporal_relation_13_semantic}\",\n                    )\n                    temporal_relation_13_semantic = (\n                        f\"{duration} {temporal_relation_13_semantic}\"\n                    )\n                    logger.debug(temporal_relation_13_semantic)\n                    can_generate_duration_question = True\n                if can_generate_duration_question:\n                    # copy a new question draft\n                    duration_question_draft = copy.deepcopy(question_draft)\n                    duration_question_draft[\"question\"] = (\n                        random_pick_template.format(\n                            first_event_subject=first_event_subject,\n                            first_event_predicate=first_event_predicate,\n                            first_event_object=first_event_object,\n                            temporal_relation_12=temporal_relation_12_semantic,\n                            second_event_subject=second_event_subject,\n                            second_event_predicate=second_event_predicate,\n                            second_event_object=second_event_object,\n                            temporal_relation_13=temporal_relation_13_semantic,\n                            third_event_subject=third_event_subject,\n                            third_event_predicate=third_event_predicate,\n                            third_event_object=third_event_object,\n                        )\n                    )\n                    duration_question_draft[\"temporal_relation\"] = (\n                        f\"duration_{temporal_relation_12_semantic}&amp;duration_{temporal_relation_13_semantic}\"\n                    )\n                    complex_type_1_b_questions.append(duration_question_draft)\n            else:\n                # handle the Timeline Position Retrieval + Timeline Position Retrieval + Timeline Position Retrieval\n                if (\n                    question_draft[\"answer_type\"]\n                    == \"relation_union_or_intersection\"\n                ):\n                    temporal_relation = question_draft[\"temporal_relation\"]\n                    random_pick_template = random.choice(\n                        this_type_templates[temporal_relation]\n                    )\n                    temporal_answer = self.relation_union_or_intersection(\n                        time_ranges=[\n                            (first_event_start_time_dt, first_event_end_time_dt),\n                            (second_event_start_time_dt, second_event_end_time_dt),\n                            (third_event_start_time_dt, third_event_end_time_dt),\n                        ],\n                        temporal_operator=temporal_relation,\n                    )\n\n                    question_draft[\"question\"] = random_pick_template.format(\n                        first_event_subject=first_event_subject,\n                        first_event_predicate=first_event_predicate,\n                        first_event_object=first_event_object,\n                        second_event_subject=second_event_subject,\n                        second_event_predicate=second_event_predicate,\n                        second_event_object=second_event_object,\n                        third_event_subject=third_event_subject,\n                        third_event_predicate=third_event_predicate,\n                        third_event_object=third_event_object,\n                    )\n                    logger.debug(question_draft[\"question\"])\n                    logger.debug(temporal_answer)\n                    if temporal_answer is None:\n                        temporal_answer = \"No Answer\"\n                    question_draft[\"answer\"] = temporal_answer\n                elif question_draft[\"answer_type\"] == \"relation_duration\":\n                    \"\"\"\n                    There are four types in this category\n                    - duration =&gt; which is the intersection of the two time range\n                    - duration_compare =&gt; longer shorter equal\n                    - sum =&gt; total duration of the two time range, which is actually the union\n                    - average =&gt; average duration of the two time range\n                    \"\"\"\n                    temporal_relation = random.choice(\n                        [\n                            \"duration\",\n                            \"duration_compare\",\n                            \"sum\",\n                            \"average\",\n                        ]\n                    )\n                    random_pick_template = random.choice(\n                        this_type_templates[temporal_relation]\n                    )\n                    question_draft[\"temporal_relation\"] = temporal_relation\n                    if temporal_relation == \"duration\":\n                        temporal_answer = self.relation_union_or_intersection(\n                            time_ranges=[\n                                (\n                                    first_event_start_time_dt,\n                                    first_event_end_time_dt,\n                                ),\n                                (\n                                    second_event_start_time_dt,\n                                    second_event_end_time_dt,\n                                ),\n                                (\n                                    third_event_start_time_dt,\n                                    third_event_end_time_dt,\n                                ),\n                            ],\n                            temporal_operator=\"intersection\",\n                        )\n                        question_draft[\"question\"] = random_pick_template.format(\n                            first_event_subject=first_event_subject,\n                            first_event_predicate=first_event_predicate,\n                            first_event_object=first_event_object,\n                            second_event_subject=second_event_subject,\n                            second_event_predicate=second_event_predicate,\n                            second_event_object=second_event_object,\n                            third_event_subject=third_event_subject,\n                            third_event_predicate=third_event_predicate,\n                            third_event_object=third_event_object,\n                        )\n                    elif temporal_relation == \"duration_compare\":\n                        # we do duration ranking here\n                        duration_rank_by_index = self.relation_duration(\n                            time_ranges=[\n                                [\n                                    first_event_start_time_dt,\n                                    first_event_end_time_dt,\n                                ],\n                                [\n                                    second_event_start_time_dt,\n                                    second_event_end_time_dt,\n                                ],\n                                [\n                                    third_event_start_time_dt,\n                                    third_event_end_time_dt,\n                                ],\n                            ],\n                            agg_temporal_operator=\"ranking\",\n                        )\n                        logger.debug(duration_rank_by_index)\n                        temporal_answer = duration_rank_by_index[0] + 1\n                        question_draft[\"question\"] = random_pick_template.format(\n                            first_event_subject=first_event_subject,\n                            first_event_predicate=first_event_predicate,\n                            first_event_object=first_event_object,\n                            second_event_subject=second_event_subject,\n                            second_event_predicate=second_event_predicate,\n                            second_event_object=second_event_object,\n                            third_event_subject=third_event_subject,\n                            third_event_predicate=third_event_predicate,\n                            third_event_object=third_event_object,\n                            temporal_duration_rank=temporal_answer,\n                        )\n                    elif temporal_relation == \"sum\":\n                        temporal_answer = self.util_average_duration_calculation(\n                            time_ranges=[\n                                [\n                                    first_event_start_time_dt,\n                                    first_event_end_time_dt,\n                                ],\n                                [\n                                    second_event_start_time_dt,\n                                    second_event_end_time_dt,\n                                ],\n                                [\n                                    third_event_start_time_dt,\n                                    third_event_end_time_dt,\n                                ],\n                            ],\n                            temporal_operator=\"sum\",\n                        )\n                        question_draft[\"question\"] = random_pick_template.format(\n                            first_event_subject=first_event_subject,\n                            first_event_predicate=first_event_predicate,\n                            first_event_object=first_event_object,\n                            second_event_subject=second_event_subject,\n                            second_event_predicate=second_event_predicate,\n                            second_event_object=second_event_object,\n                            third_event_subject=third_event_subject,\n                            third_event_predicate=third_event_predicate,\n                            third_event_object=third_event_object,\n                        )\n                    elif temporal_relation == \"average\":\n                        temporal_answer = self.util_average_duration_calculation(\n                            time_ranges=[\n                                [\n                                    first_event_start_time_dt,\n                                    first_event_end_time_dt,\n                                ],\n                                [\n                                    second_event_start_time_dt,\n                                    second_event_end_time_dt,\n                                ],\n                                [\n                                    third_event_start_time_dt,\n                                    third_event_end_time_dt,\n                                ],\n                            ],\n                            temporal_operator=\"average\",\n                        )\n                        logger.debug(temporal_answer)\n                        question_draft[\"question\"] = random_pick_template.format(\n                            first_event_subject=first_event_subject,\n                            first_event_predicate=first_event_predicate,\n                            first_event_object=first_event_object,\n                            second_event_subject=second_event_subject,\n                            second_event_predicate=second_event_predicate,\n                            second_event_object=second_event_object,\n                            third_event_subject=third_event_subject,\n                            third_event_predicate=third_event_predicate,\n                            third_event_object=third_event_object,\n                        )\n                    question_draft[\"answer\"] = temporal_answer\n                    question_draft[\"temporal_relation\"] = temporal_relation\n                elif question_draft[\"answer_type\"] == \"relation_ranking\":\n                    # random select, ranking based on start time or end time\n                    rank_by_what = random.choice(\n                        [\"rank_start_time\", \"rank_end_time\"]\n                    )\n                    rank_by_index = self.relation_ordinal_time_range(\n                        time_ranges=[\n                            [first_event_start_time_dt, first_event_end_time_dt],\n                            [second_event_start_time_dt, second_event_end_time_dt],\n                            [third_event_start_time_dt, third_event_end_time_dt],\n                        ],\n                        agg_temporal_operator=rank_by_what,\n                    )\n\n                    random_pick_template = random.choice(\n                        this_type_templates[rank_by_what]\n                    )\n                    question_draft[\"question\"] = random_pick_template.format(\n                        first_event_subject=first_event_subject,\n                        first_event_predicate=first_event_predicate,\n                        first_event_object=first_event_object,\n                        second_event_subject=second_event_subject,\n                        second_event_predicate=second_event_predicate,\n                        second_event_object=second_event_object,\n                        third_event_subject=third_event_subject,\n                        third_event_predicate=third_event_predicate,\n                        third_event_object=third_event_object,\n                    )\n                    temporal_answer = rank_by_index[0] + 1\n                    question_draft[\"answer\"] = temporal_answer\n                    question_draft[\"temporal_relation\"] = rank_by_what\n\n    questions += complex_type_1_b_questions\n    if paraphrased:\n        for question_obj in questions:\n            paraphrased_question = paraphrase_medium_question(\n                question=question_obj[\"question\"],\n            )\n            logger.info(f\"paraphrased_question: {paraphrased_question}\")\n            question_obj[\"paraphrased_question\"] = paraphrased_question\n\n    return questions\n</code></pre>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.medium_question_generation","title":"<code>medium_question_generation()</code>","text":"<p>This will involve mainly two types of questions</p> <ul> <li>Timeline Position Retrival =&gt; Temporal Constrained Retrieval</li> <li>Timeline Position Retrival + Timeline Position Retrival</li> </ul> <ul> <li>question_level: medium</li> <li>question_type:<ul> <li>timeline_recovery_temporal_constrained_retrieval</li> <li>timeline_recovery_timeline_recovery</li> </ul> </li> <li>answer_type:<ul> <li>entity:<ul> <li>subject:</li> <li>object</li> </ul> </li> <li>temporal related<ul> <li>Infer a new time range: Union/Intersection</li> <li>Infer a temporal relation: Allen</li> <li>Infer duration, and then compare</li> <li>Note: Ranking will be the same as Allen, so it will be in Complex level</li> </ul> </li> </ul> </li> </ul> Source code in <code>TimelineKGQA/generator.py</code> <pre><code>def medium_question_generation(self):\n    \"\"\"\n    This will involve mainly two types of questions\n\n    - **Timeline Position Retrival =&gt; Temporal Constrained Retrieval**\n    - **Timeline Position Retrival + Timeline Position Retrival**\n\n    ---\n\n    - question_level: medium\n    - question_type:\n        - timeline_recovery_temporal_constrained_retrieval\n        - timeline_recovery_timeline_recovery\n    - answer_type:\n        - entity:\n            - subject:\n            - object\n        - temporal related\n            - Infer a new time range: Union/Intersection\n            - Infer a temporal relation: Allen\n            - Infer duration, and then compare\n            - Note: Ranking will be the same as Allen, so it will be in **Complex** level\n\n    \"\"\"\n\n    insert_values_list = []\n    bulk_sql_pointer = 0\n\n    for item in self.sample_medium_events:\n        first_event = self.events_df.iloc[item[0]]\n        second_event = self.events_df.iloc[item[1]]\n\n        source_kg_id = first_event[\"id\"] * 1000000 + second_event[\"id\"]\n        logger.debug(f\"Generating question for source_kg_id: {source_kg_id}\")\n        questions = self.medium_question_generation_individual(\n            first_event=first_event.to_dict(),\n            second_event=second_event.to_dict(),\n            template_based=True,\n            paraphrased=self.paraphrased,\n        )\n\n        for question_obj in questions:\n\n            question_obj[\"source_kg_id\"] = int(source_kg_id)\n            # get dict to tuple, sequence should be the same as the sql command\n            data = (\n                question_obj[\"source_kg_id\"],\n                question_obj[\"question\"],\n                question_obj[\"answer\"],\n                question_obj[\"paraphrased_question\"],\n                question_obj[\"events\"],\n                question_obj[\"question_level\"],\n                question_obj[\"question_type\"],\n                question_obj[\"answer_type\"],\n                question_obj[\"temporal_relation\"],\n            )\n            insert_values_list.append(data)\n            bulk_sql_pointer += 1\n            if bulk_sql_pointer % self.bulk_sql_size == 0:\n                self.bulk_insert(values=insert_values_list)\n                insert_values_list = []\n    self.bulk_insert(values=insert_values_list)\n</code></pre>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.medium_question_generation_individual","title":"<code>medium_question_generation_individual(first_event, second_event, template_based=True, paraphrased=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>first_event</code> <code>dict</code> <p>The first event</p> required <code>second_event</code> <code>dict</code> <p>The second event</p> required <code>template_based</code> <code>bool</code> <p>Whether you use the template based question generation</p> <code>True</code> <code>paraphrased</code> <code>bool</code> <p>Whether you do the paraphrase for the question, if set to False,     then the paraphrased_question will be the same as the question</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>List[dict]</code> <p>The generated questions - question - answer - paraphrased_question - events - question_level: Medium - question_type: The type of the question - answer_type: The type of the answer</p> <ul> <li>question_type:<ul> <li>timeline_position_retrieval_temporal_constrained_retrieval<ul> <li>For this one, the logic/reasoning/math part will be like: TimeRange +     Temporal Semantic Operation =&gt; TimeRange</li> <li>Then the interesting part will be the Timeline Operation,     we have mentioned several types of operations below.<ul> <li>There are mostly from numeric to semantic perspective</li> <li>Here is the reverse process: name it Temporal Semantic Operation</li> <li>So this is trying to convert the temporal semantic representation to     a numeric operation and then get a new operation.</li> </ul> </li> </ul> </li> <li>timeline_position_retrieval_timeline_position_retrieval<ul> <li>For the logic/reasoning/math side, it actually is TimeRange vs TimeRange =&gt; Timeline Operation<ul> <li>Get a way to ask about this comparison relations.</li> <li>So the question will mainly be about whether this relation is True, or which relation it is.</li> <li>For duration, we can ask about the duration of the two events, and then compare</li> <li>Or we can compare the event ranking based on the time range</li> </ul> </li> </ul> </li> <li>there is another types: Three years before 2019,  who is the president of China? =&gt;     It is a valid question, but nobody will in this way.<ul> <li>It will be normally classified into simple: in 2016, who is the president of China?</li> <li>Or it will be something like: Three years before bush end the term, who is the president of China?     =&gt; This will be classified into Medium,         and belong to the timeline_position_retrieval_temporal_constrained_retrieval</li> </ul> </li> </ul> </li> <li>answer_type:<ul> <li>subject, object for timeline_position_retrieval_temporal_constrained_retrieval<ul> <li>subject</li> <li>object</li> <li>only focus on the first one, as the second will always become the first later</li> </ul> </li> <li>temporal related for timeline_position_retrieval_timeline_position_retrieval<ul> <li>Infer a new time range: Union/Intersection</li> <li>Infer a temporal relation: Allen</li> <li>Infer a list of time ranges: Ranking</li> <li>Infer duration, and then compare</li> </ul> </li> </ul> </li> </ul> <p>Process:</p> <p>The quality of the question is not guaranteed by LLM directly if we just mask out the answer. So we will use the template to generate the questions, then use LLM to paraphrase the questions.</p> Source code in <code>TimelineKGQA/generator.py</code> <pre><code>def medium_question_generation_individual(\n    self,\n    first_event: dict,\n    second_event: dict,\n    template_based: bool = True,\n    paraphrased: bool = False,\n) -&gt; List[dict]:\n    \"\"\"\n\n    Args:\n        first_event (dict): The first event\n        second_event (dict): The second event\n        template_based (bool): Whether you use the template based question generation\n        paraphrased (bool): Whether you do the paraphrase for the question, if set to False,\n                then the paraphrased_question will be the same as the question\n\n    Returns:\n        dict: The generated questions\n            - question\n            - answer\n            - paraphrased_question\n            - events\n            - question_level: Medium\n            - question_type: The type of the question\n            - answer_type: The type of the answer\n\n    - question_type:\n        - timeline_position_retrieval_temporal_constrained_retrieval\n            - For this one, the logic/reasoning/math part will be like: **TimeRange** +\n                Temporal Semantic Operation =&gt; **TimeRange**\n            - Then the interesting part will be the Timeline Operation,\n                we have mentioned several types of operations below.\n                - There are mostly from numeric to semantic perspective\n                - Here is the reverse process: name it Temporal Semantic Operation\n                - So this is trying to convert the temporal semantic representation to\n                    a numeric operation and then get a new operation.\n        - timeline_position_retrieval_timeline_position_retrieval\n            - For the logic/reasoning/math side, it actually is **TimeRange** vs **TimeRange** =&gt; Timeline Operation\n                - Get a way to ask about this comparison relations.\n                - So the question will mainly be about whether this relation is True, or which relation it is.\n                - For duration, we can ask about the duration of the two events, and then compare\n                - Or we can compare the event ranking based on the time range\n        - there is another types: Three years before 2019,  who is the president of China? =&gt;\n            It is a valid question, but nobody will in this way.\n            - It will be normally classified into **simple**: in 2016, who is the president of China?\n            - Or it will be something like: Three years before bush end the term, who is the president of China?\n                =&gt; This will be classified into **Medium**,\n                    and belong to the timeline_position_retrieval_temporal_constrained_retrieval\n    - answer_type:\n        - subject, object for timeline_position_retrieval_temporal_constrained_retrieval\n            - subject\n            - object\n            - only focus on the first one, as the second will always become the first later\n        - temporal related for timeline_position_retrieval_timeline_position_retrieval\n            - Infer a new time range: Union/Intersection\n            - Infer a temporal relation: Allen\n            - Infer a list of time ranges: Ranking\n            - Infer duration, and then compare\n\n    Process:\n\n    The quality of the question is not guaranteed by LLM directly if we just mask out the answer.\n    So we will use the template to generate the questions, then use LLM to paraphrase the questions.\n\n    \"\"\"\n    first_event_subject = first_event[\"subject\"]\n    first_event_predicate = first_event[\"predicate\"]\n    first_event_object = first_event[\"object\"]\n    first_event_start_time = first_event[\"start_time\"]\n    first_event_end_time = first_event[\"end_time\"]\n\n    second_event_subject = second_event[\"subject\"]\n    second_event_predicate = second_event[\"predicate\"]\n    second_event_object = second_event[\"object\"]\n    second_event_start_time = second_event[\"start_time\"]\n    second_event_end_time = second_event[\"end_time\"]\n\n    first_event_start_time_dt, first_event_end_time_dt = self.util_str_to_datetime(\n        [first_event_start_time, first_event_end_time]\n    )\n    second_event_start_time_dt, second_event_end_time_dt = (\n        self.util_str_to_datetime([second_event_start_time, second_event_end_time])\n    )\n\n    # first generate\n    # timeline_position_retrieval =&gt; timeline_position_retrieval\n    # this will ask for the subject or object in one of the event\n\n    medium_type_1_a_questions = []\n    questions = []\n    \"\"\"\n    Timeline Position Retrieval =&gt; Temporal Constrained Retrieval Questions\n    \"\"\"\n    # NOTES: question here actually is not used, because we will replace it with the template.\n    # It is putting there to get the idea about the types of questions we are generating\n    \"\"\"\n    The key part of this type is:\n    We need to cover as many temporal semantic operations as possible\n    - Before, After, During, this is the most common one and shown in the literature\n    - Starts from the same time, Ends at the same time, Meets, Overlap, this is another way to add the\n        temporal condition (inspired by the allen logic)\n    - Above are from allen temporal logic and intersection/union\n    - We can also add the ranking ones, however, before/after is the same as first/last, under this category\n    - Then the rest is the one for duration, question like 3 years before, 3 years after, etc.\n\n    So we have main two types of questions here:\n    - Relation: Before, After, During \uff5c Starts from the same time, Ends at the same time, Meets, Overlap\n        - calculate the relation first, then generated based on template\n        - Before: Who is the president of US before the end of Bush's term?\n        - After: Who is the president of US after the start of Bush's term?\n        - Starts from the same time: Who and Bush start their term as father and\n            President of US respectively at the same time?\n        - Ends at the same time: Who and Bush end their term as father and\n            President of US respectively at the same time?\n        - Meets: ?\n        - During: Who is the president of US during Bush's term?\n        - Overlap: Bush as the president of US meets who when the guy become the father?\n    - Duration: 3 years before, 3 years after, 3 years after the end, etc.\n        - calculate the duration first, then generated based on template\n        - 3 years before: Who is the president of US 3 years before the end of Bush's term?\n        - 3 years after: Who is the president of US 3 years after the start of Bush's term?\n        - 3 years after the end: Who is the president of US 3 years after the end of Bush's term?\n        - 3 years after the start: Who is the president of US 3 years after the start of Bush's term?\n        - meets/during/overlap hard to get a time point, so not considered here.\n    \"\"\"\n    # ask for first subject\n    medium_type_1_a_questions.append(\n        {\n            \"question\": f\"??? {first_event_predicate} {first_event_object} [Timeline Operation on \"\n            f\"({first_event_start_time}, {first_event_end_time}) vs ({second_event_start_time}, \"\n            f\"{second_event_end_time})] {second_event_subject}\"\n            f\"{second_event_predicate} {second_event_object}?\",\n            \"answer\": f\"{first_event_subject}\",\n            \"paraphrased_question\": None,\n            \"events\": [\n                f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                f\"{first_event_start_time}|{first_event_end_time}\",\n                f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                f\"{second_event_start_time}|{second_event_end_time}\",\n            ],\n            \"question_level\": \"medium\",\n            \"question_type\": \"timeline_position_retrieval_temporal_constrained_retrieval\",\n            \"answer_type\": \"subject\",\n            \"temporal_relation\": None,\n        }\n    )\n\n    # ask for first object\n    medium_type_1_a_questions.append(\n        {\n            \"question\": f\"{first_event_subject} {first_event_predicate} ??? [Timeline Operation on \"\n            f\"({first_event_start_time}, {first_event_end_time}) vs ({second_event_start_time},\"\n            f\"{second_event_end_time})] {second_event_subject}\"\n            f\" {second_event_predicate} {second_event_object}?\",\n            \"answer\": f\"{first_event_object}\",\n            \"paraphrased_question\": None,\n            \"events\": [\n                f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                f\"{first_event_start_time}|{first_event_end_time}\",\n                f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                f\"{second_event_start_time}|{second_event_end_time}\",\n            ],\n            \"question_level\": \"medium\",\n            \"question_type\": \"timeline_position_retrieval_temporal_constrained_retrieval\",\n            \"answer_type\": \"object\",\n            \"temporal_relation\": None,\n        }\n    )\n    questions += medium_type_1_a_questions\n\n    \"\"\"\n    For duration before, duration after type  question\n    \"\"\"\n    medium_type_1_b_questions = []\n    # this will be added later when we process the questions with template\n\n    \"\"\"\n    Timeline Position Retrieval + Timeline Position Retrieval Questions\n\n    This one is mainly from numeric to temporal semantic\n\n    - Infer a new time range: Union/Intersection\n    - Infer a temporal relation: Allen\n    - Infer a list of time ranges: Ranking (not considered here)\n    - Infer duration, and then compare\n    \"\"\"\n    # Timeline Position Retrieval + Timeline Position Retrieval\n\n    # ask for union/intersection of the time range\n\n    medium_type_2_questions = [\n        {\n            \"question\": f\"{first_event_subject} {first_event_predicate} {first_event_object} ???\"\n            f\"[Timeline Operation on ({first_event_start_time}, {first_event_end_time}) vs \"\n            f\"({second_event_start_time}, {second_event_end_time})]??? \"\n            f\"{second_event_subject} {second_event_predicate} {second_event_object}?\",\n            \"answer\": \"Union/Intersection of the time range\",\n            \"paraphrased_question\": None,\n            \"events\": [\n                f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                f\"{first_event_start_time}|{first_event_end_time}\",\n                f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                f\"{second_event_start_time}|{second_event_end_time}\",\n            ],\n            \"question_level\": \"medium\",\n            \"question_type\": \"timeline_position_retrieval_timeline_position_retrieval\",\n            \"answer_type\": \"relation_union_or_intersection\",\n            \"temporal_relation\": \"intersection\",\n        },\n        {\n            \"question\": f\"{first_event_subject} {first_event_predicate} {first_event_object} \"\n            f\"???[Timeline Operation on ({first_event_start_time}, {first_event_end_time}) \"\n            f\"vs ({second_event_start_time}, {second_event_end_time})]??? \"\n            f\"{second_event_subject} {second_event_predicate} {second_event_object}?\",\n            \"answer\": \"Union/Intersection of the time range\",\n            \"paraphrased_question\": None,\n            \"events\": [\n                f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                f\"{first_event_start_time}|{first_event_end_time}\",\n                f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                f\"{second_event_start_time}|{second_event_end_time}\",\n            ],\n            \"question_level\": \"medium\",\n            \"question_type\": \"timeline_position_retrieval_timeline_position_retrieval\",\n            \"answer_type\": \"relation_union_or_intersection\",\n            \"temporal_relation\": \"union\",\n        },\n        {\n            \"question\": f\"{first_event_subject} {first_event_predicate} {first_event_object} \"\n            f\"???[Timeline Operation on ({first_event_start_time}, \"\n            f\"{first_event_end_time}) vs ({second_event_start_time}, \"\n            f\"{second_event_end_time})]??? {second_event_subject} \"\n            f\"{second_event_predicate} {second_event_object}?\",\n            \"answer\": \"Duration\",\n            \"paraphrased_question\": None,\n            \"events\": [\n                f\"{first_event_subject}|{first_event_predicate}|{first_event_object}|\"\n                f\"{first_event_start_time}|{first_event_end_time}\",\n                f\"{second_event_subject}|{second_event_predicate}|{second_event_object}|\"\n                f\"{second_event_start_time}|{second_event_end_time}\",\n            ],\n            \"question_level\": \"medium\",\n            \"question_type\": \"timeline_position_retrieval_timeline_position_retrieval\",\n            \"answer_type\": \"relation_duration\",\n            \"temporal_relation\": None,\n        },\n    ]\n    questions += medium_type_2_questions\n    temporal_answer = None\n    if template_based:\n        for question_draft in questions:\n            this_type_templates = QUESTION_TEMPLATES[\n                question_draft[\"question_level\"]\n            ][question_draft[\"question_type\"]][question_draft[\"answer_type\"]]\n\n            if (\n                question_draft[\"answer_type\"] == \"subject\"\n                or question_draft[\"answer_type\"] == \"object\"\n            ):\n                \"\"\"\n                Handle the Medium Type 1 Questions here: Both a and b\n                First calculate the relations, then based on relations to select the template\n                \"\"\"\n                temporal_relation = self.relation_allen_time_range(\n                    time_range_a=[\n                        first_event_start_time_dt,\n                        first_event_end_time_dt,\n                    ],\n                    time_range_b=[\n                        second_event_start_time_dt,\n                        second_event_end_time_dt,\n                    ],\n                )\n                temporal_relation_semantic = temporal_relation.get(\"semantic\")\n                question_draft[\"temporal_relation\"] = temporal_relation[\"relation\"]\n                random_pick_template = random.choice(\n                    this_type_templates[temporal_relation_semantic]\n                )\n\n                question_draft[\"question\"] = random_pick_template.format(\n                    first_event_subject=first_event_subject,\n                    first_event_predicate=first_event_predicate,\n                    first_event_object=first_event_object,\n                    temporal_relation=temporal_relation,\n                    second_event_subject=second_event_subject,\n                    second_event_predicate=second_event_predicate,\n                    second_event_object=second_event_object,\n                )\n                # this will generate the basic temporal relation questions.\n                # TODO: we also need to generate the one duration_before, duration_after\n                # If relation is before or after, then we can generate the duration_before, duration_after\n                # Add it to variable medium_type_1_b_questions\n                if temporal_relation_semantic in [\"before\", \"after\"]:\n                    random_pick_template = random.choice(\n                        this_type_templates[\n                            f\"duration_{temporal_relation_semantic}\"\n                        ]\n                    )\n                    # get the duration year\n                    # Example: 3 years before Bush as the president of US, who is the president of China?\n                    # The duration is calculated based on first_end_time - second_start time\n                    # NOTE: It can be extended further later to calculate first_start - second_start\n                    duration = self.relation_duration_calculation(\n                        time_range_a=[\n                            first_event_start_time_dt,\n                            first_event_end_time_dt,\n                        ],\n                        time_range_b=[\n                            second_event_start_time_dt,\n                            second_event_end_time_dt,\n                        ],\n                        temporal_operator=f\"duration_{temporal_relation_semantic}\",\n                    )\n                    # copy a new question draft\n                    duration_question_draft = copy.deepcopy(question_draft)\n                    duration_question_draft[\"question\"] = (\n                        random_pick_template.format(\n                            first_event_subject=first_event_subject,\n                            first_event_predicate=first_event_predicate,\n                            first_event_object=first_event_object,\n                            temporal_relation=f\"{duration} {temporal_relation}\",\n                            second_event_subject=second_event_subject,\n                            second_event_predicate=second_event_predicate,\n                            second_event_object=second_event_object,\n                        )\n                    )\n                    duration_question_draft[\"temporal_relation\"] = (\n                        f\"duration_{temporal_relation_semantic}\"\n                    )\n                    medium_type_1_b_questions.append(duration_question_draft)\n            else:\n                \"\"\"\n                Handle in theory four types of questions here\n                \"\"\"\n                if (\n                    question_draft[\"answer_type\"]\n                    == \"relation_union_or_intersection\"\n                ):\n                    temporal_relation = question_draft[\"temporal_relation\"]\n                    random_pick_template = random.choice(\n                        this_type_templates[temporal_relation]\n                    )\n                    temporal_answer = self.relation_union_or_intersection(\n                        time_ranges=[\n                            (first_event_start_time_dt, first_event_end_time_dt),\n                            (second_event_start_time_dt, second_event_end_time_dt),\n                        ],\n                        temporal_operator=temporal_relation,\n                    )\n\n                    question_draft[\"question\"] = random_pick_template.format(\n                        first_event_subject=first_event_subject,\n                        first_event_predicate=first_event_predicate,\n                        first_event_object=first_event_object,\n                        second_event_subject=second_event_subject,\n                        second_event_predicate=second_event_predicate,\n                        second_event_object=second_event_object,\n                    )\n                    if temporal_answer is None:\n                        temporal_answer = \"No Answer\"\n                    question_draft[\"answer\"] = temporal_answer\n                elif question_draft[\"answer_type\"] == \"relation_allen\":\n                    temporal_allen_relation = self.relation_allen_time_range(\n                        time_range_a=[\n                            first_event_start_time_dt,\n                            first_event_end_time_dt,\n                        ],\n                        time_range_b=[\n                            second_event_start_time_dt,\n                            second_event_end_time_dt,\n                        ],\n                    )\n                    question_draft[\"temporal_relation\"] = temporal_allen_relation[\n                        \"relation\"\n                    ]\n                    # random select from [choices, true_false]\n                    question_format = random.choice([\"choice\", \"true_false\"])\n                    if question_format == \"choice\":\n                        random_pick_template = random.choice(\n                            this_type_templates[\"choice\"]\n                        )\n                        temporal_answer = temporal_allen_relation[\"relation\"]\n                        question_draft[\"question\"] = random_pick_template.format(\n                            first_event_subject=first_event_subject,\n                            first_event_predicate=first_event_predicate,\n                            first_event_object=first_event_object,\n                            second_event_subject=second_event_subject,\n                            second_event_predicate=second_event_predicate,\n                            second_event_object=second_event_object,\n                        )\n                        question_draft[\"answer\"] = temporal_answer\n\n                    else:\n                        random_pick_template = random.choice(\n                            this_type_templates[\"true_false\"]\n                        )\n                        random_yes_no_answer = random.choice([\"True\", \"False\"])\n                        if random_yes_no_answer == \"True\":\n                            temporal_relation = temporal_allen_relation[\"relation\"]\n                        else:\n                            temporal_relation = random.choice(\n                                list(\n                                    set(self.allen_relations)\n                                    - {temporal_allen_relation[\"relation\"]}\n                                )\n                            )\n                        question_draft[\"question\"] = random_pick_template.format(\n                            first_event_subject=first_event_subject,\n                            first_event_predicate=first_event_predicate,\n                            first_event_object=first_event_object,\n                            second_event_subject=second_event_subject,\n                            second_event_predicate=second_event_predicate,\n                            second_event_object=second_event_object,\n                            temporal_relation=temporal_relation,\n                        )\n                        question_draft[\"answer\"] = random_yes_no_answer\n                elif question_draft[\"answer_type\"] == \"relation_duration\":\n                    \"\"\"There are four types in this category\n                    - duration =&gt; which is the intersection of the two time range\n                    - duration_compare =&gt; longer shorter equal\n                    - sum =&gt; total duration of the two time range, which is actually the union\n                    - average =&gt; average duration of the two time range\n                    \"\"\"\n                    temporal_relation = random.choice(\n                        [\n                            \"duration\",\n                            \"duration_compare\",\n                            \"sum\",\n                            \"average\",\n                        ]\n                    )\n                    random_pick_template = random.choice(\n                        this_type_templates[temporal_relation]\n                    )\n                    question_draft[\"temporal_relation\"] = temporal_relation\n                    if temporal_relation == \"duration\":\n                        temporal_answer = self.relation_union_or_intersection(\n                            time_ranges=[\n                                (\n                                    first_event_start_time_dt,\n                                    first_event_end_time_dt,\n                                ),\n                                (\n                                    second_event_start_time_dt,\n                                    second_event_end_time_dt,\n                                ),\n                            ],\n                            temporal_operator=\"intersection\",\n                        )\n                        question_draft[\"question\"] = random_pick_template.format(\n                            first_event_subject=first_event_subject,\n                            first_event_predicate=first_event_predicate,\n                            first_event_object=first_event_object,\n                            second_event_subject=second_event_subject,\n                            second_event_predicate=second_event_predicate,\n                            second_event_object=second_event_object,\n                        )\n                    elif temporal_relation == \"duration_compare\":\n                        temporal_relation_duration = (\n                            self.relation_allen_time_duration(\n                                time_range_a=[\n                                    first_event_start_time_dt,\n                                    first_event_end_time_dt,\n                                ],\n                                time_range_b=[\n                                    second_event_start_time_dt,\n                                    second_event_end_time_dt,\n                                ],\n                            )\n                        )\n                        temporal_answer = temporal_relation_duration[\"semantic\"]\n                        question_draft[\"question\"] = random_pick_template.format(\n                            first_event_subject=first_event_subject,\n                            first_event_predicate=first_event_predicate,\n                            first_event_object=first_event_object,\n                            temporal_relation=temporal_answer,\n                            second_event_subject=second_event_subject,\n                            second_event_predicate=second_event_predicate,\n                            second_event_object=second_event_object,\n                        )\n                    elif temporal_relation == \"sum\":\n                        temporal_answer = self.util_average_duration_calculation(\n                            time_ranges=[\n                                [\n                                    first_event_start_time_dt,\n                                    first_event_end_time_dt,\n                                ],\n                                [\n                                    second_event_start_time_dt,\n                                    second_event_end_time_dt,\n                                ],\n                            ],\n                            temporal_operator=\"sum\",\n                        )\n                        question_draft[\"question\"] = random_pick_template.format(\n                            first_event_subject=first_event_subject,\n                            first_event_predicate=first_event_predicate,\n                            first_event_object=first_event_object,\n                            second_event_subject=second_event_subject,\n                            second_event_predicate=second_event_predicate,\n                            second_event_object=second_event_object,\n                        )\n                    elif temporal_relation == \"average\":\n                        temporal_answer = self.util_average_duration_calculation(\n                            time_ranges=[\n                                [\n                                    first_event_start_time_dt,\n                                    first_event_end_time_dt,\n                                ],\n                                [\n                                    second_event_start_time_dt,\n                                    second_event_end_time_dt,\n                                ],\n                            ],\n                            temporal_operator=\"average\",\n                        )\n                        question_draft[\"question\"] = random_pick_template.format(\n                            first_event_subject=first_event_subject,\n                            first_event_predicate=first_event_predicate,\n                            first_event_object=first_event_object,\n                            second_event_subject=second_event_subject,\n                            second_event_predicate=second_event_predicate,\n                            second_event_object=second_event_object,\n                        )\n                    question_draft[\"answer\"] = temporal_answer\n                    question_draft[\"temporal_relation\"] = temporal_relation\n\n    questions += medium_type_1_b_questions\n    if paraphrased:\n        for question_obj in questions:\n            paraphrased_question = paraphrase_medium_question(\n                question=question_obj[\"question\"],\n            )\n            logger.info(f\"paraphrased_question: {paraphrased_question}\")\n            question_obj[\"paraphrased_question\"] = paraphrased_question\n\n    return questions\n</code></pre>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.relation_allen_time_duration","title":"<code>relation_allen_time_duration(time_range_a, time_range_b)</code>  <code>staticmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>time_range_a</code> <code>list[datetime, datetime]</code> <p>The first time range</p> required <code>time_range_b</code> <code>list[datetime, datetime]</code> <p>The second time range</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The allen temporal relation between the two time ranges</p> Source code in <code>TimelineKGQA/generator.py</code> <pre><code>@staticmethod\ndef relation_allen_time_duration(time_range_a: list, time_range_b: list) -&gt; dict:\n    \"\"\"\n\n    Args:\n        time_range_a (list[datetime, datetime]): The first time range\n        time_range_b (list[datetime, datetime]): The second time range\n\n    Returns:\n        dict: The allen temporal relation between the two time ranges\n    \"\"\"\n    duration_a = abs(time_range_a[1] - time_range_a[0])\n    duration_b = abs(time_range_b[1] - time_range_b[0])\n    if duration_a &lt; duration_b:\n        return {\n            \"relation\": \"X &lt; Y\",\n            \"description\": \"X is shorter Y\",\n            \"category\": \"td\",\n            \"code\": \"td-1\",\n            \"semantic\": \"shorter\",\n        }\n    elif duration_a == duration_b:\n        return {\n            \"relation\": \"X = Y\",\n            \"description\": \"X equals Y\",\n            \"category\": \"td\",\n            \"code\": \"td-2\",\n            \"semantic\": \"equals\",\n        }\n    else:\n        return {\n            \"relation\": \"X &gt; Y\",\n            \"description\": \"X is longer Y\",\n            \"category\": \"td\",\n            \"code\": \"td-3\",\n            \"semantic\": \"longer\",\n        }\n</code></pre>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.relation_allen_time_range","title":"<code>relation_allen_time_range(time_range_a, time_range_b)</code>  <code>staticmethod</code>","text":"<p>This function will return the allen temporal relation between two time ranges</p> <p>We have the 26 possible relations - 13 for time range operation - 10 for time point and time range operation - 3 for time point operation</p> <p>We will need to extract them from the quantitatively time range</p> <p>We will have \"beginning of time\" or \"end of time\" to represent the infinite time range We will need to convert it to a numerical value in np.inf Others will be converted to a numerical value in the timestamp</p> <p>Parameters:</p> Name Type Description Default <code>time_range_a</code> <code>list[datetime, datetime]</code> <p>The first time range</p> required <code>time_range_b</code> <code>list[datetime, datetime]</code> <p>The second time range</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The allen temporal relation between the two time ranges</p> Source code in <code>TimelineKGQA/generator.py</code> <pre><code>@staticmethod\ndef relation_allen_time_range(time_range_a: list, time_range_b: list) -&gt; dict:\n    \"\"\"\n    This function will return the allen temporal relation between two time ranges\n\n    We have the 26 possible relations\n    - 13 for time range operation\n    - 10 for time point and time range operation\n    - 3 for time point operation\n\n    We will need to extract them from the quantitatively time range\n\n    We will have \"beginning of time\" or \"end of time\" to represent the infinite time range\n    We will need to convert it to a numerical value in np.inf\n    Others will be converted to a numerical value in the timestamp\n\n    Args:\n        time_range_a (list[datetime, datetime]): The first time range\n        time_range_b (list[datetime, datetime]): The second time range\n\n    Returns:\n        dict: The allen temporal relation between the two time ranges\n    \"\"\"\n    start_time1, end_time1 = time_range_a\n    start_time2, end_time2 = time_range_b\n\n    logger.debug(\n        f\"start_time1: {start_time1}, end_time1: {end_time1}, start_time2: {start_time2}, end_time2: {end_time2}\"\n    )\n\n    # 13 for time range operation\n    time_range_a_datetime = [start_time1, end_time1]\n    time_range_b_datetime = [start_time2, end_time2]\n    # then we will do the operation for the time range, get the allen temporal relation\n    \"\"\"\n    x_start &lt;= x_end\n    y_start &lt;= y_end\n    allen_operator = [\n        x_start - x_end,  # 0, or -1, which means a is a point or a range\n        y_start - y_end, # 0, or -1\n        x_start - y_start,\n        x_start - y_end,\n        x_end - y_start,\n        x_end - y_end,\n    ]\n\n    After this do a operation for the allen_operator, if value = 0, keep it, &lt; 0, set it to -1, &gt; 0, set it to 1\n\n    Then we will have:\n    13 for time range operation, which means x_start &lt; x_end, y_start &lt; y_end\n        - X &lt;  Y =&gt; [-1, -1, -1, -1, -1, -1]\n        - X m  Y =&gt; [-1, -1, -1, -1,  0, -1]\n        - X o  Y =&gt; [-1, -1, -1, -1,  1, -1]\n        - X fi Y =&gt; [-1, -1, -1, -1,  1,  0]\n        - X di Y =&gt; [-1, -1, -1, -1,  1,  1]\n        - X s  Y =&gt; [-1, -1,  0, -1,  1, -1]\n        - X =  Y =&gt; [-1, -1,  0, -1,  1,  0]\n        - X si Y =&gt; [-1, -1,  0, -1,  1,  1]\n        - X d  Y =&gt; [-1, -1,  1, -1,  1, -1]\n        - X f  Y =&gt; [-1, -1,  1, -1,  1,  0]\n        - X oi Y =&gt; [-1, -1,  1, -1,  1,  1]\n        - X mi Y =&gt; [-1, -1,  1,  0,  1,  1]\n        - X &gt;  Y =&gt; [-1, -1,  1,  1,  1,  1]\n\n    10 for time point and time range operation\n    Among the 10, 5 for X is a point, 5 for Y is a point\n    5 for X is a point, Y is a range, which means x_start = x_end, y_start &lt; y_end\n        - X &lt;  Y =&gt; [0, -1, -1, -1, -1, -1]\n        - X s  Y =&gt; [0, -1,  0, -1,  0, -1]\n        - X d  Y =&gt; [0, -1,  1, -1,  1, -1]\n        - X f  Y =&gt; [0, -1,  1,  0,  1,  0]\n        - X &gt;  Y =&gt; [0, -1,  1,  1,  1,  1]\n    5 for X is a range, Y is a point, which means x_start &lt; x_end, y_start = y_end\n        - X &lt;  Y =&gt; [-1, 0, -1, -1, -1, -1]\n        - X fi Y =&gt; [-1, 0, -1\uff0c-1,  0,  0]\n        - X di Y =&gt; [-1, 0, -1, -1,  1,  1]\n        - X si Y =&gt; [-1, 0,  0,  0,  1,  1]\n        - X &gt;  Y =&gt; [-1, 0,  1,  1,  1,  1]\n\n    3 for time point operation, which means x_start = x_end, y_start = y_end\n        - X &lt; Y =&gt; [0, 0, -1, -1, -1, -1]\n        - X = Y =&gt; [0, 0,  0,  0,  0,  0]\n        - X &gt; Y =&gt; [0, 0,  1,  1,  1,  1]\n    \"\"\"\n\n    allen_operator = [\n        time_range_a_datetime[0] - time_range_a_datetime[1],\n        time_range_b_datetime[0] - time_range_b_datetime[1],\n        time_range_a_datetime[0] - time_range_b_datetime[0],\n        time_range_a_datetime[0] - time_range_b_datetime[1],\n        time_range_a_datetime[1] - time_range_b_datetime[0],\n        time_range_a_datetime[1] - time_range_b_datetime[1],\n    ]\n\n    # do the operation for the allen_operator\n    for index, value in enumerate(allen_operator):\n        if value == 0:\n            allen_operator[index] = 0\n        elif value &lt; 0:\n            allen_operator[index] = -1\n        else:\n            allen_operator[index] = 1\n\n    # logger.critical(f\"allen_operator: {allen_operator}\")\n    # get it to be a tuple\n    allen_operator = tuple(allen_operator)\n    logger.debug(f\"allen_operator: {allen_operator}\")\n    try:\n        logger.debug(f\"ALLEN_OPERATOR_DICT: {ALLEN_OPERATOR_DICT[allen_operator]}\")\n        return ALLEN_OPERATOR_DICT[allen_operator]\n    except KeyError:\n        logger.info(f\"allen_operator: {allen_operator}\")\n        logger.info(f\"time_range_a: {time_range_a}\")\n        logger.info(f\"time_range_b: {time_range_b}\")\n        raise ValueError(\"The allen operator is not found\")\n</code></pre>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.relation_duration","title":"<code>relation_duration(time_ranges, agg_temporal_operator=None)</code>  <code>staticmethod</code>","text":"<p>For the time range, it will do the rank operation, sort it</p> <p>First calculate the duration of the time range, then do the rank operation based on the duration</p> <p>Parameters:</p> Name Type Description Default <code>time_ranges</code> <code>list</code> <p>The list of time ranges</p> required <code>agg_temporal_operator</code> <code>str</code> <p>The aggregation temporal operator</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <p>the list of sorted index for the time range</p> <p>Example: <pre><code>time_ranges = [\n    (datetime(2023, 5, 1, 12, 0), datetime(2023, 5, 1, 15, 0)),  # 3 hours\n    (datetime(2023, 5, 1, 9, 30), datetime(2023, 5, 1, 14, 0)),  # 4.5 hours\n    (datetime(2023, 5, 1, 8, 0), datetime(2023, 5, 1, 11, 30)),  # 3.5 hours\n    (datetime(2023, 5, 2, 9, 30), datetime(2023, 5, 2, 12, 0)),  # 2.5 hours\n    (datetime(2023, 5, 1, 10, 30), datetime(2023, 5, 1, 13, 0))  # 2.5 hours\n]\n</code></pre></p> <p>The output will be: <pre><code>[2, 4, 3, 0, 1]\n</code></pre></p> Source code in <code>TimelineKGQA/generator.py</code> <pre><code>@staticmethod\ndef relation_duration(\n    time_ranges: list[[datetime, datetime]], agg_temporal_operator: str = None\n):\n    \"\"\"\n    For the time range, it will do the rank operation, sort it\n\n    First calculate the duration of the time range, then do the rank operation based on the duration\n\n    Args:\n        time_ranges (list): The list of time ranges\n        agg_temporal_operator (str): The aggregation temporal operator\n\n    Returns:\n        list: the list of sorted index for the time range\n\n\n    Example:\n    ```\n    time_ranges = [\n        (datetime(2023, 5, 1, 12, 0), datetime(2023, 5, 1, 15, 0)),  # 3 hours\n        (datetime(2023, 5, 1, 9, 30), datetime(2023, 5, 1, 14, 0)),  # 4.5 hours\n        (datetime(2023, 5, 1, 8, 0), datetime(2023, 5, 1, 11, 30)),  # 3.5 hours\n        (datetime(2023, 5, 2, 9, 30), datetime(2023, 5, 2, 12, 0)),  # 2.5 hours\n        (datetime(2023, 5, 1, 10, 30), datetime(2023, 5, 1, 13, 0))  # 2.5 hours\n    ]\n    ```\n\n    The output will be:\n    ```\n    [2, 4, 3, 0, 1]\n    ```\n    \"\"\"\n    # Create a list of indices paired with time ranges\n    if agg_temporal_operator == \"ranking\":\n        indexed_time_ranges = list(enumerate(time_ranges))\n\n        indexed_time_ranges.sort(key=lambda x: abs(x[1][1] - x[1][0]))\n        rank_by_index = [0] * len(time_ranges)  # Pre-initialize a list of zeros\n        for index, (original_index, _) in enumerate(indexed_time_ranges):\n            rank_by_index[original_index] = index\n        return rank_by_index\n    if agg_temporal_operator == \"sum\":\n        # total value of the time range\n        durations = [\n            abs(time_range[1] - time_range[0]) for time_range in time_ranges\n        ]\n        return sum(durations)\n    if agg_temporal_operator == \"average\":\n        # average value of the time range\n        durations = [\n            abs(time_range[1] - time_range[0]) for time_range in time_ranges\n        ]\n        return sum(durations) / len(durations)\n    raise ValueError(\n        \"Unsupported aggregation temporal operator. Please use 'ranking', 'sum' or 'average'.\"\n    )\n</code></pre>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.relation_duration_calculation","title":"<code>relation_duration_calculation(time_range_a, time_range_b, temporal_operator=None)</code>  <code>staticmethod</code>","text":"<p>We will calculate the time difference between two time ranges</p> <p>However, there are several combination we can do here</p> <ul> <li>duration_before =&gt; abs(time_range_a[1] - time_range_b[0])</li> <li>duration_after =&gt; abs(time_range_b[1] - time_range_a[0]) We also have other combinations, but we will not consider them here</li> </ul> <p>Parameters:</p> Name Type Description Default <code>time_range_a</code> <code>list[datetime, datetime]</code> <p>The first time range</p> required <code>time_range_b</code> <code>list[datetime, datetime]</code> <p>The second time range</p> required <code>temporal_operator</code> <code>str</code> <p>The temporal operator</p> <code>None</code> <p>Returns:</p> Name Type Description <code>timedelta</code> <code>Optional[timedelta]</code> <p>The time difference between two time ranges</p> Source code in <code>TimelineKGQA/generator.py</code> <pre><code>@staticmethod\ndef relation_duration_calculation(\n    time_range_a: list,\n    time_range_b: list,\n    temporal_operator: str = None,\n) -&gt; Optional[timedelta]:\n    \"\"\"\n    We will calculate the time difference between two time ranges\n\n    However, there are several combination we can do here\n\n    - duration_before =&gt; abs(time_range_a[1] - time_range_b[0])\n    - duration_after =&gt; abs(time_range_b[1] - time_range_a[0])\n    We also have other combinations, but we will not consider them here\n\n    Args:\n        time_range_a (list[datetime, datetime]): The first time range\n        time_range_b (list[datetime, datetime]): The second time range\n        temporal_operator (str): The temporal operator\n\n    Returns:\n        timedelta: The time difference between two time ranges\n\n    \"\"\"\n    if temporal_operator is None or temporal_operator not in [\n        \"duration_before\",\n        \"duration_after\",\n    ]:\n        raise ValueError(\n            \"temporal_operator should be one of the following: duration_before, duration_after\"\n        )\n    if temporal_operator == \"duration_before\":\n        return abs(time_range_a[1] - time_range_b[0])\n    if temporal_operator == \"duration_after\":\n        return abs(time_range_b[1] - time_range_a[0])\n    return None\n</code></pre>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.relation_ordinal_time_range","title":"<code>relation_ordinal_time_range(time_ranges, agg_temporal_operator=None)</code>  <code>staticmethod</code>","text":"<p>For the time range, it will do the rank operation, sort it</p> <p>Aggregation operator can be: - ranking(min, max)     - ranking_start     - ranking_end</p> <p>Parameters:</p> Name Type Description Default <code>time_ranges</code> <code>list</code> <p>The list of time ranges</p> required <code>agg_temporal_operator</code> <code>str</code> <p>The aggregation temporal operator</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>the list of sorted index for the time range</p> <p>For example, we have the time range:</p> <pre><code>time_ranges = [\n    (datetime(2023, 5, 1, 12, 0), datetime(2023, 5, 1, 15, 0)),  # 3 hours\n    (datetime(2023, 5, 1, 9, 30), datetime(2023, 5, 1, 14, 0)),  # 4.5 hours\n    (datetime(2023, 5, 1, 8, 0), datetime(2023, 5, 1, 11, 30)),  # 3.5 hours\n    (datetime(2023, 5, 2, 9, 30), datetime(2023, 5, 2, 12, 0)),  # 2.5 hours\n    (datetime(2023, 5, 1, 10, 30), datetime(2023, 5, 1, 13, 0))  # 2.5 hours\n]\n\nresult_start = TKGQAGenerator.aggregate_tr_temporal_operator(time_ranges, \"ranking_start\")\n[3,1,0,4,2]\n\nresult_end = TKGQAGenerator.aggregate_tr_temporal_operator(time_ranges, \"ranking_end\")\n[2,4,3,0,1]\n</code></pre> Source code in <code>TimelineKGQA/generator.py</code> <pre><code>@staticmethod\ndef relation_ordinal_time_range(\n    time_ranges: list[[datetime, datetime]], agg_temporal_operator: str = None\n) -&gt; list:\n    \"\"\"\n    For the time range, it will do the rank operation, sort it\n\n    Aggregation operator can be:\n    - ranking(min, max)\n        - ranking_start\n        - ranking_end\n\n    Args:\n        time_ranges (list): The list of time ranges\n        agg_temporal_operator (str): The aggregation temporal operator\n\n    Returns:\n        list: the list of sorted index for the time range\n\n    For example, we have the time range:\n\n    ```\n    time_ranges = [\n        (datetime(2023, 5, 1, 12, 0), datetime(2023, 5, 1, 15, 0)),  # 3 hours\n        (datetime(2023, 5, 1, 9, 30), datetime(2023, 5, 1, 14, 0)),  # 4.5 hours\n        (datetime(2023, 5, 1, 8, 0), datetime(2023, 5, 1, 11, 30)),  # 3.5 hours\n        (datetime(2023, 5, 2, 9, 30), datetime(2023, 5, 2, 12, 0)),  # 2.5 hours\n        (datetime(2023, 5, 1, 10, 30), datetime(2023, 5, 1, 13, 0))  # 2.5 hours\n    ]\n\n    result_start = TKGQAGenerator.aggregate_tr_temporal_operator(time_ranges, \"ranking_start\")\n    [3,1,0,4,2]\n\n    result_end = TKGQAGenerator.aggregate_tr_temporal_operator(time_ranges, \"ranking_end\")\n    [2,4,3,0,1]\n    ```\n    \"\"\"\n\n    # Create a list of indices paired with time ranges\n    indexed_time_ranges = list(enumerate(time_ranges))\n\n    if agg_temporal_operator == \"rank_start_time\":\n        # Sort by start time, but maintain original indices\n        indexed_time_ranges.sort(key=lambda x: x[1][0])\n    elif agg_temporal_operator == \"rank_end_time\":\n        # Sort by end time, but maintain original indices\n        indexed_time_ranges.sort(key=lambda x: x[1][1])\n    else:\n        raise ValueError(\n            \"Unsupported aggregation temporal operator. Please use 'rank_start_time' or 'rank_end_time'.\"\n        )\n\n    # After sorting, create a new list that maps the original index to its new rank\n    rank_by_index = [0] * len(time_ranges)  # Pre-initialize a list of zeros\n    for rank, (original_index, _) in enumerate(indexed_time_ranges):\n        rank_by_index[original_index] = rank\n\n    return rank_by_index\n</code></pre>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.relation_union_or_intersection","title":"<code>relation_union_or_intersection(time_ranges, temporal_operator='intersection')</code>  <code>staticmethod</code>","text":"<p>This function will return the temporal operator between multiple time ranges The temporal operator can be:     - 'intersection'     - 'union'</p> <p>Parameters:</p> Name Type Description Default <code>time_ranges</code> <code>List[Tuple[datetime, datetime]]</code> <p>A list of time ranges</p> required <code>temporal_operator</code> <code>str</code> <p>The temporal operator</p> <code>'intersection'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>A string representation of the new time range, or None if no valid range exists.</p> Source code in <code>TimelineKGQA/generator.py</code> <pre><code>@staticmethod\ndef relation_union_or_intersection(\n    time_ranges: List[Tuple[np.datetime64, np.datetime64]],\n    temporal_operator: str = \"intersection\",\n) -&gt; Optional[str]:\n    \"\"\"\n    This function will return the temporal operator between multiple time ranges\n    The temporal operator can be:\n        - 'intersection'\n        - 'union'\n\n    Args:\n        time_ranges (List[Tuple[datetime, datetime]]): A list of time ranges\n        temporal_operator (str): The temporal operator\n\n    Returns:\n        str: A string representation of the new time range, or None if no valid range exists.\n\n    \"\"\"\n    if temporal_operator not in [\"intersection\", \"union\"]:\n        raise ValueError(\n            \"temporal_operator should be either 'intersection' or 'union'\"\n        )\n\n    if not time_ranges:\n        return None\n\n    # Start with the first time range\n    result = time_ranges[0]\n\n    for current in time_ranges[1:]:\n        if temporal_operator == \"intersection\":\n            # Find the latest start time and earliest end time\n            start = max(result[0], current[0])\n            end = min(result[1], current[1])\n            if start &gt;= end:\n                return None  # No intersection\n            result = (start, end)\n        elif temporal_operator == \"union\":\n            # Find the earliest start time and latest end time\n            start = min(result[0], current[0])\n            end = max(result[1], current[1])\n            # Check if there is a gap between the ranges\n            if result[1] &lt; current[0] or current[1] &lt; result[0]:\n                return None  # No continuous union possible\n            result = (start, end)\n\n    return f\"({result[0]}, {result[1]})\"\n</code></pre>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.sampling_events","title":"<code>sampling_events(sample_percentage=0.1, sample_strategy='temporal_close')</code>","text":"<p>This function will sample the events from the list</p> <p>Parameters:</p> Name Type Description Default <code>sample_percentage</code> <code>float</code> <p>The sample percentage</p> <code>0.1</code> <code>sample_strategy</code> <code>str</code> <p>The sample strategy, can be random, temporal_close, degree_high, both</p> <code>'temporal_close'</code> <p>Returns:</p> Type Description <p>List[Tuple]: The list Tuples (event1_id, event2_id, event3_id)</p> Source code in <code>TimelineKGQA/generator.py</code> <pre><code>def sampling_events(\n    self,\n    sample_percentage: Union[float, dict, int] = 0.1,\n    sample_strategy: str = \"temporal_close\",\n):\n    \"\"\"\n    This function will sample the events from the list\n\n    Args:\n        sample_percentage (float): The sample percentage\n        sample_strategy (str): The sample strategy, can be random, temporal_close, degree_high, both\n\n    Returns:\n        List[Tuple]: The list Tuples (event1_id, event2_id, event3_id)\n\n    \"\"\"\n\n    if sample_strategy not in (\"random\", \"temporal_close\", \"degree_high\", \"both\"):\n        raise ValueError(\n            \"sample_strategy should be random, temporal_close, degree_high, or both\"\n        )\n\n    \"\"\"\n    Do matrix sampling based on the element value\n    If the dimension is 2, we will have a nxn matrix\n        - value of the matrix is 1 for random\n        - value will be calculated based on the temporal information if it is temporal_close\n        - value will be calculated based on the degree information if it is degree_high\n        - value will be calculated based on both temporal and degree information if it is both\n\n    And then use the value as weight to do the sampling over the matrix\n    \"\"\"\n\n    num_events = len(self.events_df)\n    # for dimension 1 generate, first construct a matrix with len(event_df), using numpy,\n    # and it is always sampling randomly\n    if sample_strategy == \"degree_high\" or sample_strategy == \"both\":\n        degree_scores = self.calculate_degree_scores(self.events_df)\n    else:\n        degree_scores = None\n\n    with timer(the_logger=logger, message=\"Generating the matrix D1\"):\n        dimension_1_matrix = np.ones(num_events)\n\n        # make sure every element in the matrix sum to 1\n        dimension_1_matrix = dimension_1_matrix / dimension_1_matrix.sum()\n\n    with timer(the_logger=logger, message=\"Generating the matrix D2\"):\n        # for dimension 2 generate, first construct a matrix with len(event_df), using numpy\n        dimension_2_matrix = np.zeros((num_events, num_events))\n\n        if sample_strategy == \"random\":\n            dimension_2_matrix = np.ones((num_events, num_events))\n        elif sample_strategy == \"temporal_close\":\n            start_times = self.events_df[\"start_time\"].values\n            end_times = self.events_df[\"end_time\"].values\n\n            for x in range(num_events):\n                for y in range(x + 1, num_events):  # y &gt; x to avoid redundancy\n                    score = self.temporal_close_score(\n                        time_ranges=[\n                            [start_times[x], end_times[x]],\n                            [start_times[y], end_times[y]],\n                        ]\n                    )\n                    dimension_2_matrix[x, y] = score\n                    dimension_2_matrix[y, x] = score  # Leverage symmetry\n\n        elif sample_strategy == \"degree_high\":\n\n            for x in range(num_events):\n                for y in range(x + 1, num_events):\n                    score = degree_scores[x] = degree_scores[y]\n                    dimension_2_matrix[x, y] = score\n                    dimension_2_matrix[y, x] = score  # Leverage symmetry\n\n        elif sample_strategy == \"both\":\n            # park here\n            start_times = self.events_df[\"start_time\"].values\n            end_times = self.events_df[\"end_time\"].values\n            for x in tqdm(range(num_events), desc=\"Processing events\"):\n                for y in range(x + 1, num_events):\n                    degree_score = degree_scores[x] + degree_scores[y]\n                    temporal_score = self.temporal_close_score(\n                        time_ranges=[\n                            [start_times[x], end_times[x]],\n                            [start_times[y], end_times[y]],\n                        ]\n                    )\n                    score = degree_score + temporal_score\n                    dimension_2_matrix[x, y] = score\n                    dimension_2_matrix[y, x] = score\n\n        # make sure every element in the matrix sum to 1\n        dimension_2_matrix = dimension_2_matrix / dimension_2_matrix.sum()\n\n    with timer(the_logger=logger, message=\"Generating the matrix D3\"):\n        # for dimension 3 generate, first construct a matrix with len(event_df), using numpy\n        dimension_3_matrix = np.zeros((num_events, num_events, num_events))\n        if sample_strategy == \"random\":\n            dimension_3_matrix = np.ones((num_events, num_events, num_events))\n        elif sample_strategy == \"temporal_close\":\n            start_times = self.events_df[\"start_time\"].values\n            end_times = self.events_df[\"end_time\"].values\n\n            for x in tqdm(range(num_events), desc=\"Processing 3D events\"):\n                for y in range(x + 1, num_events):  # y &gt; x to avoid redundancy\n                    for z in range(y + 1, num_events):  # z &gt; y to avoid redundancy\n                        score = self.temporal_close_score(\n                            time_ranges=[\n                                [start_times[x], end_times[x]],\n                                [start_times[y], end_times[y]],\n                                [start_times[z], end_times[z]],\n                            ]\n                        )\n                        dimension_3_matrix[x, y, z] = score\n                        dimension_3_matrix[x, z, y] = score\n                        dimension_3_matrix[y, x, z] = score\n                        dimension_3_matrix[y, z, x] = score\n                        dimension_3_matrix[z, x, y] = score\n                        dimension_3_matrix[z, y, x] = score\n        elif sample_strategy == \"degree_high\":\n            for x in range(num_events):\n                for y in range(x + 1, num_events):\n                    for z in range(y + 1, num_events):\n                        score = (\n                            degree_scores[x] + degree_scores[y] + degree_scores[z]\n                        )\n                        dimension_3_matrix[x, y, z] = score\n                        dimension_3_matrix[x, z, y] = score\n                        dimension_3_matrix[y, x, z] = score\n                        dimension_3_matrix[y, z, x] = score\n                        dimension_3_matrix[z, x, y] = score\n                        dimension_3_matrix[z, y, x] = score\n        elif sample_strategy == \"both\":\n            # park here\n            start_times = self.events_df[\"start_time\"].values\n            end_times = self.events_df[\"end_time\"].values\n            for x in tqdm(range(num_events), desc=\"Processing 3D events\"):\n                for y in range(x + 1, num_events):\n                    for z in range(y + 1, num_events):\n                        degree_score = (\n                            degree_scores[x] + degree_scores[y] + degree_scores[z]\n                        )\n                        temporal_score = self.temporal_close_score(\n                            time_ranges=[\n                                [start_times[x], end_times[x]],\n                                [start_times[y], end_times[y]],\n                                [start_times[z], end_times[z]],\n                            ]\n                        )\n                        score = degree_score + temporal_score\n                        dimension_3_matrix[x, y, z] = score\n                        dimension_3_matrix[x, z, y] = score\n                        dimension_3_matrix[y, x, z] = score\n                        dimension_3_matrix[y, z, x] = score\n                        dimension_3_matrix[z, x, y] = score\n                        dimension_3_matrix[z, y, x] = score\n\n        # make sure every element in the matrix sum to 1\n        dimension_3_matrix = dimension_3_matrix / dimension_3_matrix.sum()\n    # do the sampling based on the matrix\n    # if sampling is a float value, then it means all three dimensions will be sampled based on the rate\n    # if samping is an int value, then it means all three dimension will have that many questions\n    # if sampling is a dict value, then it means the sampling rate for each dimension\n\n    with timer(the_logger=logger, message=\"Sampling the events\"):\n        if isinstance(sample_percentage, float):\n            # sample based on the rate, and the value (weight) is the matrix value\n            dimension_1_samples = np.random.choice(\n                num_events,\n                int(num_events * sample_percentage),\n                p=dimension_1_matrix,\n            )\n            # sample it from dimension 2 matrix\n            dimension_2_samples = self.random_selection(\n                dimension_2_matrix,\n                int(dimension_2_matrix.size * sample_percentage),\n            )\n            # sample it from dimension 3 matrix\n            dimension_3_samples = self.random_selection(\n                dimension_3_matrix,\n                int(dimension_3_matrix.size * sample_percentage),\n            )\n\n        elif isinstance(sample_percentage, int):\n            dimension_1_samples = np.random.choice(\n                num_events, sample_percentage, p=dimension_1_matrix\n            )\n            dimension_2_samples = self.random_selection(\n                dimension_2_matrix,\n                sample_percentage,\n            )\n            dimension_3_samples = self.random_selection(\n                dimension_3_matrix,\n                sample_percentage,\n            )\n\n        elif isinstance(sample_percentage, dict):\n            dimension_1_samples = sample_percentage.get(\"dimension_1\", 0)\n            dimension_2_samples = sample_percentage.get(\"dimension_2\", 0)\n            dimension_3_samples = sample_percentage.get(\"dimension_3\", 0)\n            if (\n                dimension_1_samples == 0\n                or dimension_2_samples == 0\n                or dimension_3_samples == 0\n            ):\n                raise ValueError(\n                    \"The sample_percentage should have all three dimensions\"\n                )\n            # if all types of\n            # dimension_1_sample_percentage,\n            # dimension_2_sample_percentage,\n            # dimension_3_sample_percentage are float\n            # then we will sample based on the rate\n            if all(\n                isinstance(i, float)\n                for i in [\n                    dimension_1_samples,\n                    dimension_2_samples,\n                    dimension_3_samples,\n                ]\n            ):\n                dimension_1_samples = np.random.choice(\n                    len(self.events_df),\n                    int(len(self.events_df) * dimension_1_samples),\n                    p=dimension_1_matrix,\n                )\n                dimension_2_samples = self.random_selection(\n                    dimension_2_matrix,\n                    int(dimension_2_matrix.size * dimension_2_samples),\n                )\n                dimension_3_samples = self.random_selection(\n                    dimension_3_matrix,\n                    int(dimension_3_matrix.size * dimension_3_samples),\n                )\n            # if all types of\n            # dimension_1_sample_percentage,\n            # dimension_2_sample_percentage,\n            # dimension_3_sample_percentage are int\n            # then we will sample based on the number\n            elif all(\n                isinstance(i, int)\n                for i in [\n                    dimension_1_samples,\n                    dimension_2_samples,\n                    dimension_3_samples,\n                ]\n            ):\n                dimension_1_samples = np.random.choice(\n                    len(self.events_df), dimension_1_samples, p=dimension_1_matrix\n                )\n                dimension_2_samples = self.random_selection(\n                    dimension_2_matrix,\n                    dimension_2_samples,\n                )\n                dimension_3_samples = self.random_selection(\n                    dimension_3_matrix,\n                    dimension_3_samples,\n                )\n            else:\n                raise ValueError(\n                    \"The sample_percentage should have all three dimensions\"\n                )\n        else:\n            raise ValueError(\n                \"The sample_percentage should be either float, int, or dict\"\n            )\n\n    logger.info(len(dimension_1_samples))\n    logger.info(len(dimension_2_samples))\n    logger.info(len(dimension_3_samples))\n\n    \"\"\"\n    Examples of the output:\n\n    ```\n    [  0  10  20  30  40  50  60  70  80  90 100]\n    [(0, 10), (20, 30), (40, 50), (60, 70), (80, 90), (100, 110)]\n    [(0, 10, 20), (30, 40, 50), (60, 70, 80), (90, 100, 110)]\n    ```\n    \"\"\"\n    self.sample_simple_events = dimension_1_samples\n    self.sample_medium_events = dimension_2_samples\n    self.sample_complex_events = dimension_3_samples\n    return dimension_1_samples, dimension_2_samples, dimension_3_samples\n</code></pre>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.simple_question_generation","title":"<code>simple_question_generation()</code>","text":""},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.simple_question_generation--types-of-questions","title":"Types of Questions","text":"<p>This is used to generate the simple question, we will have two types of questions.</p> <p>For each type of questions, based on the answer or the question focus, we can further divide them into - Timeline Position Retrival     - Start TimePoint     - End TimePoint     - Time Range     - Duration - Temporal Constrained Retrieval (Ignore predicate for now)     - Subject     - Object</p> <p>Simple: Timeline and One Event Involved - Timeline Position Retrival: When Bush starts his term as president of US?     - General Information Retrieval =&gt; Timeline Position Retrival =&gt; Answer the question     - Question Focus can be: Timestamp Start, Timestamp End, Duration, Timestamp Start and End - Temporal Constrained Retrieval: In 2009, who is the president of US?     - General Information Retrieval =&gt; Temporal Constraint Retrieval =&gt; Answer the question     - Question Focus can be: Subject, Object, Predicate. Can be more complex if we want mask out more elements</p>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.simple_question_generation--templates","title":"Templates","text":"<p>To generate the questions, We can try to feed into the LLM, and generate the questions. However, the diversity of the questions is not guaranteed, so we can use the template to generate the questions. Then use LLM to paraphrase the questions.</p> <p>Template examples: - Timeline Position Retrival     - Start TimePoint: When did {subject} start the term as {object}?     - End TimePoint: When did {subject} end the term as {object}?     - Time Range: When did {subject} serve as {object}?     - Duration: How long did {subject} serve as {object}? - Temporal Constrained Retrieval     - Subject:         - Who is affiliated to {subject} from {timestamp start} to {timestamp end}?         - Who is affiliated to {subject} in {timestamp}?     - Object:         - {subject} is affiliated to which organisation from {timestamp start} to {timestamp end}?         - {subject} is affiliated to which organisation during {temporal representation}?</p>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.simple_question_generation--process","title":"Process","text":"<ul> <li>Extract {subject}, {predicate}, {object}, {start_time}, {end_time} from the unified graph</li> <li>Generate the questions based on the template for each type</li> <li>Use LLM to paraphrase the questions</li> </ul> <p>Output format will be: - {question} - {answer} - {paraphrased_question} - subject, predicate, object, start_time, end_time - {question_level} =&gt; Simple - {question_type} =&gt; Timeline Position Retrival, Temporal Constrained Retrieval - {answer_type} =&gt; Subject, Object | Timestamp Start, Timestamp End, Duration, Timestamp Start and End</p> Source code in <code>TimelineKGQA/generator.py</code> <pre><code>def simple_question_generation(self):\n    \"\"\"\n    ## Types of Questions\n    This is used to generate the simple question, we will have two types of questions.\n\n    For each type of questions, based on the answer or the question focus, we can further divide them into\n    - Timeline Position Retrival\n        - Start TimePoint\n        - End TimePoint\n        - Time Range\n        - Duration\n    - Temporal Constrained Retrieval (Ignore predicate for now)\n        - Subject\n        - Object\n\n    Simple: Timeline and One Event Involved\n    - Timeline Position Retrival: When Bush starts his term as president of US?\n        - General Information Retrieval =&gt; Timeline Position Retrival =&gt; Answer the question\n        - Question Focus can be: Timestamp Start, Timestamp End, Duration, Timestamp Start and End\n    - Temporal Constrained Retrieval: In 2009, who is the president of US?\n        - General Information Retrieval =&gt; Temporal Constraint Retrieval =&gt; Answer the question\n        - Question Focus can be: Subject, Object, Predicate. Can be more complex if we want mask out more elements\n\n\n    ## Templates\n    To generate the questions, We can try to feed into the LLM, and generate the questions.\n    However, the diversity of the questions is not guaranteed, so we can use the template to generate the questions.\n    Then use LLM to paraphrase the questions.\n\n    Template examples:\n    - Timeline Position Retrival\n        - Start TimePoint: When did {subject} start the term as {object}?\n        - End TimePoint: When did {subject} end the term as {object}?\n        - Time Range: When did {subject} serve as {object}?\n        - Duration: How long did {subject} serve as {object}?\n    - Temporal Constrained Retrieval\n        - Subject:\n            - Who is affiliated to {subject} from {timestamp start} to {timestamp end}?\n            - Who is affiliated to {subject} in {timestamp}?\n        - Object:\n            - {subject} is affiliated to which organisation from {timestamp start} to {timestamp end}?\n            - {subject} is affiliated to which organisation during {temporal representation}?\n\n    ## Process\n    - Extract {subject}, {predicate}, {object}, {start_time}, {end_time} from the unified graph\n    - Generate the questions based on the template for each type\n    - Use LLM to paraphrase the questions\n\n    Output format will be:\n    - {question}\n    - {answer}\n    - {paraphrased_question}\n    - subject, predicate, object, start_time, end_time\n    - {question_level} =&gt; Simple\n    - {question_type} =&gt; Timeline Position Retrival, Temporal Constrained Retrieval\n    - {answer_type} =&gt; Subject, Object | Timestamp Start, Timestamp End, Duration, Timestamp Start and End\n    \"\"\"\n    # get records not yet generated questions\n\n    insert_values_list = []\n    bulk_sql_pointer = 0\n    for item in self.sample_simple_events:\n        event = self.events_df.iloc[item]\n        questions = self.simple_question_generation_individual(\n            subject=event[\"subject\"],\n            predicate=event[\"predicate\"],\n            tail_object=event[\"object\"],\n            start_time=event[\"start_time\"],\n            end_time=event[\"end_time\"],\n            template_based=True,\n            paraphrased=self.paraphrased,\n        )\n\n        # insert each qa into the table, have a flat table\n        for question_obj in questions:\n            question_obj[\"source_kg_id\"] = int(event[\"id\"])\n            # get dict to tuple, sequence should be the same as the sql command\n            data = (\n                question_obj[\"source_kg_id\"],\n                question_obj[\"question\"],\n                question_obj[\"answer\"],\n                question_obj[\"paraphrased_question\"],\n                question_obj[\"events\"],\n                question_obj[\"question_level\"],\n                question_obj[\"question_type\"],\n                question_obj[\"answer_type\"],\n                \"timeline\",\n            )\n            insert_values_list.append(data)\n            bulk_sql_pointer += 1\n            if bulk_sql_pointer % self.bulk_sql_size == 0:\n                self.bulk_insert(values=insert_values_list)\n                insert_values_list = []\n\n    self.bulk_insert(values=insert_values_list)\n</code></pre>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.simple_question_generation_individual","title":"<code>simple_question_generation_individual(subject, predicate, tail_object, start_time, end_time, template_based=False, paraphrased=False)</code>  <code>staticmethod</code>","text":"<p>This will try to generate four questions belong to RE type</p> <p>The questions will be: - ? p o during the time range from start_time to end_time? - s p ? during the time range from start_time to end_time? - s p o from ? to end_time? - s p o from start_time to ? - s p o from ? to ? - [How long/What's the duration, etc.] ? for the statement s p o</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>str</code> <p>The subject</p> required <code>predicate</code> <code>str</code> <p>The predicate</p> required <code>tail_object</code> <code>str</code> <p>The tail_object</p> required <code>start_time</code> <code>str</code> <p>The start time</p> required <code>end_time</code> <code>str</code> <p>The end time</p> required <code>template_based</code> <code>bool</code> <p>Whether you use the template based question generation</p> <code>False</code> <code>paraphrased</code> <code>bool</code> <p>Whether you do the paraphrase for the question, if set to False,     then the paraphrased_question will be the same as the question</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>List[dict]</code> <p>The generated questions - question - answer - paraphrased_question - events - question_level: Simple - question_type: The type of the question - answer_type: The type of the answer</p> Source code in <code>TimelineKGQA/generator.py</code> <pre><code>@staticmethod\ndef simple_question_generation_individual(\n    subject: str,\n    predicate: str,\n    tail_object: str,\n    start_time: str,\n    end_time: str,\n    template_based: bool = False,\n    paraphrased: bool = False,\n) -&gt; List[dict]:\n    \"\"\"\n    This will try to generate four questions belong to RE type\n\n    The questions will be:\n    - ? p o during the time range from start_time to end_time?\n    - s p ? during the time range from start_time to end_time?\n    - s p o from ? to end_time?\n    - s p o from start_time to ?\n    - s p o from ? to ?\n    - [How long/What's the duration, etc.] ? for the statement s p o\n\n    Args:\n        subject (str): The subject\n        predicate (str): The predicate\n        tail_object (str): The tail_object\n        start_time (str): The start time\n        end_time (str): The end time\n        template_based (bool): Whether you use the template based question generation\n        paraphrased (bool): Whether you do the paraphrase for the question, if set to False,\n                then the paraphrased_question will be the same as the question\n\n    Returns:\n        dict: The generated questions\n            - question\n            - answer\n            - paraphrased_question\n            - events\n            - question_level: Simple\n            - question_type: The type of the question\n            - answer_type: The type of the answer\n    \"\"\"\n\n    questions = [\n        {\n            \"question\": f\"??? {predicate} {tail_object} during the time range from {start_time} to {end_time}?\",\n            \"answer\": f\"{subject}\",\n            \"paraphrased_question\": None,\n            \"events\": [\n                f\"{subject}|{predicate}|{tail_object}|{start_time}|{end_time}\"\n            ],\n            \"question_level\": \"simple\",\n            \"question_type\": \"temporal_constrained_retrieval\",\n            \"answer_type\": \"subject\",\n        },\n        {\n            \"question\": f\"{subject} {predicate} ??? during the time range from {start_time} to {end_time}?\",\n            \"answer\": f\"{tail_object}\",\n            \"paraphrased_question\": None,\n            \"events\": [\n                f\"{subject}|{predicate}|{tail_object}|{start_time}|{end_time}\"\n            ],\n            \"question_level\": \"simple\",\n            \"question_type\": \"temporal_constrained_retrieval\",\n            \"answer_type\": \"object\",\n        },\n        {\n            \"question\": f\"{subject} {predicate} {tail_object} from ??? to {end_time}?\",\n            \"answer\": f\"{start_time}\",\n            \"paraphrased_question\": None,\n            \"events\": [\n                f\"{subject}|{predicate}|{tail_object}|{start_time}|{end_time}\"\n            ],\n            \"question_level\": \"simple\",\n            \"question_type\": \"timeline_position_retrieval\",\n            \"answer_type\": \"timestamp_start\",\n        },\n        {\n            \"question\": f\"{subject} {predicate} {tail_object} from {start_time} to ???\",\n            \"answer\": f\"{end_time}\",\n            \"paraphrased_question\": None,\n            \"events\": [\n                f\"{subject}|{predicate}|{tail_object}|{start_time}|{end_time}\"\n            ],\n            \"question_level\": \"simple\",\n            \"question_type\": \"timeline_position_retrieval\",\n            \"answer_type\": \"timestamp_end\",\n        },\n        {\n            \"question\": f\"{subject} {predicate} {tail_object} from ??? to ???\",\n            \"answer\": f\"{start_time} and {end_time}\",\n            \"paraphrased_question\": None,\n            \"events\": [\n                f\"{subject}|{predicate}|{tail_object}|{start_time}|{end_time}\"\n            ],\n            \"question_level\": \"simple\",\n            \"question_type\": \"timeline_position_retrieval\",\n            \"answer_type\": \"timestamp_range\",\n        },\n        {\n            \"question\": f\"[How long/What's the duration/etc] ??? for the statement \"\n            f\"{subject} {predicate} {tail_object}\",\n            \"answer\": f\"{end_time} - {start_time}\",\n            \"paraphrased_question\": None,\n            \"events\": [\n                f\"{subject}|{predicate}|{tail_object}|{start_time}|{end_time}\"\n            ],\n            \"question_level\": \"simple\",\n            \"question_type\": \"timeline_position_retrieval\",\n            \"answer_type\": \"duration\",\n        },\n    ]\n    if template_based:\n        # we will random pick one from the template\n        for question_draft in questions:\n            this_type_templates = QUESTION_TEMPLATES[\n                question_draft[\"question_level\"]\n            ][question_draft[\"question_type\"]][question_draft[\"answer_type\"]]\n            logger.debug(f\"this_type_templates: {this_type_templates}\")\n            random_pick_template = random.choice(this_type_templates)\n            # replace {subject}, {predicate}, {tail_object}, {start_time}, {end_time} with the real value\n            question_draft[\"question\"] = random_pick_template.format(\n                subject=subject,\n                predicate=predicate,\n                tail_object=tail_object,\n                start_time=start_time,\n                end_time=end_time,\n            )\n\n    if paraphrased:\n        for question_obj in questions:\n            paraphrased_question = paraphrase_simple_question(\n                question=question_obj[\"question\"]\n            )\n            logger.info(f\"paraphrased_question: {paraphrased_question}\")\n            question_obj[\"paraphrased_question\"] = paraphrased_question\n\n    return questions\n</code></pre>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.temporal_close_score","title":"<code>temporal_close_score(time_ranges)</code>","text":"<p>This function will calculate the temporal close score between two/three time ranges.</p> <p>range_a = [start_time_a, end_time_a] range_b = [start_time_b, end_time_b] range_c = [start_time_c, end_time_c] (if three ranges are provided)</p> <p>score = 1 / ((start_time_b - start_time_a)2 + (end_time_b - end_time_a)2) (for two ranges)</p> <p>score = 1 / ((start_time_b - start_time_a)2 + (end_time_b - end_time_a)2 +              (start_time_c - start_time_a)2 + (end_time_c - end_time_a)2) (for three ranges)</p> <p>Parameters:</p> Name Type Description Default <code>time_ranges</code> <code>List[Tuple[datetime, datetime]]</code> <p>The list of time ranges</p> required <p>Returns:</p> Name Type Description <code>temporal_close_score</code> <code>float</code> <p>The temporal close score between two/three time ranges, if it is close</p> <code>float</code> <p>to 1, it means the time ranges are close to each other, if it is close to 0, it means the time ranges</p> <code>float</code> <p>are far from each other</p> Source code in <code>TimelineKGQA/generator.py</code> <pre><code>def temporal_close_score(self, time_ranges: List) -&gt; float:\n    \"\"\"\n    This function will calculate the temporal close score between two/three time ranges.\n\n    range_a = [start_time_a, end_time_a]\n    range_b = [start_time_b, end_time_b]\n    range_c = [start_time_c, end_time_c] (if three ranges are provided)\n\n    score = 1 / ((start_time_b - start_time_a)**2 + (end_time_b - end_time_a)**2)\n    (for two ranges)\n\n    score = 1 / ((start_time_b - start_time_a)**2 + (end_time_b - end_time_a)**2 +\n                 (start_time_c - start_time_a)**2 + (end_time_c - end_time_a)**2)\n    (for three ranges)\n\n    Args:\n        time_ranges (List[Tuple[datetime, datetime]]): The list of time ranges\n\n    Returns:\n        temporal_close_score (float): The temporal close score between two/three time ranges, if it is close\n        to 1, it means the time ranges are close to each other, if it is close to 0, it means the time ranges\n        are far from each other\n    \"\"\"\n    if len(time_ranges) not in [2, 3]:\n        raise ValueError(\"The function only supports two or three time ranges\")\n\n    # Utility function to convert string to datetime\n    start_time_a_dt, end_time_a_dt = self.util_str_to_datetime(time_ranges[0])\n    start_time_b_dt, end_time_b_dt = self.util_str_to_datetime(time_ranges[1])\n\n    # Calculate the differences in days\n    start_diff_days = (start_time_b_dt - start_time_a_dt) / np.timedelta64(1, \"D\")\n    end_diff_days = (end_time_b_dt - end_time_a_dt) / np.timedelta64(1, \"D\")\n\n    # Calculate the score using days difference\n    score = start_diff_days**2 + end_diff_days**2\n\n    if len(time_ranges) == 3:\n        start_time_c_dt, end_time_c_dt = self.util_str_to_datetime(time_ranges[2])\n        start_diff_days_c = (start_time_c_dt - start_time_a_dt) / np.timedelta64(\n            1, \"D\"\n        )\n        end_diff_days_c = (end_time_c_dt - end_time_a_dt) / np.timedelta64(1, \"D\")\n        score += start_diff_days_c**2 + end_diff_days_c**2\n    if score == 0:\n        return 0\n    return 1 / score\n</code></pre>"},{"location":"Code/generator/#TimelineKGQA.generator.TKGQAGenerator.util_str_to_datetime","title":"<code>util_str_to_datetime(time_range)</code>  <code>staticmethod</code>","text":"<p>Convert the string to datetime</p> <p>Parameters:</p> Name Type Description Default <code>time_range</code> <code>list[str, str]</code> <p>The time range in string format</p> required <p>Returns:</p> Type Description <code>Tuple[datetime64, datetime64]</code> <p>list[datetime, datetime]: The time range in datetime format</p> Source code in <code>TimelineKGQA/generator.py</code> <pre><code>@staticmethod\ndef util_str_to_datetime(time_range: list) -&gt; Tuple[np.datetime64, np.datetime64]:\n    \"\"\"\n    Convert the string to datetime\n\n    Args:\n        time_range (list[str, str]): The time range in string format\n\n    Returns:\n        list[datetime, datetime]: The time range in datetime format\n\n    \"\"\"\n    start_time, end_time = time_range\n    if start_time == \"beginning of time\":\n        start_time = datetime.min.replace(year=1)\n    if end_time == \"end of time\":\n        end_time = datetime.max.replace(year=9999)\n\n    # convert the time to numerical value, format is like this: 1939-04-25\n    start_time = np.datetime64(start_time)\n    end_time = np.datetime64(end_time)\n\n    return start_time, end_time\n</code></pre>"},{"location":"Code/openai_utils/","title":"Openai utils","text":""},{"location":"Code/openai_utils/#TimelineKGQA.openai_utils.embedding_content","title":"<code>embedding_content(prompt, model_name='text-embedding-3-small')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>prompt</code> <p>The prompt to generate the embedding for</p> required <code>model_name</code> <p>The model to use for generating the embedding</p> <code>'text-embedding-3-small'</code> Source code in <code>TimelineKGQA/openai_utils.py</code> <pre><code>def embedding_content(prompt, model_name=\"text-embedding-3-small\"):\n    \"\"\"\n    Args:\n        prompt: The prompt to generate the embedding for\n        model_name: The model to use for generating the embedding\n\n    \"\"\"\n    response = client.embeddings.create(input=prompt, model=model_name)\n\n    return response.data[0].embedding\n</code></pre>"},{"location":"Code/openai_utils/#TimelineKGQA.openai_utils.paraphrase_medium_question","title":"<code>paraphrase_medium_question(question, model_name='gpt-4o')</code>","text":"<p>Paraphrases the given question using the OpenAI model specified.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question to paraphrase.</p> required <code>model_name</code> <code>str</code> <p>The model to use for paraphrasing.</p> <code>'gpt-4o'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The paraphrased question.</p> Source code in <code>TimelineKGQA/openai_utils.py</code> <pre><code>def paraphrase_medium_question(\n    question: str,\n    model_name: str = \"gpt-4o\",\n) -&gt; str:\n    \"\"\"\n    Paraphrases the given question using the OpenAI model specified.\n\n    Args:\n        question (str): The question to paraphrase.\n\n        model_name (str): The model to use for paraphrasing.\n\n    Returns:\n        str: The paraphrased question.\n    \"\"\"\n    prompt_text = f\"Paraphrase the following question: '{question}'\"\n    try:\n        # Some examples include:\n        # Who is affiliated with the organization during a given time.\n        # Which or what's the organization's name a specific guy is affiliated to.\n        # When/During/when is start time ...\n        # Etc.\n        # If there is a statement from beginning of time to the end of time, this will mean it is always true for the whole timeline.\n        response = client.chat.completions.create(\n            model=model_name,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"You are an expert on paraphrasing questions.\n                                  ??? is the masked out answer\n                                  You job is paraphrasing this into a natural language question.\n                                  The missing part can be someone, some organisation or some time.\n                                  Representive question types include:\n                                  - When/Before/After/During/(temporal conditions calculated based on Timeline Operation) event A happen, who is leader of organization B?\n                                  - Is event A before/after/during event B?\n                                  And we should not mention any specific time in the question, it is a type of implicit temporal questions.\n                                  \"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": prompt_text,\n                },\n            ],\n            max_tokens=100,\n            temperature=0.8,\n            stop=[\"\\n\"],\n        )\n        paraphrased_question = response.choices[0].message.content\n        return paraphrased_question\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return \"\"\n</code></pre>"},{"location":"Code/openai_utils/#TimelineKGQA.openai_utils.paraphrase_simple_question","title":"<code>paraphrase_simple_question(question, answer=None, answer_type=None, model_name='gpt-4o')</code>","text":"<p>Paraphrases the given question using the OpenAI model specified.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question to paraphrase.</p> required <code>answer</code> <code>str</code> <p>The answer to the question, which can help in generating a context-aware paraphrase.</p> <code>None</code> <code>answer_type</code> <code>str</code> <p>The type of the answer, which can help in generating a context-aware paraphrase.</p> <code>None</code> <code>model_name</code> <code>str</code> <p>The model to use for paraphrasing.</p> <code>'gpt-4o'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The paraphrased question.</p> Source code in <code>TimelineKGQA/openai_utils.py</code> <pre><code>def paraphrase_simple_question(\n    question: str,\n    answer: str = None,\n    answer_type: str = None,\n    model_name: str = \"gpt-4o\",\n) -&gt; str:\n    \"\"\"\n    Paraphrases the given question using the OpenAI model specified.\n\n    Args:\n        question (str): The question to paraphrase.\n        answer (str, optional): The answer to the question, which can help in generating a context-aware paraphrase.\n        answer_type (str, optional): The type of the answer, which can help in generating a context-aware paraphrase.\n        model_name (str): The model to use for paraphrasing.\n\n    Returns:\n        str: The paraphrased question.\n    \"\"\"\n    prompt_text = f\"Paraphrase the following question: '{question}'\"\n    try:\n        # Some examples include:\n        # Who is affiliated with the organization during a given time.\n        # Which or what's the organization's name a specific guy is affiliated to.\n        # When/During/when is start time ...\n        # Etc.\n        # If there is a statement from beginning of time to the end of time,\n        # this will mean it is always true for the whole timeline.\n        response = client.chat.completions.create(\n            model=model_name,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"You are an expert on paraphrasing questions.\n                                  You job is paraphrasing this into a natural language question.\n                                  The missing part can be someone, some organisation or some time.\n                                  Use diverse ways to represent the temporal aspect of the question.\n                                  Only return the paraphrased question, nothing else.\n                                  \"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": prompt_text,\n                },\n            ],\n            max_tokens=100,\n            temperature=0.8,\n            stop=[\"\\n\"],\n        )\n        paraphrased_question = response.choices[0].message.content\n        return paraphrased_question\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return \"\"\n</code></pre>"},{"location":"Code/paraphrase/","title":"Paraphrase","text":"<p>Read a table of generated questions, and paraphrase them using OpenAI's GPT-4o model.</p> <p>Given a table name, then read from the database, update the paraphrased questions, and write them back to the database.</p>"},{"location":"Code/processor/","title":"Processor","text":""},{"location":"Code/templates/","title":"Templates","text":""},{"location":"Code/utils/","title":"Utils","text":""},{"location":"Code/utils/#TimelineKGQA.utils.timer","title":"<code>timer</code>","text":"<p>util function used to log the time taken by a part of program</p> Source code in <code>TimelineKGQA/utils.py</code> <pre><code>class timer:\n    \"\"\"\n    util function used to log the time taken by a part of program\n    \"\"\"\n\n    def __init__(self, the_logger: Logger, message: str):\n        \"\"\"\n        init the timer\n\n        Parameters\n        ----------\n        the_logger: Logger\n            logger to write the logs\n        message: str\n            message to log, like start xxx\n        \"\"\"\n        self.message = message\n        self.logger = the_logger\n        self.start = 0\n        self.duration = 0\n        self.sub_timers = []\n\n    def __enter__(self):\n        \"\"\"\n        context enters to start to write this\n        \"\"\"\n        self.start = time.time()\n        self.logger.info(\"Starting %s\" % self.message)\n        return self\n\n    def __exit__(self, context, value, traceback):\n        \"\"\"\n        context exit will write this\n        \"\"\"\n        self.duration = time.time() - self.start\n        self.logger.info(f\"Finished {self.message}, that took {self.duration:.3f}\")\n</code></pre>"},{"location":"Code/utils/#TimelineKGQA.utils.timer.__enter__","title":"<code>__enter__()</code>","text":"<p>context enters to start to write this</p> Source code in <code>TimelineKGQA/utils.py</code> <pre><code>def __enter__(self):\n    \"\"\"\n    context enters to start to write this\n    \"\"\"\n    self.start = time.time()\n    self.logger.info(\"Starting %s\" % self.message)\n    return self\n</code></pre>"},{"location":"Code/utils/#TimelineKGQA.utils.timer.__exit__","title":"<code>__exit__(context, value, traceback)</code>","text":"<p>context exit will write this</p> Source code in <code>TimelineKGQA/utils.py</code> <pre><code>def __exit__(self, context, value, traceback):\n    \"\"\"\n    context exit will write this\n    \"\"\"\n    self.duration = time.time() - self.start\n    self.logger.info(f\"Finished {self.message}, that took {self.duration:.3f}\")\n</code></pre>"},{"location":"Code/utils/#TimelineKGQA.utils.timer.__init__","title":"<code>__init__(the_logger, message)</code>","text":"<p>init the timer</p>"},{"location":"Code/utils/#TimelineKGQA.utils.timer.__init__--parameters","title":"Parameters","text":"<p>the_logger: Logger     logger to write the logs message: str     message to log, like start xxx</p> Source code in <code>TimelineKGQA/utils.py</code> <pre><code>def __init__(self, the_logger: Logger, message: str):\n    \"\"\"\n    init the timer\n\n    Parameters\n    ----------\n    the_logger: Logger\n        logger to write the logs\n    message: str\n        message to log, like start xxx\n    \"\"\"\n    self.message = message\n    self.logger = the_logger\n    self.start = 0\n    self.duration = 0\n    self.sub_timers = []\n</code></pre>"},{"location":"Code/data_loader/","title":"Index","text":""},{"location":"Code/data_loader/load_cronquestions/","title":"Load cronquestions","text":""},{"location":"Code/data_loader/load_cronquestions/#TimelineKGQA.data_loader.load_cronquestions.CronQuestions","title":"<code>CronQuestions</code>","text":"Source code in <code>TimelineKGQA/data_loader/load_cronquestions.py</code> <pre><code>class CronQuestions:\n    def __init__(self):\n        self.engine = create_engine(DB_CONNECTION_STR)\n        self.cron_question_dir = DATA_DIR / \"CronQuestions\" / \"questions\"\n        self.cron_kg_dir = DATA_DIR / \"CronQuestions\" / \"kg\"\n        self.id_2_relation = None\n        self.id_2_entity = None\n        self.id_alias = None\n        self.full_df = None\n\n    def load_questions(self):\n        # read pickle file\n        question_files = [\n            \"test.pickle\",\n            \"train.pickle\",\n            \"valid.pickle\",\n        ]\n        questions_df = pd.DataFrame()\n        for question_file in question_files:\n            with open(self.cron_question_dir / question_file, \"rb\") as f:\n                questions = pickle.load(f)\n                # Convert questions to a DataFrame before appending if not already in DataFrame format\n                if not isinstance(questions, pd.DataFrame):\n                    questions = pd.DataFrame(questions)\n                questions_df = pd.concat([questions_df, questions], ignore_index=True)\n\n        questions_df.to_csv(self.cron_question_dir / \"questions.csv\", index=False)\n        # question,answers,answer_type,template ONLY USE this 4 columns to SQL\n        questions_df = questions_df[[\"question\", \"answer_type\", \"template\", \"type\"]]\n        questions_df.to_sql(\"cron_questions\", DB_CONNECTION_STR, if_exists=\"replace\")\n        logger.info(len(questions_df))\n\n    def load_kg(self):\n        \"\"\"\n        Load the questions to the unified KG table.\n        :return:\n        \"\"\"\n        # go to database to check whether we have a table cron_kg exists and with record, if not\n        # load it from the file\n        # first check whether the table exists\n        load_full_kg = False\n        with self.engine.connect() as conn:\n            result = conn.execute(\n                text(\n                    \"SELECT COUNT(*) FROM information_schema.tables WHERE table_name = 'cron_kg'\"\n                )\n            )\n            cron_kg_table_count = (\n                result.scalar()\n            )  # Using scalar() to get the first column of the first row\n            logger.info(cron_kg_table_count)\n            if cron_kg_table_count != 0:\n                result = conn.execute(text(\"SELECT COUNT(*) FROM cron_kg\"))\n                cron_kg_record_count = (\n                    result.scalar()\n                )  # Using scalar() to get the first column of the first row\n\n                if cron_kg_record_count == 0:\n                    load_full_kg = True\n            else:\n                load_full_kg = True\n\n        if load_full_kg is False:\n            with timer(logger, \"read full.csv\"):\n                self.full_df = pd.read_sql(\"SELECT * FROM cron_kg\", self.engine)\n            return\n\n        with timer(logger, \"load basic dataset information\"):\n            id_2_entity = pd.read_table(\n                self.cron_kg_dir / \"wd_id2entity_text.txt\", header=None\n            )\n            id_2_relation = pd.read_table(\n                self.cron_kg_dir / \"wd_id2relation_text.txt\", header=None\n            )\n            # rename columns to ['id', 'entity']\n            id_2_entity = id_2_entity.rename(columns={0: \"id\", 1: \"entity\"})\n            # rename columns to ['id', 'relation']\n            id_2_relation = id_2_relation.rename(columns={0: \"id\", 1: \"relation\"})\n\n            # Ensure that 'id' is the index\n            id_2_entity.set_index(\"id\", inplace=True)\n            id_2_relation.set_index(\"id\", inplace=True)\n\n            self.id_2_relation = id_2_relation\n            self.id_2_entity = id_2_entity\n            self.id_alias = pd.read_pickle(self.cron_kg_dir / \"wd_id_to_aliases.pickle\")\n\n        with timer(logger, \"load full.csv\"):\n\n            full_df = pd.read_table(self.cron_kg_dir / \"full.txt\", header=None)\n            full_df = full_df.rename(\n                columns={\n                    0: \"head\",\n                    1: \"relation\",\n                    2: \"tail\",\n                    3: \"start_year\",\n                    4: \"end_year\",\n                }\n            )\n\n            # Create mapping series\n            entity_mapping = id_2_entity[\"entity\"]\n            relation_mapping = id_2_relation[\"relation\"]\n\n            # Convert to nlp triple format\n            full_df[\"head\"] = full_df[\"head\"].map(entity_mapping)\n            full_df[\"relation\"] = full_df[\"relation\"].map(relation_mapping)\n            full_df[\"tail\"] = full_df[\"tail\"].map(entity_mapping)\n            self.full_df = full_df\n            # add -01-01 to start_year and end_year\n            self.full_df[\"start_year\"] = (\n                self.full_df[\"start_year\"].astype(str) + \"-01-01\"\n            )\n            self.full_df[\"end_year\"] = self.full_df[\"end_year\"].astype(str) + \"-01-01\"\n            self.full_df.to_sql(\"cron_kg\", DB_CONNECTION_STR, if_exists=\"replace\")\n\n    def unified_kg(self):\n        \"\"\"\n        Get the cron_kg into the unified kg format\n\n        Which include the following columns:\n        subject\n        predicate\n        object\n        subject_json (json) empty here\n        object_json (json) empty here\n        start_time (str)\n        end_time (str)\n\n        \"\"\"\n        cursor = self.engine.connect()\n        # run sql directly from the cron_kg\n        cursor.execute(\n            text(\n                \"\"\"\n                DO\n                $$\n                BEGIN\n                    IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'unified_kg_cron') THEN\n                        CREATE TABLE unified_kg_cron(\n                            id SERIAL PRIMARY KEY,\n                            subject TEXT,\n                            subject_json JSON DEFAULT '{}'::JSON,\n                            predicate TEXT,\n                            predicate_json JSON DEFAULT '{}'::JSON,\n                            object TEXT,\n                            object_json JSON DEFAULT '{}'::JSON,\n                            start_time TEXT,\n                            end_time TEXT\n                        );\n                    END IF;\n                    TRUNCATE TABLE unified_kg_cron;\n                    INSERT INTO unified_kg_cron(subject, predicate, object, start_time, end_time)\n                    SELECT head, relation, tail, start_year, end_year FROM cron_kg;\n                END\n                $$\n                \"\"\"\n            )\n        )\n        cursor.commit()\n        cursor.close()\n</code></pre>"},{"location":"Code/data_loader/load_cronquestions/#TimelineKGQA.data_loader.load_cronquestions.CronQuestions.load_kg","title":"<code>load_kg()</code>","text":"<p>Load the questions to the unified KG table. :return:</p> Source code in <code>TimelineKGQA/data_loader/load_cronquestions.py</code> <pre><code>def load_kg(self):\n    \"\"\"\n    Load the questions to the unified KG table.\n    :return:\n    \"\"\"\n    # go to database to check whether we have a table cron_kg exists and with record, if not\n    # load it from the file\n    # first check whether the table exists\n    load_full_kg = False\n    with self.engine.connect() as conn:\n        result = conn.execute(\n            text(\n                \"SELECT COUNT(*) FROM information_schema.tables WHERE table_name = 'cron_kg'\"\n            )\n        )\n        cron_kg_table_count = (\n            result.scalar()\n        )  # Using scalar() to get the first column of the first row\n        logger.info(cron_kg_table_count)\n        if cron_kg_table_count != 0:\n            result = conn.execute(text(\"SELECT COUNT(*) FROM cron_kg\"))\n            cron_kg_record_count = (\n                result.scalar()\n            )  # Using scalar() to get the first column of the first row\n\n            if cron_kg_record_count == 0:\n                load_full_kg = True\n        else:\n            load_full_kg = True\n\n    if load_full_kg is False:\n        with timer(logger, \"read full.csv\"):\n            self.full_df = pd.read_sql(\"SELECT * FROM cron_kg\", self.engine)\n        return\n\n    with timer(logger, \"load basic dataset information\"):\n        id_2_entity = pd.read_table(\n            self.cron_kg_dir / \"wd_id2entity_text.txt\", header=None\n        )\n        id_2_relation = pd.read_table(\n            self.cron_kg_dir / \"wd_id2relation_text.txt\", header=None\n        )\n        # rename columns to ['id', 'entity']\n        id_2_entity = id_2_entity.rename(columns={0: \"id\", 1: \"entity\"})\n        # rename columns to ['id', 'relation']\n        id_2_relation = id_2_relation.rename(columns={0: \"id\", 1: \"relation\"})\n\n        # Ensure that 'id' is the index\n        id_2_entity.set_index(\"id\", inplace=True)\n        id_2_relation.set_index(\"id\", inplace=True)\n\n        self.id_2_relation = id_2_relation\n        self.id_2_entity = id_2_entity\n        self.id_alias = pd.read_pickle(self.cron_kg_dir / \"wd_id_to_aliases.pickle\")\n\n    with timer(logger, \"load full.csv\"):\n\n        full_df = pd.read_table(self.cron_kg_dir / \"full.txt\", header=None)\n        full_df = full_df.rename(\n            columns={\n                0: \"head\",\n                1: \"relation\",\n                2: \"tail\",\n                3: \"start_year\",\n                4: \"end_year\",\n            }\n        )\n\n        # Create mapping series\n        entity_mapping = id_2_entity[\"entity\"]\n        relation_mapping = id_2_relation[\"relation\"]\n\n        # Convert to nlp triple format\n        full_df[\"head\"] = full_df[\"head\"].map(entity_mapping)\n        full_df[\"relation\"] = full_df[\"relation\"].map(relation_mapping)\n        full_df[\"tail\"] = full_df[\"tail\"].map(entity_mapping)\n        self.full_df = full_df\n        # add -01-01 to start_year and end_year\n        self.full_df[\"start_year\"] = (\n            self.full_df[\"start_year\"].astype(str) + \"-01-01\"\n        )\n        self.full_df[\"end_year\"] = self.full_df[\"end_year\"].astype(str) + \"-01-01\"\n        self.full_df.to_sql(\"cron_kg\", DB_CONNECTION_STR, if_exists=\"replace\")\n</code></pre>"},{"location":"Code/data_loader/load_cronquestions/#TimelineKGQA.data_loader.load_cronquestions.CronQuestions.unified_kg","title":"<code>unified_kg()</code>","text":"<p>Get the cron_kg into the unified kg format</p> <p>Which include the following columns: subject predicate object subject_json (json) empty here object_json (json) empty here start_time (str) end_time (str)</p> Source code in <code>TimelineKGQA/data_loader/load_cronquestions.py</code> <pre><code>def unified_kg(self):\n    \"\"\"\n    Get the cron_kg into the unified kg format\n\n    Which include the following columns:\n    subject\n    predicate\n    object\n    subject_json (json) empty here\n    object_json (json) empty here\n    start_time (str)\n    end_time (str)\n\n    \"\"\"\n    cursor = self.engine.connect()\n    # run sql directly from the cron_kg\n    cursor.execute(\n        text(\n            \"\"\"\n            DO\n            $$\n            BEGIN\n                IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'unified_kg_cron') THEN\n                    CREATE TABLE unified_kg_cron(\n                        id SERIAL PRIMARY KEY,\n                        subject TEXT,\n                        subject_json JSON DEFAULT '{}'::JSON,\n                        predicate TEXT,\n                        predicate_json JSON DEFAULT '{}'::JSON,\n                        object TEXT,\n                        object_json JSON DEFAULT '{}'::JSON,\n                        start_time TEXT,\n                        end_time TEXT\n                    );\n                END IF;\n                TRUNCATE TABLE unified_kg_cron;\n                INSERT INTO unified_kg_cron(subject, predicate, object, start_time, end_time)\n                SELECT head, relation, tail, start_year, end_year FROM cron_kg;\n            END\n            $$\n            \"\"\"\n        )\n    )\n    cursor.commit()\n    cursor.close()\n</code></pre>"},{"location":"Code/data_loader/load_icews/","title":"Load icews","text":""},{"location":"Code/data_loader/load_icews/#TimelineKGQA.data_loader.load_icews.ICEWSDataLoader","title":"<code>ICEWSDataLoader</code>","text":"Source code in <code>TimelineKGQA/data_loader/load_icews.py</code> <pre><code>class ICEWSDataLoader:\n    def __init__(\n        self,\n        data_type=\"all\",\n        view_sector_tree_web: bool = False,\n        token: str = \"\",\n        queue_name: str = \"\",\n        db_connection_str: str = DB_CONNECTION_STR,\n    ):\n        self.engine = create_engine(db_connection_str)\n        self.data_type = data_type\n        self.view_sector_tree_web = view_sector_tree_web\n        self.api = API(token=token)\n        self.queue_name = queue_name\n\n    def icews_load_data(self):\n        \"\"\"\n        Before doing anything, you will need to download the ICEWS data from the Harvard Dataverse.\n\n        Data name is: \"ICEWS Coded Event Data\",\n            https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/28075\n\n        After downloading the data, extract the zip file and put the data in the following directory:\n            TimelineKGQA/TimelineKGQA/data/icews_events_data/ICEWS/ICEWS Coded Event Data\n\n        :return:\n        \"\"\"\n\n        if self.data_type == \"all\" or self.data_type == \"icews\":\n\n            # loop the folder, and unzip all the files ending with .zip\n            # it will override the data if it is running twice\n            for file in os.listdir(DATA_ICEWS_EVENTS_DATA_DIR):\n                if file.endswith(\".zip\"):\n                    # unzip the file\n                    zip_path = DATA_ICEWS_EVENTS_DATA_DIR / file\n                    logger.info(f\"Unzipping {zip_path}\")\n                    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n                        zip_ref.extractall(DATA_ICEWS_EVENTS_DATA_DIR)\n\n            # read all .tab or .csv files into the df and check their column distribution\n            # pandas read from tab, csv files\n            # have done the check, all files have the consistent same column names\n            combined_df = None\n            for file in os.listdir(DATA_ICEWS_EVENTS_DATA_DIR):\n                if file.endswith(\".tab\") or file.endswith(\".csv\"):\n                    tab_path = DATA_ICEWS_EVENTS_DATA_DIR / file\n                    logger.info(f\"Reading {tab_path}\")\n                    df = pd.read_csv(tab_path, sep=\"\\t\", low_memory=False)\n                    if combined_df is None:\n                        combined_df = df\n                    else:\n                        # combine df and combined_df\n                        combined_df = pd.concat([combined_df, df], ignore_index=True)\n            logger.info(\"Loading data into database\")\n            combined_df.to_sql(\n                \"icews\", con=self.engine, if_exists=\"replace\", index=False\n            )\n\n        if self.data_type == \"all\" or self.data_type == \"icews_dicts\":\n            # load the ICEWS Dictionaries into the database for further review\n            logger.info(\"Loading dictionaries into database\")\n            # loop all the files in the directory, and saving them into the database\n            for file in os.listdir(DATA_ICEWS_DICTS_DATA_DIR):\n                if file.endswith(\".csv\"):\n                    csv_path = DATA_ICEWS_DICTS_DATA_DIR / file\n                    logger.info(f\"Reading {csv_path}\")\n                    # if sector in the file name, then the csv do not have header\n                    if \"sector\" in file:\n                        df = pd.read_csv(csv_path, header=None, low_memory=False)\n                    else:\n                        df = pd.read_csv(csv_path, low_memory=False)\n                    table_name = file.rsplit(\".\", 2)[0].replace(\".\", \"_\")\n                    logger.info(f\"Loading {table_name} into database\")\n                    if \"id\" not in df.columns:\n                        df[\"id\"] = range(1, 1 + len(df))\n                    df.to_sql(\n                        table_name, con=self.engine, if_exists=\"replace\", index=False\n                    )\n\n    def icews_explore_data(self):\n        \"\"\"\n        Read the ICEWS_Sector, as it is a tree, plot a tree for this.\n        :return:\n        \"\"\"\n\n        df = pd.read_sql_table(\"icews_sectors\", con=self.engine)\n        # Initialize lists to hold the transformed data\n        names = []\n        parents = []\n\n        logger.info(df.head())\n\n        # Track the last seen name at each level to establish parent-child relationships\n        last_seen = {-1: \"\"}  # Root has no name\n\n        # Iterate over the rows in the original dataframe\n        for _, row in df.iterrows():\n            for level in range(len(row)):\n                logger.info(f\"Level: {level}\")\n                # Check if the cell is not empty\n                if not pd.isnull(row[level]):\n                    # This level's name\n                    name = row[level]\n                    # Parent is the last seen name in the previous level\n                    parent = last_seen[level - 1]\n                    # Update this level's last seen name\n                    last_seen[level] = name\n                    # If this name at this level is not already added, add it to the lists\n                    if (name not in names) or parents[names.index(name)] != parent:\n                        names.append(name)\n                        parents.append(parent)\n                    break  # Move to the next row once the first non-empty cell is processed\n\n        # Creating a new dataframe from the transformed data\n        transformed_df = pd.DataFrame({\"name\": names, \"parent\": parents})\n\n        # Display the first few rows of the transformed dataframe\n        logger.info(transformed_df.head())\n\n        # Creating a tree diagram with Plotly\n        fig = go.Figure(\n            go.Treemap(\n                labels=transformed_df[\"name\"],\n                parents=transformed_df[\"parent\"],\n            )\n        )\n\n        fig.update_layout(margin=dict(t=0, l=0, r=0, b=0))\n\n        if self.view_sector_tree_web:\n            fig.show()\n\n    def icews_actor_unified_kg(self):\n        \"\"\"\n        run sql query\n        ```sql\n        CREATE TABLE unified_kg_icews_actor AS\n        SELECT\n            \"Actor Name\" AS subject,\n            json_build_object('Country', \"Country\", 'Aliases', \"Aliases\") AS subject_json,\n            'Affiliation To' AS predicate,\n            '{}'::json AS predicate_json, -- Correctly cast empty JSON object\n            \"Affiliation To\" AS object,\n            '{}'::json AS object_json, -- Correctly cast empty JSON object\n            \"Affiliation Start Date\" AS start_time,\n            \"Affiliation End Date\" AS end_time\n        FROM\n            icews_actors;\n        ```\n        :return:\n        \"\"\"\n        cursor = self.engine.connect()\n        cursor.execute(\n            text(\n                \"\"\"\n            DO\n            $$\n                BEGIN\n                    -- Attempt to create the table if it doesn't exist\n                    -- This part only creates the table structure\n                    IF NOT EXISTS (SELECT FROM pg_tables WHERE schemaname = 'public' AND tablename = 'unified_kg_icews_actor') THEN\n                        CREATE TABLE public.unified_kg_icews_actor\n                        (\n                            id             SERIAL PRIMARY KEY,\n                            subject        TEXT,\n                            subject_json   JSON,\n                            predicate      TEXT,\n                            predicate_json JSON DEFAULT '{}'::json,\n                            object         TEXT,\n                            object_json    JSON DEFAULT '{}'::json,\n                            start_time     TEXT,\n                            end_time       TEXT\n                        );\n                    END IF;\n                    TRUNCATE TABLE public.unified_kg_icews_actor;\n                    INSERT INTO unified_kg_icews_actor(\n                        subject,\n                        subject_json,\n                        predicate,\n                        predicate_json,\n                        object,\n                        object_json,\n                        start_time,\n                        end_time\n                    )\n                    SELECT\n                        \"Actor Name\" AS subject,\n                        json_build_object('Country', \"Country\", 'Aliases', \"Aliases\") AS subject_json,\n                        'Affiliation To' AS predicate,\n                        '{}'::json AS predicate_json, -- Correctly cast empty JSON object\n                        \"Affiliation To\" AS object,\n                        '{}'::json AS object_json, -- Correctly cast empty JSON object\n                        \"Affiliation Start Date\" AS start_time,\n                        \"Affiliation End Date\" AS end_time\n                    FROM\n                        icews_actors;\n                END\n            $$;\n\n            \"\"\"\n            )\n        )\n        cursor.commit()\n        cursor.close()\n\n    def icews_actor_queue_embedding(\n        self, model_name: str = \"Mixtral-8x7b\", embedding_field_name: str = None\n    ):\n        \"\"\"\n        embedding iceews actors with several models, add columns to original table\n        embedding content will be subject affiliated to object\n        :return:\n        \"\"\"\n        # add a json field for the embedding, then we can have {\"model_name\": \"embedding\"}\n        if embedding_field_name is None:\n            embedding_field_name = model_name.replace(\"-\", \"_\")\n\n        self.__db_embedding_field(embedding_field_name)\n        # get the one that has not been embedded with SQL, embedding?.model_name is null\n        with self.engine.connect() as conn:\n            r = conn.execute(\n                text(\n                    f\"\"\"\n                        SELECT *\n                        FROM icews_actors\n                        WHERE {embedding_field_name} IS NULL\n                        ORDER BY id\n                        DESC\n                        ;\n                        \"\"\"\n                )\n            )\n\n            prompts = []\n            logger.info(self.queue_name)\n            logger.info(model_name)\n            for row in r.mappings():\n                logger.debug(row)\n                # record_id = row[\"id\"]\n                subject = row[\"Actor Name\"]\n                object = row[\"Affiliation To\"]\n                prompt = f\"{subject} affiliated to {object}\"\n                prompts.append(prompt)\n\n            # every 100 prompts, send to the queue\n            for i in range(0, len(prompts), 100):\n                if i + 100 &gt; len(prompts):\n                    queued_prompts = prompts[i:]\n                else:\n                    queued_prompts = prompts[i : i + 100]\n                response = self.api.queue_create_embedding(\n                    queued_prompts,\n                    model_name=model_name,\n                    name=self.queue_name,\n                )\n                time.sleep(0.3)\n                logger.info(response)\n\n    def icews_actor_queue_actor_name_embedding(\n        self,\n        model_name: str = \"bert\",\n        field_name: str = \"Actor Name\",\n        embedding_field_name: str = None,\n    ):\n        \"\"\"\n        embedding iceews actors with several models, add columns to original table\n        embedding content will be subject affiliated to object\n        :return:\n        \"\"\"\n        if embedding_field_name is None:\n            embedding_field_name = model_name.replace(\"-\", \"_\")\n\n        # get the one that has not been embedded with SQL, embedding?.model_name is null\n        with self.engine.connect() as conn:\n            r = conn.execute(\n                text(\n                    f\"\"\"\n                        SELECT \"{field_name}\"\n                        FROM icews_actors\n                        WHERE \"{embedding_field_name}\" IS NULL\n                        GROUP BY \"{field_name}\"\n                        ;\n                        \"\"\"\n                )\n            )\n\n            prompts = []\n            logger.info(self.queue_name)\n            logger.info(model_name)\n            for row in r.mappings():\n                prompt = row[field_name]\n                prompts.append(prompt)\n\n            # every 100 prompts, send to the queue\n            for i in range(0, len(prompts), 100):\n                if i + 100 &gt; len(prompts):\n                    response = self.api.queue_create_embedding(\n                        prompts[i:], model_name=model_name, name=self.queue_name\n                    )\n                else:\n                    response = self.api.queue_create_embedding(\n                        prompts[i : i + 100],\n                        model_name=model_name,\n                        name=self.queue_name,\n                    )\n                    logger.info(response)\n                time.sleep(0.3)\n\n    def icews_actor_embedding_csv(\n        self,\n        queue_embedding_filename: str,\n        model_name: str,\n        embedding_field_name: str = None,\n        prompt_field: str = None,\n    ):\n        \"\"\"\n        Load the embedding from the queue into the database\n        :param queue_embedding_filename:\n        :param model_name:\n        :param embedding_field_name:\n        :return:\n        \"\"\"\n        if embedding_field_name is None:\n            embedding_field_name = model_name.replace(\"-\", \"_\")\n        self.__db_embedding_field(embedding_field_name)\n        conn = self.engine.connect()\n        df = pd.read_csv(DATA_DIR / \"ICEWS\" / \"processed\" / queue_embedding_filename)\n        for _, row in df.iterrows():\n            if row[\"model_name\"] != model_name:\n                continue\n\n            if model_name == \"bert\":\n                embedding = json.loads(json.loads(row[\"response\"]))[\"embedding\"]\n            else:\n                embedding = json.loads(json.loads(row[\"response\"]))[\"data\"][0][\n                    \"embedding\"\n                ]\n            logger.debug(embedding)\n\n            if prompt_field is None:\n                prompt = row[\"prompt\"]\n                subject = prompt.split(\" affiliated to \")[0].replace(\"'\", \"''\")\n                object = prompt.split(\" affiliated to \")[1].replace(\"'\", \"''\")\n                # update the embedding column\n                conn.execute(\n                    text(\n                        f\"\"\"\n                        UPDATE icews_actors\n                        SET {embedding_field_name} = array{embedding}::vector\n                        WHERE \"Actor Name\" = '{subject}' AND \"Affiliation To\" = '{object}';\n                        \"\"\"\n                    )\n                )\n            else:\n                prompt = row[\"prompt\"]\n                prompt = prompt.replace(\"'\", \"''\")\n                conn.execute(\n                    text(\n                        f\"\"\"\n                        UPDATE icews_actors\n                        SET {embedding_field_name} = array{embedding}::vector\n                        WHERE \"{prompt_field}\" = '{prompt}';\n                        \"\"\"\n                    )\n                )\n            conn.commit()\n\n    def __db_embedding_field(self, embedding_field_name: str):\n        add_embedding_column_sql = f\"\"\"\n        DO $$\n        BEGIN\n            IF NOT EXISTS (\n                SELECT FROM information_schema.columns\n                WHERE table_name = 'icews_actors' AND column_name = '{embedding_field_name}' AND table_schema = 'public'\n            ) THEN\n                ALTER TABLE public.icews_actors ADD COLUMN {embedding_field_name} vector;\n            END IF;\n        END\n        $$;\n        \"\"\"\n        cursor = self.engine.connect()\n        cursor.execute(text(add_embedding_column_sql))\n        cursor.commit()\n        cursor.close()\n\n    def __icews_actor_bert_embedding(self, prompt: str):\n        \"\"\"\n        Use the BERT model to embed the ICEWS actors\n        :return:\n        \"\"\"\n        # Load pre-trained model tokenizer (vocabulary)\n        # Generate embeddings\n        model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n        embeddings = model.encode(prompt)\n        return embeddings.tolist()\n\n    @staticmethod\n    def __similarity_to_color(value):\n        \"\"\"\n        Returns an RGB color tuple (r, g, b) based on the given value between 0 and 1.\n        The color gradient transitions from red (for 0) to green (for 1).\n        \"\"\"\n        # Clamp the value between 0 and 1\n        value = max(0, min(1, value))\n\n        # Map the value to the hue range (0 to 120)\n        hue = (value) * 1.2  # Scaling factor to adjust the hue range\n\n        # Convert the hue to RGB color tuple\n        rgb = colorsys.hsv_to_rgb(hue / 3, 1, 1)  # HSV to RGB conversion\n\n        # Convert RGB values to integers between 0 and 255\n        rgb = tuple(int(c * 255) for c in rgb)\n\n        return rgb\n\n    # @staticmethod\n    # def __similarity_to_color(similarity):\n    #     # Assuming similarity ranges from -1 to 1, normalize to 0-1\n    #     # normalized_similarity = (similarity + 1) / 2\n    #     # Use a colormap (e.g., 'RdYlGn' for Red-Yellow-Green)\n    #     return plt.get_cmap(\"viridis\")(similarity)\n\n    def icews_actor_entity_resolution_check(self):\n        \"\"\"\n        Check the entity resolution for the ICEWS actors\n        :return:\n        \"\"\"\n        pass\n\n    def icews_actor_subject_count_distribution(\n        self,\n        actor_name: str,\n        semantic_search: bool = False,\n        model_name: str = \"bert\",\n        embedding_field_name: str = None,\n    ):\n        \"\"\"\n        Get all records for the actor_name and present the occurrence across a timeline.\n        X-axis: Year\n        Y-axis: Month\n        When hovering over a point, it shows the value of \"Affiliation To\".\n        \"\"\"\n        if embedding_field_name is None:\n            embedding_field_name = model_name.replace(\"-\", \"_\")\n        if not semantic_search:\n            # SQL query to get all records for the specified actor_name\n            get_all_records_for_actor_name = f\"\"\"\n            SELECT\n            \"Actor Name\",\n            \"Affiliation Start Date\",\n            \"Affiliation End Date\",\n            \"Affiliation To\",\n            {embedding_field_name} as embedding\n            FROM icews_actors WHERE \"Actor Name\" = '{actor_name}';\n            \"\"\"\n            # Execute the query\n            actor_df = pd.read_sql_query(\n                get_all_records_for_actor_name, con=self.engine\n            )\n        else:\n            # get the embedding of the actor_name\n            if model_name == \"bert\":\n                actor_name_embedding = self.__icews_actor_bert_embedding(actor_name)\n            else:\n                actor_name_embedding = self.api.queue_embedding_and_wait_for_result(\n                    [actor_name], model_name=model_name, name=\"tkgqa\"\n                )\n            # query\n            get_relevant_records_for_actor_name = f\"\"\"\n            SELECT\n            \"Actor Name\"\n            FROM icews_actors\n            WHERE {embedding_field_name} IS NOT NULL\n            ORDER BY {embedding_field_name} &lt;-&gt; array{actor_name_embedding}::vector\n            LIMIT 10;\n            \"\"\"\n            # Execute the query\n            related_actors = pd.read_sql_query(\n                get_relevant_records_for_actor_name, con=self.engine\n            )\n            # find the one with the highest occurrence in the records for 'Actor Name' field\n            # voting in RAG\n            vote_winner = related_actors[\"Actor Name\"].value_counts().idxmax()\n            logger.info(f\"Vote Winner: {vote_winner}\")\n            get_all_records_for_actor_name = f\"\"\"\n                        SELECT\n                        \"Actor Name\",\n                        \"Affiliation Start Date\",\n                        \"Affiliation End Date\",\n                        \"Affiliation To\",\n                        {model_name.replace(\"-\", \"_\")} as embedding\n                        FROM icews_actors WHERE \"Actor Name\" = '{vote_winner}';\n                        \"\"\"\n            # Execute the query\n            actor_df = pd.read_sql_query(\n                get_all_records_for_actor_name, con=self.engine\n            )\n\n        # Replace placeholders with extreme dates for ease of handling\n        actor_df[\"Affiliation Start Date\"] = actor_df[\"Affiliation Start Date\"].replace(\n            \"beginning of time\", \"1990-01-01\"\n        )\n        actor_df[\"Affiliation End Date\"] = actor_df[\"Affiliation End Date\"].replace(\n            \"end of time\", \"2025-12-31\"\n        )\n\n        # Convert dates to datetime format\n        actor_df[\"Affiliation Start Date\"] = pd.to_datetime(\n            actor_df[\"Affiliation Start Date\"]\n        )\n        actor_df[\"Affiliation End Date\"] = pd.to_datetime(\n            actor_df[\"Affiliation End Date\"]\n        )\n\n        # Extract year and month for both start and end dates\n        actor_df[\"start_year\"] = actor_df[\"Affiliation Start Date\"].dt.year\n        actor_df[\"start_month\"] = actor_df[\"Affiliation Start Date\"].dt.month\n        actor_df[\"end_year\"] = actor_df[\"Affiliation End Date\"].dt.year\n        actor_df[\"end_month\"] = actor_df[\"Affiliation End Date\"].dt.month\n\n        # order by start year and month\n        actor_df = actor_df.sort_values(by=[\"start_year\", \"start_month\"])\n        actor_df = actor_df.reset_index(drop=True)\n\n        # Prepare a figure object\n        fig = go.Figure()\n        first_embedding_value = actor_df.iloc[0][\"embedding\"]\n        logger.info(type(first_embedding_value))\n        first_embedding_value = torch.tensor(eval(first_embedding_value))\n        logger.info(first_embedding_value.shape)\n        # Iterate over each record to plot it\n        embeddings = []\n        for index, row in actor_df.iterrows():\n            # Adding a line for each affiliation duration\n            logger.info(row[\"start_year\"])\n            logger.info(index)\n            embedding_value = row[\"embedding\"]\n            if type(embedding_value) is str:\n                embedding_value = eval(embedding_value)\n            embedding_value = torch.tensor(embedding_value)\n            similarity = torch.nn.functional.cosine_similarity(\n                torch.tensor(first_embedding_value),\n                torch.tensor(embedding_value),\n                dim=0,\n            )\n            embeddings.append(embedding_value)\n            logger.info(f\"Similarity: {similarity}\")\n            line_color = self.__similarity_to_color(similarity)\n            fig.add_trace(\n                go.Scatter(\n                    x=[\n                        row[\"start_year\"] + row[\"start_month\"] / 12,\n                        row[\"end_year\"] + row[\"end_month\"] / 12,\n                    ],\n                    y=[index + 1, index + 1],\n                    mode=\"lines+markers+text\",  # Keep markers and text in the mode\n                    line=dict(color=\"rgb\" + str(line_color[:3]), width=4),\n                    name=row[\"Affiliation To\"],\n                    hoverinfo=\"text\",\n                    text=[\n                        f\"{row['Actor Name']} Affiliation To: {row['Affiliation To']}&lt;br&gt;Start: {row['start_year']}-{row['start_month']}&lt;br&gt;End: {row['end_year']}-{row['end_month']} &lt;br&gt;Similarity: {similarity:.2f}\",\n                        \"\",  # No text for the end point\n                    ],\n                    textposition=\"top center\",  # Adjust as needed for the starting point\n                )\n            )\n        min_start_year = (\n            actor_df[\"start_year\"].min() + actor_df[\"start_month\"].min() / 12 - 5\n        )  # Extend left by subtracting 1\n        max_end_year = (\n            actor_df[\"end_year\"].max() + actor_df[\"end_month\"].max() / 12 + 5\n        )  # Optionally extend right\n        max_index = (\n            actor_df.index.max() + 1\n        )  # Assuming index is continuous and starts from 0\n\n        # Update layout for readability and adjust x and y axis ranges\n        fig.update_layout(\n            title=f\"Affiliation Timeline for {actor_name}\",\n            xaxis_title=\"Year\",\n            yaxis_title=\"Index\",\n            xaxis=dict(\n                range=[min_start_year, max_end_year]  # Extend the x-axis to the left\n            ),\n            yaxis=dict(\n                range=[0, max_index + 2],  # Extend the y-axis to the top\n                tickmode=\"array\",\n                tickvals=actor_df.index.tolist(),\n                ticktext=actor_df.index.tolist(),\n            ),\n        )\n\n        fig.show()\n\n        # calculate the similarity between the embeddings\n        embeddings = torch.stack(embeddings)\n        similarity_matrix = torch.mm(embeddings, embeddings.T)\n        logger.info(similarity_matrix.shape)\n        # visualize the similarity matrix\n        fig = px.imshow(similarity_matrix)\n        fig.show()\n\n    def icews_actor_entity_timeline(self, actor_name: str):\n        \"\"\"\n        SELECT * FROM icews_actors WHERE \"Actor Name\" = actor_name;\n        :param actor_name:\n        :return:\n        \"\"\"\n        pass\n</code></pre>"},{"location":"Code/data_loader/load_icews/#TimelineKGQA.data_loader.load_icews.ICEWSDataLoader.__icews_actor_bert_embedding","title":"<code>__icews_actor_bert_embedding(prompt)</code>","text":"<p>Use the BERT model to embed the ICEWS actors :return:</p> Source code in <code>TimelineKGQA/data_loader/load_icews.py</code> <pre><code>def __icews_actor_bert_embedding(self, prompt: str):\n    \"\"\"\n    Use the BERT model to embed the ICEWS actors\n    :return:\n    \"\"\"\n    # Load pre-trained model tokenizer (vocabulary)\n    # Generate embeddings\n    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n    embeddings = model.encode(prompt)\n    return embeddings.tolist()\n</code></pre>"},{"location":"Code/data_loader/load_icews/#TimelineKGQA.data_loader.load_icews.ICEWSDataLoader.__similarity_to_color","title":"<code>__similarity_to_color(value)</code>  <code>staticmethod</code>","text":"<p>Returns an RGB color tuple (r, g, b) based on the given value between 0 and 1. The color gradient transitions from red (for 0) to green (for 1).</p> Source code in <code>TimelineKGQA/data_loader/load_icews.py</code> <pre><code>@staticmethod\ndef __similarity_to_color(value):\n    \"\"\"\n    Returns an RGB color tuple (r, g, b) based on the given value between 0 and 1.\n    The color gradient transitions from red (for 0) to green (for 1).\n    \"\"\"\n    # Clamp the value between 0 and 1\n    value = max(0, min(1, value))\n\n    # Map the value to the hue range (0 to 120)\n    hue = (value) * 1.2  # Scaling factor to adjust the hue range\n\n    # Convert the hue to RGB color tuple\n    rgb = colorsys.hsv_to_rgb(hue / 3, 1, 1)  # HSV to RGB conversion\n\n    # Convert RGB values to integers between 0 and 255\n    rgb = tuple(int(c * 255) for c in rgb)\n\n    return rgb\n</code></pre>"},{"location":"Code/data_loader/load_icews/#TimelineKGQA.data_loader.load_icews.ICEWSDataLoader.icews_actor_embedding_csv","title":"<code>icews_actor_embedding_csv(queue_embedding_filename, model_name, embedding_field_name=None, prompt_field=None)</code>","text":"<p>Load the embedding from the queue into the database :param queue_embedding_filename: :param model_name: :param embedding_field_name: :return:</p> Source code in <code>TimelineKGQA/data_loader/load_icews.py</code> <pre><code>def icews_actor_embedding_csv(\n    self,\n    queue_embedding_filename: str,\n    model_name: str,\n    embedding_field_name: str = None,\n    prompt_field: str = None,\n):\n    \"\"\"\n    Load the embedding from the queue into the database\n    :param queue_embedding_filename:\n    :param model_name:\n    :param embedding_field_name:\n    :return:\n    \"\"\"\n    if embedding_field_name is None:\n        embedding_field_name = model_name.replace(\"-\", \"_\")\n    self.__db_embedding_field(embedding_field_name)\n    conn = self.engine.connect()\n    df = pd.read_csv(DATA_DIR / \"ICEWS\" / \"processed\" / queue_embedding_filename)\n    for _, row in df.iterrows():\n        if row[\"model_name\"] != model_name:\n            continue\n\n        if model_name == \"bert\":\n            embedding = json.loads(json.loads(row[\"response\"]))[\"embedding\"]\n        else:\n            embedding = json.loads(json.loads(row[\"response\"]))[\"data\"][0][\n                \"embedding\"\n            ]\n        logger.debug(embedding)\n\n        if prompt_field is None:\n            prompt = row[\"prompt\"]\n            subject = prompt.split(\" affiliated to \")[0].replace(\"'\", \"''\")\n            object = prompt.split(\" affiliated to \")[1].replace(\"'\", \"''\")\n            # update the embedding column\n            conn.execute(\n                text(\n                    f\"\"\"\n                    UPDATE icews_actors\n                    SET {embedding_field_name} = array{embedding}::vector\n                    WHERE \"Actor Name\" = '{subject}' AND \"Affiliation To\" = '{object}';\n                    \"\"\"\n                )\n            )\n        else:\n            prompt = row[\"prompt\"]\n            prompt = prompt.replace(\"'\", \"''\")\n            conn.execute(\n                text(\n                    f\"\"\"\n                    UPDATE icews_actors\n                    SET {embedding_field_name} = array{embedding}::vector\n                    WHERE \"{prompt_field}\" = '{prompt}';\n                    \"\"\"\n                )\n            )\n        conn.commit()\n</code></pre>"},{"location":"Code/data_loader/load_icews/#TimelineKGQA.data_loader.load_icews.ICEWSDataLoader.icews_actor_entity_resolution_check","title":"<code>icews_actor_entity_resolution_check()</code>","text":"<p>Check the entity resolution for the ICEWS actors :return:</p> Source code in <code>TimelineKGQA/data_loader/load_icews.py</code> <pre><code>def icews_actor_entity_resolution_check(self):\n    \"\"\"\n    Check the entity resolution for the ICEWS actors\n    :return:\n    \"\"\"\n    pass\n</code></pre>"},{"location":"Code/data_loader/load_icews/#TimelineKGQA.data_loader.load_icews.ICEWSDataLoader.icews_actor_entity_timeline","title":"<code>icews_actor_entity_timeline(actor_name)</code>","text":"<p>SELECT * FROM icews_actors WHERE \"Actor Name\" = actor_name; :param actor_name: :return:</p> Source code in <code>TimelineKGQA/data_loader/load_icews.py</code> <pre><code>def icews_actor_entity_timeline(self, actor_name: str):\n    \"\"\"\n    SELECT * FROM icews_actors WHERE \"Actor Name\" = actor_name;\n    :param actor_name:\n    :return:\n    \"\"\"\n    pass\n</code></pre>"},{"location":"Code/data_loader/load_icews/#TimelineKGQA.data_loader.load_icews.ICEWSDataLoader.icews_actor_queue_actor_name_embedding","title":"<code>icews_actor_queue_actor_name_embedding(model_name='bert', field_name='Actor Name', embedding_field_name=None)</code>","text":"<p>embedding iceews actors with several models, add columns to original table embedding content will be subject affiliated to object :return:</p> Source code in <code>TimelineKGQA/data_loader/load_icews.py</code> <pre><code>def icews_actor_queue_actor_name_embedding(\n    self,\n    model_name: str = \"bert\",\n    field_name: str = \"Actor Name\",\n    embedding_field_name: str = None,\n):\n    \"\"\"\n    embedding iceews actors with several models, add columns to original table\n    embedding content will be subject affiliated to object\n    :return:\n    \"\"\"\n    if embedding_field_name is None:\n        embedding_field_name = model_name.replace(\"-\", \"_\")\n\n    # get the one that has not been embedded with SQL, embedding?.model_name is null\n    with self.engine.connect() as conn:\n        r = conn.execute(\n            text(\n                f\"\"\"\n                    SELECT \"{field_name}\"\n                    FROM icews_actors\n                    WHERE \"{embedding_field_name}\" IS NULL\n                    GROUP BY \"{field_name}\"\n                    ;\n                    \"\"\"\n            )\n        )\n\n        prompts = []\n        logger.info(self.queue_name)\n        logger.info(model_name)\n        for row in r.mappings():\n            prompt = row[field_name]\n            prompts.append(prompt)\n\n        # every 100 prompts, send to the queue\n        for i in range(0, len(prompts), 100):\n            if i + 100 &gt; len(prompts):\n                response = self.api.queue_create_embedding(\n                    prompts[i:], model_name=model_name, name=self.queue_name\n                )\n            else:\n                response = self.api.queue_create_embedding(\n                    prompts[i : i + 100],\n                    model_name=model_name,\n                    name=self.queue_name,\n                )\n                logger.info(response)\n            time.sleep(0.3)\n</code></pre>"},{"location":"Code/data_loader/load_icews/#TimelineKGQA.data_loader.load_icews.ICEWSDataLoader.icews_actor_queue_embedding","title":"<code>icews_actor_queue_embedding(model_name='Mixtral-8x7b', embedding_field_name=None)</code>","text":"<p>embedding iceews actors with several models, add columns to original table embedding content will be subject affiliated to object :return:</p> Source code in <code>TimelineKGQA/data_loader/load_icews.py</code> <pre><code>def icews_actor_queue_embedding(\n    self, model_name: str = \"Mixtral-8x7b\", embedding_field_name: str = None\n):\n    \"\"\"\n    embedding iceews actors with several models, add columns to original table\n    embedding content will be subject affiliated to object\n    :return:\n    \"\"\"\n    # add a json field for the embedding, then we can have {\"model_name\": \"embedding\"}\n    if embedding_field_name is None:\n        embedding_field_name = model_name.replace(\"-\", \"_\")\n\n    self.__db_embedding_field(embedding_field_name)\n    # get the one that has not been embedded with SQL, embedding?.model_name is null\n    with self.engine.connect() as conn:\n        r = conn.execute(\n            text(\n                f\"\"\"\n                    SELECT *\n                    FROM icews_actors\n                    WHERE {embedding_field_name} IS NULL\n                    ORDER BY id\n                    DESC\n                    ;\n                    \"\"\"\n            )\n        )\n\n        prompts = []\n        logger.info(self.queue_name)\n        logger.info(model_name)\n        for row in r.mappings():\n            logger.debug(row)\n            # record_id = row[\"id\"]\n            subject = row[\"Actor Name\"]\n            object = row[\"Affiliation To\"]\n            prompt = f\"{subject} affiliated to {object}\"\n            prompts.append(prompt)\n\n        # every 100 prompts, send to the queue\n        for i in range(0, len(prompts), 100):\n            if i + 100 &gt; len(prompts):\n                queued_prompts = prompts[i:]\n            else:\n                queued_prompts = prompts[i : i + 100]\n            response = self.api.queue_create_embedding(\n                queued_prompts,\n                model_name=model_name,\n                name=self.queue_name,\n            )\n            time.sleep(0.3)\n            logger.info(response)\n</code></pre>"},{"location":"Code/data_loader/load_icews/#TimelineKGQA.data_loader.load_icews.ICEWSDataLoader.icews_actor_subject_count_distribution","title":"<code>icews_actor_subject_count_distribution(actor_name, semantic_search=False, model_name='bert', embedding_field_name=None)</code>","text":"<p>Get all records for the actor_name and present the occurrence across a timeline. X-axis: Year Y-axis: Month When hovering over a point, it shows the value of \"Affiliation To\".</p> Source code in <code>TimelineKGQA/data_loader/load_icews.py</code> <pre><code>def icews_actor_subject_count_distribution(\n    self,\n    actor_name: str,\n    semantic_search: bool = False,\n    model_name: str = \"bert\",\n    embedding_field_name: str = None,\n):\n    \"\"\"\n    Get all records for the actor_name and present the occurrence across a timeline.\n    X-axis: Year\n    Y-axis: Month\n    When hovering over a point, it shows the value of \"Affiliation To\".\n    \"\"\"\n    if embedding_field_name is None:\n        embedding_field_name = model_name.replace(\"-\", \"_\")\n    if not semantic_search:\n        # SQL query to get all records for the specified actor_name\n        get_all_records_for_actor_name = f\"\"\"\n        SELECT\n        \"Actor Name\",\n        \"Affiliation Start Date\",\n        \"Affiliation End Date\",\n        \"Affiliation To\",\n        {embedding_field_name} as embedding\n        FROM icews_actors WHERE \"Actor Name\" = '{actor_name}';\n        \"\"\"\n        # Execute the query\n        actor_df = pd.read_sql_query(\n            get_all_records_for_actor_name, con=self.engine\n        )\n    else:\n        # get the embedding of the actor_name\n        if model_name == \"bert\":\n            actor_name_embedding = self.__icews_actor_bert_embedding(actor_name)\n        else:\n            actor_name_embedding = self.api.queue_embedding_and_wait_for_result(\n                [actor_name], model_name=model_name, name=\"tkgqa\"\n            )\n        # query\n        get_relevant_records_for_actor_name = f\"\"\"\n        SELECT\n        \"Actor Name\"\n        FROM icews_actors\n        WHERE {embedding_field_name} IS NOT NULL\n        ORDER BY {embedding_field_name} &lt;-&gt; array{actor_name_embedding}::vector\n        LIMIT 10;\n        \"\"\"\n        # Execute the query\n        related_actors = pd.read_sql_query(\n            get_relevant_records_for_actor_name, con=self.engine\n        )\n        # find the one with the highest occurrence in the records for 'Actor Name' field\n        # voting in RAG\n        vote_winner = related_actors[\"Actor Name\"].value_counts().idxmax()\n        logger.info(f\"Vote Winner: {vote_winner}\")\n        get_all_records_for_actor_name = f\"\"\"\n                    SELECT\n                    \"Actor Name\",\n                    \"Affiliation Start Date\",\n                    \"Affiliation End Date\",\n                    \"Affiliation To\",\n                    {model_name.replace(\"-\", \"_\")} as embedding\n                    FROM icews_actors WHERE \"Actor Name\" = '{vote_winner}';\n                    \"\"\"\n        # Execute the query\n        actor_df = pd.read_sql_query(\n            get_all_records_for_actor_name, con=self.engine\n        )\n\n    # Replace placeholders with extreme dates for ease of handling\n    actor_df[\"Affiliation Start Date\"] = actor_df[\"Affiliation Start Date\"].replace(\n        \"beginning of time\", \"1990-01-01\"\n    )\n    actor_df[\"Affiliation End Date\"] = actor_df[\"Affiliation End Date\"].replace(\n        \"end of time\", \"2025-12-31\"\n    )\n\n    # Convert dates to datetime format\n    actor_df[\"Affiliation Start Date\"] = pd.to_datetime(\n        actor_df[\"Affiliation Start Date\"]\n    )\n    actor_df[\"Affiliation End Date\"] = pd.to_datetime(\n        actor_df[\"Affiliation End Date\"]\n    )\n\n    # Extract year and month for both start and end dates\n    actor_df[\"start_year\"] = actor_df[\"Affiliation Start Date\"].dt.year\n    actor_df[\"start_month\"] = actor_df[\"Affiliation Start Date\"].dt.month\n    actor_df[\"end_year\"] = actor_df[\"Affiliation End Date\"].dt.year\n    actor_df[\"end_month\"] = actor_df[\"Affiliation End Date\"].dt.month\n\n    # order by start year and month\n    actor_df = actor_df.sort_values(by=[\"start_year\", \"start_month\"])\n    actor_df = actor_df.reset_index(drop=True)\n\n    # Prepare a figure object\n    fig = go.Figure()\n    first_embedding_value = actor_df.iloc[0][\"embedding\"]\n    logger.info(type(first_embedding_value))\n    first_embedding_value = torch.tensor(eval(first_embedding_value))\n    logger.info(first_embedding_value.shape)\n    # Iterate over each record to plot it\n    embeddings = []\n    for index, row in actor_df.iterrows():\n        # Adding a line for each affiliation duration\n        logger.info(row[\"start_year\"])\n        logger.info(index)\n        embedding_value = row[\"embedding\"]\n        if type(embedding_value) is str:\n            embedding_value = eval(embedding_value)\n        embedding_value = torch.tensor(embedding_value)\n        similarity = torch.nn.functional.cosine_similarity(\n            torch.tensor(first_embedding_value),\n            torch.tensor(embedding_value),\n            dim=0,\n        )\n        embeddings.append(embedding_value)\n        logger.info(f\"Similarity: {similarity}\")\n        line_color = self.__similarity_to_color(similarity)\n        fig.add_trace(\n            go.Scatter(\n                x=[\n                    row[\"start_year\"] + row[\"start_month\"] / 12,\n                    row[\"end_year\"] + row[\"end_month\"] / 12,\n                ],\n                y=[index + 1, index + 1],\n                mode=\"lines+markers+text\",  # Keep markers and text in the mode\n                line=dict(color=\"rgb\" + str(line_color[:3]), width=4),\n                name=row[\"Affiliation To\"],\n                hoverinfo=\"text\",\n                text=[\n                    f\"{row['Actor Name']} Affiliation To: {row['Affiliation To']}&lt;br&gt;Start: {row['start_year']}-{row['start_month']}&lt;br&gt;End: {row['end_year']}-{row['end_month']} &lt;br&gt;Similarity: {similarity:.2f}\",\n                    \"\",  # No text for the end point\n                ],\n                textposition=\"top center\",  # Adjust as needed for the starting point\n            )\n        )\n    min_start_year = (\n        actor_df[\"start_year\"].min() + actor_df[\"start_month\"].min() / 12 - 5\n    )  # Extend left by subtracting 1\n    max_end_year = (\n        actor_df[\"end_year\"].max() + actor_df[\"end_month\"].max() / 12 + 5\n    )  # Optionally extend right\n    max_index = (\n        actor_df.index.max() + 1\n    )  # Assuming index is continuous and starts from 0\n\n    # Update layout for readability and adjust x and y axis ranges\n    fig.update_layout(\n        title=f\"Affiliation Timeline for {actor_name}\",\n        xaxis_title=\"Year\",\n        yaxis_title=\"Index\",\n        xaxis=dict(\n            range=[min_start_year, max_end_year]  # Extend the x-axis to the left\n        ),\n        yaxis=dict(\n            range=[0, max_index + 2],  # Extend the y-axis to the top\n            tickmode=\"array\",\n            tickvals=actor_df.index.tolist(),\n            ticktext=actor_df.index.tolist(),\n        ),\n    )\n\n    fig.show()\n\n    # calculate the similarity between the embeddings\n    embeddings = torch.stack(embeddings)\n    similarity_matrix = torch.mm(embeddings, embeddings.T)\n    logger.info(similarity_matrix.shape)\n    # visualize the similarity matrix\n    fig = px.imshow(similarity_matrix)\n    fig.show()\n</code></pre>"},{"location":"Code/data_loader/load_icews/#TimelineKGQA.data_loader.load_icews.ICEWSDataLoader.icews_actor_unified_kg","title":"<code>icews_actor_unified_kg()</code>","text":"<p>run sql query <pre><code>CREATE TABLE unified_kg_icews_actor AS\nSELECT\n    \"Actor Name\" AS subject,\n    json_build_object('Country', \"Country\", 'Aliases', \"Aliases\") AS subject_json,\n    'Affiliation To' AS predicate,\n    '{}'::json AS predicate_json, -- Correctly cast empty JSON object\n    \"Affiliation To\" AS object,\n    '{}'::json AS object_json, -- Correctly cast empty JSON object\n    \"Affiliation Start Date\" AS start_time,\n    \"Affiliation End Date\" AS end_time\nFROM\n    icews_actors;\n</code></pre> :return:</p> Source code in <code>TimelineKGQA/data_loader/load_icews.py</code> <pre><code>def icews_actor_unified_kg(self):\n    \"\"\"\n    run sql query\n    ```sql\n    CREATE TABLE unified_kg_icews_actor AS\n    SELECT\n        \"Actor Name\" AS subject,\n        json_build_object('Country', \"Country\", 'Aliases', \"Aliases\") AS subject_json,\n        'Affiliation To' AS predicate,\n        '{}'::json AS predicate_json, -- Correctly cast empty JSON object\n        \"Affiliation To\" AS object,\n        '{}'::json AS object_json, -- Correctly cast empty JSON object\n        \"Affiliation Start Date\" AS start_time,\n        \"Affiliation End Date\" AS end_time\n    FROM\n        icews_actors;\n    ```\n    :return:\n    \"\"\"\n    cursor = self.engine.connect()\n    cursor.execute(\n        text(\n            \"\"\"\n        DO\n        $$\n            BEGIN\n                -- Attempt to create the table if it doesn't exist\n                -- This part only creates the table structure\n                IF NOT EXISTS (SELECT FROM pg_tables WHERE schemaname = 'public' AND tablename = 'unified_kg_icews_actor') THEN\n                    CREATE TABLE public.unified_kg_icews_actor\n                    (\n                        id             SERIAL PRIMARY KEY,\n                        subject        TEXT,\n                        subject_json   JSON,\n                        predicate      TEXT,\n                        predicate_json JSON DEFAULT '{}'::json,\n                        object         TEXT,\n                        object_json    JSON DEFAULT '{}'::json,\n                        start_time     TEXT,\n                        end_time       TEXT\n                    );\n                END IF;\n                TRUNCATE TABLE public.unified_kg_icews_actor;\n                INSERT INTO unified_kg_icews_actor(\n                    subject,\n                    subject_json,\n                    predicate,\n                    predicate_json,\n                    object,\n                    object_json,\n                    start_time,\n                    end_time\n                )\n                SELECT\n                    \"Actor Name\" AS subject,\n                    json_build_object('Country', \"Country\", 'Aliases', \"Aliases\") AS subject_json,\n                    'Affiliation To' AS predicate,\n                    '{}'::json AS predicate_json, -- Correctly cast empty JSON object\n                    \"Affiliation To\" AS object,\n                    '{}'::json AS object_json, -- Correctly cast empty JSON object\n                    \"Affiliation Start Date\" AS start_time,\n                    \"Affiliation End Date\" AS end_time\n                FROM\n                    icews_actors;\n            END\n        $$;\n\n        \"\"\"\n        )\n    )\n    cursor.commit()\n    cursor.close()\n</code></pre>"},{"location":"Code/data_loader/load_icews/#TimelineKGQA.data_loader.load_icews.ICEWSDataLoader.icews_explore_data","title":"<code>icews_explore_data()</code>","text":"<p>Read the ICEWS_Sector, as it is a tree, plot a tree for this. :return:</p> Source code in <code>TimelineKGQA/data_loader/load_icews.py</code> <pre><code>def icews_explore_data(self):\n    \"\"\"\n    Read the ICEWS_Sector, as it is a tree, plot a tree for this.\n    :return:\n    \"\"\"\n\n    df = pd.read_sql_table(\"icews_sectors\", con=self.engine)\n    # Initialize lists to hold the transformed data\n    names = []\n    parents = []\n\n    logger.info(df.head())\n\n    # Track the last seen name at each level to establish parent-child relationships\n    last_seen = {-1: \"\"}  # Root has no name\n\n    # Iterate over the rows in the original dataframe\n    for _, row in df.iterrows():\n        for level in range(len(row)):\n            logger.info(f\"Level: {level}\")\n            # Check if the cell is not empty\n            if not pd.isnull(row[level]):\n                # This level's name\n                name = row[level]\n                # Parent is the last seen name in the previous level\n                parent = last_seen[level - 1]\n                # Update this level's last seen name\n                last_seen[level] = name\n                # If this name at this level is not already added, add it to the lists\n                if (name not in names) or parents[names.index(name)] != parent:\n                    names.append(name)\n                    parents.append(parent)\n                break  # Move to the next row once the first non-empty cell is processed\n\n    # Creating a new dataframe from the transformed data\n    transformed_df = pd.DataFrame({\"name\": names, \"parent\": parents})\n\n    # Display the first few rows of the transformed dataframe\n    logger.info(transformed_df.head())\n\n    # Creating a tree diagram with Plotly\n    fig = go.Figure(\n        go.Treemap(\n            labels=transformed_df[\"name\"],\n            parents=transformed_df[\"parent\"],\n        )\n    )\n\n    fig.update_layout(margin=dict(t=0, l=0, r=0, b=0))\n\n    if self.view_sector_tree_web:\n        fig.show()\n</code></pre>"},{"location":"Code/data_loader/load_icews/#TimelineKGQA.data_loader.load_icews.ICEWSDataLoader.icews_load_data","title":"<code>icews_load_data()</code>","text":"<p>Before doing anything, you will need to download the ICEWS data from the Harvard Dataverse.</p> \"ICEWS Coded Event Data\", <p>https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/28075</p> <p>After downloading the data, extract the zip file and put the data in the following directory:     TimelineKGQA/TimelineKGQA/data/icews_events_data/ICEWS/ICEWS Coded Event Data</p> <p>:return:</p> Source code in <code>TimelineKGQA/data_loader/load_icews.py</code> <pre><code>def icews_load_data(self):\n    \"\"\"\n    Before doing anything, you will need to download the ICEWS data from the Harvard Dataverse.\n\n    Data name is: \"ICEWS Coded Event Data\",\n        https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/28075\n\n    After downloading the data, extract the zip file and put the data in the following directory:\n        TimelineKGQA/TimelineKGQA/data/icews_events_data/ICEWS/ICEWS Coded Event Data\n\n    :return:\n    \"\"\"\n\n    if self.data_type == \"all\" or self.data_type == \"icews\":\n\n        # loop the folder, and unzip all the files ending with .zip\n        # it will override the data if it is running twice\n        for file in os.listdir(DATA_ICEWS_EVENTS_DATA_DIR):\n            if file.endswith(\".zip\"):\n                # unzip the file\n                zip_path = DATA_ICEWS_EVENTS_DATA_DIR / file\n                logger.info(f\"Unzipping {zip_path}\")\n                with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n                    zip_ref.extractall(DATA_ICEWS_EVENTS_DATA_DIR)\n\n        # read all .tab or .csv files into the df and check their column distribution\n        # pandas read from tab, csv files\n        # have done the check, all files have the consistent same column names\n        combined_df = None\n        for file in os.listdir(DATA_ICEWS_EVENTS_DATA_DIR):\n            if file.endswith(\".tab\") or file.endswith(\".csv\"):\n                tab_path = DATA_ICEWS_EVENTS_DATA_DIR / file\n                logger.info(f\"Reading {tab_path}\")\n                df = pd.read_csv(tab_path, sep=\"\\t\", low_memory=False)\n                if combined_df is None:\n                    combined_df = df\n                else:\n                    # combine df and combined_df\n                    combined_df = pd.concat([combined_df, df], ignore_index=True)\n        logger.info(\"Loading data into database\")\n        combined_df.to_sql(\n            \"icews\", con=self.engine, if_exists=\"replace\", index=False\n        )\n\n    if self.data_type == \"all\" or self.data_type == \"icews_dicts\":\n        # load the ICEWS Dictionaries into the database for further review\n        logger.info(\"Loading dictionaries into database\")\n        # loop all the files in the directory, and saving them into the database\n        for file in os.listdir(DATA_ICEWS_DICTS_DATA_DIR):\n            if file.endswith(\".csv\"):\n                csv_path = DATA_ICEWS_DICTS_DATA_DIR / file\n                logger.info(f\"Reading {csv_path}\")\n                # if sector in the file name, then the csv do not have header\n                if \"sector\" in file:\n                    df = pd.read_csv(csv_path, header=None, low_memory=False)\n                else:\n                    df = pd.read_csv(csv_path, low_memory=False)\n                table_name = file.rsplit(\".\", 2)[0].replace(\".\", \"_\")\n                logger.info(f\"Loading {table_name} into database\")\n                if \"id\" not in df.columns:\n                    df[\"id\"] = range(1, 1 + len(df))\n                df.to_sql(\n                    table_name, con=self.engine, if_exists=\"replace\", index=False\n                )\n</code></pre>"},{"location":"Code/demo/","title":"Index","text":""},{"location":"Code/demo/data_exploration/","title":"Data exploration","text":""},{"location":"Code/demo/sparql_based_rag/","title":"Sparql based rag","text":""},{"location":"Code/demo/sparql_based_rag/#TimelineKGQA.demo.sparql_based_rag.on_chat_input","title":"<code>on_chat_input(e)</code>","text":"<p>Capture chat text input.</p> Source code in <code>TimelineKGQA/demo/sparql_based_rag.py</code> <pre><code>def on_chat_input(e: me.InputEvent):\n    \"\"\"Capture chat text input.\"\"\"\n    state = me.state(State)\n    state.question_id = e.value\n</code></pre>"},{"location":"Code/demo/sparql_based_rag/#TimelineKGQA.demo.sparql_based_rag.on_click_submit_chat_msg","title":"<code>on_click_submit_chat_msg(e)</code>","text":"<p>Handles submitting a chat message.</p> Source code in <code>TimelineKGQA/demo/sparql_based_rag.py</code> <pre><code>def on_click_submit_chat_msg(e: me.ClickEvent | me.InputEnterEvent):\n    \"\"\"Handles submitting a chat message.\"\"\"\n    state = me.state(State)\n    if state.in_progress or not state.input:\n        return\n    input = state.input\n    state.input = \"\"\n    yield\n</code></pre>"},{"location":"Code/demo/sparql_based_rag/#TimelineKGQA.demo.sparql_based_rag.on_rewrite_input","title":"<code>on_rewrite_input(e)</code>","text":"<p>Capture rewrite text input.</p> Source code in <code>TimelineKGQA/demo/sparql_based_rag.py</code> <pre><code>def on_rewrite_input(e: me.InputEvent):\n    \"\"\"Capture rewrite text input.\"\"\"\n    state = me.state(State)\n    state.preview_rewrite = e.value\n</code></pre>"},{"location":"Code/rag/","title":"Index","text":""},{"location":"Code/rag/finetune_llm/","title":"Finetune llm","text":""},{"location":"Code/rag/finetune_llm/#TimelineKGQA.rag.finetune_llm.FinetuneLLM","title":"<code>FinetuneLLM</code>","text":"<p>Fine tune the LLM with the train QA pairs</p> <p>We can not provide a fair game between this and RAG, and text2sql. They all can be information retrieval task, which will be more explainable.</p> <p>And if the LLM do not have the knowledge, it will not make sense even you fine tune it. So what we propose should be, we ingest the information to the LLM, and ask it from other perspective.</p> <p>Comparison of experiments:</p> <ul> <li>Fine tuned QA with paraphrased questions</li> <li>Fine tuned QA, answer as question</li> <li>Fine tuned with simple QA, and ask the relevant medium question</li> </ul> Source code in <code>TimelineKGQA/rag/finetune_llm.py</code> <pre><code>class FinetuneLLM:\n    \"\"\"\n    Fine tune the LLM with the train QA pairs\n\n    We can not provide a fair game between this and RAG, and text2sql.\n    They all can be information retrieval task, which will be more explainable.\n\n    And if the LLM do not have the knowledge, it will not make sense even you fine tune it.\n    So what we propose should be, we ingest the information to the LLM, and ask it from other perspective.\n\n    Comparison of experiments:\n\n    - Fine tuned QA with paraphrased questions\n    - Fine tuned QA, answer as question\n    - Fine tuned with simple QA, and ask the relevant medium question\n    \"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        host: str,\n        port: int,\n        user: str,\n        password: str,\n        db_name: str,\n        fine_tune_model: str = \"gpt-3.5-turbo-1106\",\n        paraphrased_model: str = \"gpt-3.5-turbo-1106\",\n    ):\n        \"\"\"\n        Args:\n            table_name (str): The table name\n            host (str): The host\n            port (int): The port\n            user (str): The user\n            password (str): The password\n            db_name (str): The db name\n            fine_tune_model (str): The fine tune model\n            paraphrased_model (str): The paraphrased model\n\n\n        \"\"\"\n        self.table_name = table_name\n        self.host = host\n        self.port = port\n        self.user = user\n        self.password = password\n        self.db_name = db_name\n\n        self.engine = create_engine(\n            f\"postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.db_name}\"\n        )\n\n        self.fine_tune_model = fine_tune_model\n        self.paraphrased_model = paraphrased_model\n\n    def generate_finetune_data_paraphrased_questions(\n        self, number_of_questions: int = 10, identifier_file_name: str = \"paraphrased\"\n    ):\n        \"\"\"\n        Args:\n            number_of_questions (int): The number of questions\n            identifier_file_name (str): The identifier file name\n\n        \"\"\"\n        paraphrased_finetune_query = f\"\"\"\n        SELECT * FROM unified_kg_icews_actor_questions\n        WHERE question_type = 'timeline_recovery'\n              or question_type = 'temporal_constrainted_retrieval'\n        ORDER BY events\n        LIMIT {number_of_questions} * 3;\n        \"\"\"\n\n        questions_df = pd.read_sql(paraphrased_finetune_query, self.engine)\n        fine_tune_data = []\n        evaluation_data = []\n\n        for index, row in tqdm(\n            questions_df.iterrows(),\n            total=questions_df.shape[0],\n            desc=\"Generating finetune data\",\n        ):\n            question = row[\"question\"]\n            answer = row[\"answer\"]\n\n            fine_tune_data.append(\n                {\n                    \"messages\": [\n                        {\n                            \"role\": \"system\",\n                            \"content\": \"You are a QA robot expert in temporal related questions.\",\n                        },\n                        {\"role\": \"user\", \"content\": question},\n                        {\"role\": \"assistant\", \"content\": answer},\n                    ]\n                }\n            )\n\n            # paraphrased question\n            paraphrased_question = self.paraphrased_question(question)\n            if paraphrased_question:\n                evaluation_data.append(\n                    {\n                        \"question\": question,\n                        \"paraphrased_question\": paraphrased_question,\n                        \"answer\": answer,\n                    }\n                )\n\n        # dump it into a jsonl file\n        with open(\n            FINE_TUNE_LOGS_DIR / f\"finetune_data_{identifier_file_name}.jsonl\", \"w\"\n        ) as f:\n            for line in fine_tune_data:\n                f.write(json.dumps(line) + \"\\n\")\n\n        with open(\n            FINE_TUNE_LOGS_DIR / f\"evaluation_data_{identifier_file_name}.jsonl\", \"w\"\n        ) as f:\n            for line in evaluation_data:\n                f.write(json.dumps(line) + \"\\n\")\n\n        self.create_fine_tune_task(\n            train_filename=(\n                FINE_TUNE_LOGS_DIR / f\"finetune_data_{identifier_file_name}.jsonl\"\n            ).as_posix(),\n            eval_filename=(\n                FINE_TUNE_LOGS_DIR / f\"evaluation_data_{identifier_file_name}.jsonl\"\n            ).as_posix(),\n            identifier_file_name=identifier_file_name,\n        )\n\n    def generate_finetune_data_answer_as_question(\n        self, number_of_questions: int = 10, identifier_file_name: str = \"a_as_q\"\n    ):\n        \"\"\"\n        Generate the finetune data\n\n        Args:\n            number_of_questions (int): The number of questions\n            identifier_file_name (str): The identifier file name\n\n        \"\"\"\n        simple_finetune_query = f\"\"\"\n        SELECT * FROM unified_kg_icews_actor_questions\n        WHERE question_type = 'timeline_recovery'\n           or question_type = 'temporal_constrainted_retrieval'\n        ORDER BY events\n        LIMIT {number_of_questions} * 3;\n        \"\"\"\n\n        questions_df = pd.read_sql(simple_finetune_query, self.engine)\n        fine_tune_data = []\n        evaluation_data = []\n\n        for index, row in tqdm(\n            questions_df.iterrows(),\n            total=questions_df.shape[0],\n            desc=\"Generating finetune data\",\n        ):\n            question = row[\"question\"]\n            answer = row[\"answer\"]\n            if row[\"question_type\"] == \"timeline_recovery\":\n                evaluation_data.append(\n                    {\n                        \"question\": question,\n                        \"answer\": answer,\n                    }\n                )\n                continue\n\n            fine_tune_data.append(\n                {\n                    \"messages\": [\n                        {\n                            \"role\": \"system\",\n                            \"content\": \"You are a QA robot expert in temporal related questions.\",\n                        },\n                        {\"role\": \"user\", \"content\": question},\n                        {\"role\": \"assistant\", \"content\": answer},\n                    ]\n                }\n            )\n        # dump it into a jsonl file\n        with open(\n            FINE_TUNE_LOGS_DIR / f\"finetune_data_{identifier_file_name}.jsonl\", \"w\"\n        ) as f:\n            for line in fine_tune_data:\n                f.write(json.dumps(line) + \"\\n\")\n\n        with open(\n            FINE_TUNE_LOGS_DIR / f\"evaluation_data_{identifier_file_name}.jsonl\", \"w\"\n        ) as f:\n            for line in evaluation_data:\n                f.write(json.dumps(line) + \"\\n\")\n\n        self.create_fine_tune_task(\n            train_filename=(\n                FINE_TUNE_LOGS_DIR / f\"finetune_data_{identifier_file_name}.jsonl\"\n            ).as_posix(),\n            eval_filename=(\n                FINE_TUNE_LOGS_DIR / f\"evaluation_data_{identifier_file_name}.jsonl\"\n            ).as_posix(),\n            identifier_file_name=identifier_file_name,\n        )\n\n    def generate_finetune_data_simple_vs_medium(\n        self,\n        number_of_questions: int = 30,\n        identifier_file_name: str = \"simple_vs_medium\",\n    ):\n        \"\"\"\n        Args:\n            number_of_questions (int): The number of questions\n            identifier_file_name (str): The identifier file name\n\n        \"\"\"\n        simple_vs_medium_query = f\"\"\"\n        SELECT * FROM unified_kg_icews_actor_questions\n        WHERE question_level = 'medium'\n        ORDER BY events\n        LIMIT {number_of_questions} * 3;\n        \"\"\"\n        questions_df = pd.read_sql(\n            simple_vs_medium_query,\n            self.engine,\n        )\n        fine_tune_data = []\n        evaluation_data = []\n        for index, row in tqdm(\n            questions_df.iterrows(),\n            total=questions_df.shape[0],\n            desc=\"Generating finetune data\",\n        ):\n            question = row[\"question\"]\n            answer = row[\"answer\"]\n\n            evaluation_data.append(\n                {\n                    \"question\": question,\n                    \"answer\": answer,\n                }\n            )\n\n            events = row[\"events\"]\n            for event in events:\n                logger.info(event)\n                # query the db for simple question with this event\n                simple_query = (\n                    \"\"\"\n                    SELECT * FROM unified_kg_icews_actor_questions\n                    WHERE events = '{\"\"\"\n                    + event.replace(\"'\", \"''\")\n                    + \"\"\"}'\n                AND question_level = 'simple'\n                LIMIT 3;\n                \"\"\"\n                )\n                logger.info(simple_query)\n                simple_question_df = pd.read_sql(\n                    simple_query,\n                    self.engine,\n                )\n                logger.info(simple_question_df)\n                if simple_question_df.empty:\n                    continue\n\n                for _, simple_row in simple_question_df.iterrows():\n                    simple_question = simple_row[\"question\"]\n                    simple_answer = simple_row[\"answer\"]\n                    fine_tune_data.append(\n                        {\n                            \"messages\": [\n                                {\n                                    \"role\": \"system\",\n                                    \"content\": \"You are a QA robot expert in temporal related questions.\",\n                                },\n                                {\"role\": \"user\", \"content\": simple_question},\n                                {\"role\": \"assistant\", \"content\": simple_answer},\n                            ]\n                        }\n                    )\n\n        # dump it into a jsonl file\n        with open(\n            FINE_TUNE_LOGS_DIR / f\"finetune_data_{identifier_file_name}.jsonl\", \"w\"\n        ) as f:\n            for line in fine_tune_data:\n                f.write(json.dumps(line) + \"\\n\")\n\n        with open(\n            FINE_TUNE_LOGS_DIR / f\"evaluation_data_{identifier_file_name}.jsonl\", \"w\"\n        ) as f:\n            for line in evaluation_data:\n                f.write(json.dumps(line) + \"\\n\")\n\n        self.create_fine_tune_task(\n            train_filename=(\n                FINE_TUNE_LOGS_DIR / f\"finetune_data_{identifier_file_name}.jsonl\"\n            ).as_posix(),\n            eval_filename=(\n                FINE_TUNE_LOGS_DIR / f\"evaluation_data_{identifier_file_name}.jsonl\"\n            ).as_posix(),\n            identifier_file_name=identifier_file_name,\n        )\n\n    def create_fine_tune_task(\n        self, train_filename: str, eval_filename: str, identifier_file_name: str\n    ):\n        \"\"\"\n        Fine tune the LLM\n\n        Args:\n            train_filename (str): The filename\n            eval_filename (str): The evaluation filename\n            identifier_file_name (str): The identifier, output csv file identifier\n        \"\"\"\n        # Upload the training file\n        response = client.files.create(\n            file=open(train_filename, \"rb\"), purpose=\"fine-tune\"\n        )\n        logger.info(response.id)\n        # logger.info(f\"Uploaded training file: {response['id']}\")\n        fine_tune_res = client.fine_tuning.jobs.create(\n            training_file=response.id, model=self.fine_tune_model\n        )\n        logger.info(fine_tune_res.id)\n\n        status = \"pending\"\n        while status not in [\"succeeded\", \"failed\"]:\n            response = client.fine_tuning.jobs.retrieve(\n                fine_tuning_job_id=fine_tune_res.id\n            )\n            status = response.status\n            logger.info(f\"Fine-tune job status: {status}\")\n            time.sleep(30)  # Wait for 30 seconds before checking the status again\n\n        # when success, then call the model to do the QA, for further evaluation\n        if status == \"succeeded\":\n            logger.info(\"Fine-tuning succeeded.\")\n            fine_tuned_model = response.fine_tuned_model\n            logger.info(f\"Fine-tuned model: {fine_tuned_model}\")\n        else:\n            logger.info(\"Fine-tuning failed.\")\n            return\n\n        self.evaluation_fine_tune_task(\n            eval_filename,\n            fine_tuned_model,\n            output_filename=f\"evaluation_results_{identifier_file_name}.csv\",\n        )\n\n    @staticmethod\n    def evaluation_fine_tune_task(\n        eval_filename: str, fine_tuned_model: str, output_filename: str\n    ):\n        # Evaluation\n        evl_df = pd.read_json(eval_filename, lines=True)\n        for index, row in evl_df.iterrows():\n            question = row[\"question\"]\n\n            response = client.chat.completions.create(\n                model=fine_tuned_model,\n                messages=[\n                    {\"role\": \"user\", \"content\": question},\n                ],\n                temperature=0.0,\n            )\n            logger.info(question)\n            logger.info(response.choices[0].message.content)\n            fine_tune_ans = response.choices[0].message.content\n            evl_df.loc[index, \"fine_tune_ans\"] = fine_tune_ans\n        evl_df.to_csv(FINE_TUNE_LOGS_DIR / output_filename, index=False)\n\n    def paraphrased_question(self, question: str):\n        \"\"\"\n        Args:\n            question (str): The question\n\n        Returns:\n            str: The paraphrased question\n\n        \"\"\"\n\n        try:\n            prompt = f\"\"\"\n            Please paraphrase the following question with the same meaning but another way to ask:\n            {question}\n            Return it in json format with the key \"paraphrased_question\"\n            \"\"\"\n            response = client.chat.completions.create(\n                model=self.paraphrased_model,\n                messages=[\n                    {\"role\": \"user\", \"content\": prompt},\n                ],\n                temperature=0.0,\n                response_format={\"type\": \"json_object\"},\n            )\n            logger.info(response.choices[0].message.content)\n            paraphrased_question_str = response.choices[0].message.content\n            paraphrased_question = json.loads(paraphrased_question_str).get(\n                \"paraphrased_question\", \"\"\n            )\n            return paraphrased_question\n        except Exception as e:\n            logger.exception(e)\n            return \"\"\n</code></pre>"},{"location":"Code/rag/finetune_llm/#TimelineKGQA.rag.finetune_llm.FinetuneLLM.__init__","title":"<code>__init__(table_name, host, port, user, password, db_name, fine_tune_model='gpt-3.5-turbo-1106', paraphrased_model='gpt-3.5-turbo-1106')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The table name</p> required <code>host</code> <code>str</code> <p>The host</p> required <code>port</code> <code>int</code> <p>The port</p> required <code>user</code> <code>str</code> <p>The user</p> required <code>password</code> <code>str</code> <p>The password</p> required <code>db_name</code> <code>str</code> <p>The db name</p> required <code>fine_tune_model</code> <code>str</code> <p>The fine tune model</p> <code>'gpt-3.5-turbo-1106'</code> <code>paraphrased_model</code> <code>str</code> <p>The paraphrased model</p> <code>'gpt-3.5-turbo-1106'</code> Source code in <code>TimelineKGQA/rag/finetune_llm.py</code> <pre><code>def __init__(\n    self,\n    table_name: str,\n    host: str,\n    port: int,\n    user: str,\n    password: str,\n    db_name: str,\n    fine_tune_model: str = \"gpt-3.5-turbo-1106\",\n    paraphrased_model: str = \"gpt-3.5-turbo-1106\",\n):\n    \"\"\"\n    Args:\n        table_name (str): The table name\n        host (str): The host\n        port (int): The port\n        user (str): The user\n        password (str): The password\n        db_name (str): The db name\n        fine_tune_model (str): The fine tune model\n        paraphrased_model (str): The paraphrased model\n\n\n    \"\"\"\n    self.table_name = table_name\n    self.host = host\n    self.port = port\n    self.user = user\n    self.password = password\n    self.db_name = db_name\n\n    self.engine = create_engine(\n        f\"postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.db_name}\"\n    )\n\n    self.fine_tune_model = fine_tune_model\n    self.paraphrased_model = paraphrased_model\n</code></pre>"},{"location":"Code/rag/finetune_llm/#TimelineKGQA.rag.finetune_llm.FinetuneLLM.create_fine_tune_task","title":"<code>create_fine_tune_task(train_filename, eval_filename, identifier_file_name)</code>","text":"<p>Fine tune the LLM</p> <p>Parameters:</p> Name Type Description Default <code>train_filename</code> <code>str</code> <p>The filename</p> required <code>eval_filename</code> <code>str</code> <p>The evaluation filename</p> required <code>identifier_file_name</code> <code>str</code> <p>The identifier, output csv file identifier</p> required Source code in <code>TimelineKGQA/rag/finetune_llm.py</code> <pre><code>def create_fine_tune_task(\n    self, train_filename: str, eval_filename: str, identifier_file_name: str\n):\n    \"\"\"\n    Fine tune the LLM\n\n    Args:\n        train_filename (str): The filename\n        eval_filename (str): The evaluation filename\n        identifier_file_name (str): The identifier, output csv file identifier\n    \"\"\"\n    # Upload the training file\n    response = client.files.create(\n        file=open(train_filename, \"rb\"), purpose=\"fine-tune\"\n    )\n    logger.info(response.id)\n    # logger.info(f\"Uploaded training file: {response['id']}\")\n    fine_tune_res = client.fine_tuning.jobs.create(\n        training_file=response.id, model=self.fine_tune_model\n    )\n    logger.info(fine_tune_res.id)\n\n    status = \"pending\"\n    while status not in [\"succeeded\", \"failed\"]:\n        response = client.fine_tuning.jobs.retrieve(\n            fine_tuning_job_id=fine_tune_res.id\n        )\n        status = response.status\n        logger.info(f\"Fine-tune job status: {status}\")\n        time.sleep(30)  # Wait for 30 seconds before checking the status again\n\n    # when success, then call the model to do the QA, for further evaluation\n    if status == \"succeeded\":\n        logger.info(\"Fine-tuning succeeded.\")\n        fine_tuned_model = response.fine_tuned_model\n        logger.info(f\"Fine-tuned model: {fine_tuned_model}\")\n    else:\n        logger.info(\"Fine-tuning failed.\")\n        return\n\n    self.evaluation_fine_tune_task(\n        eval_filename,\n        fine_tuned_model,\n        output_filename=f\"evaluation_results_{identifier_file_name}.csv\",\n    )\n</code></pre>"},{"location":"Code/rag/finetune_llm/#TimelineKGQA.rag.finetune_llm.FinetuneLLM.generate_finetune_data_answer_as_question","title":"<code>generate_finetune_data_answer_as_question(number_of_questions=10, identifier_file_name='a_as_q')</code>","text":"<p>Generate the finetune data</p> <p>Parameters:</p> Name Type Description Default <code>number_of_questions</code> <code>int</code> <p>The number of questions</p> <code>10</code> <code>identifier_file_name</code> <code>str</code> <p>The identifier file name</p> <code>'a_as_q'</code> Source code in <code>TimelineKGQA/rag/finetune_llm.py</code> <pre><code>def generate_finetune_data_answer_as_question(\n    self, number_of_questions: int = 10, identifier_file_name: str = \"a_as_q\"\n):\n    \"\"\"\n    Generate the finetune data\n\n    Args:\n        number_of_questions (int): The number of questions\n        identifier_file_name (str): The identifier file name\n\n    \"\"\"\n    simple_finetune_query = f\"\"\"\n    SELECT * FROM unified_kg_icews_actor_questions\n    WHERE question_type = 'timeline_recovery'\n       or question_type = 'temporal_constrainted_retrieval'\n    ORDER BY events\n    LIMIT {number_of_questions} * 3;\n    \"\"\"\n\n    questions_df = pd.read_sql(simple_finetune_query, self.engine)\n    fine_tune_data = []\n    evaluation_data = []\n\n    for index, row in tqdm(\n        questions_df.iterrows(),\n        total=questions_df.shape[0],\n        desc=\"Generating finetune data\",\n    ):\n        question = row[\"question\"]\n        answer = row[\"answer\"]\n        if row[\"question_type\"] == \"timeline_recovery\":\n            evaluation_data.append(\n                {\n                    \"question\": question,\n                    \"answer\": answer,\n                }\n            )\n            continue\n\n        fine_tune_data.append(\n            {\n                \"messages\": [\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are a QA robot expert in temporal related questions.\",\n                    },\n                    {\"role\": \"user\", \"content\": question},\n                    {\"role\": \"assistant\", \"content\": answer},\n                ]\n            }\n        )\n    # dump it into a jsonl file\n    with open(\n        FINE_TUNE_LOGS_DIR / f\"finetune_data_{identifier_file_name}.jsonl\", \"w\"\n    ) as f:\n        for line in fine_tune_data:\n            f.write(json.dumps(line) + \"\\n\")\n\n    with open(\n        FINE_TUNE_LOGS_DIR / f\"evaluation_data_{identifier_file_name}.jsonl\", \"w\"\n    ) as f:\n        for line in evaluation_data:\n            f.write(json.dumps(line) + \"\\n\")\n\n    self.create_fine_tune_task(\n        train_filename=(\n            FINE_TUNE_LOGS_DIR / f\"finetune_data_{identifier_file_name}.jsonl\"\n        ).as_posix(),\n        eval_filename=(\n            FINE_TUNE_LOGS_DIR / f\"evaluation_data_{identifier_file_name}.jsonl\"\n        ).as_posix(),\n        identifier_file_name=identifier_file_name,\n    )\n</code></pre>"},{"location":"Code/rag/finetune_llm/#TimelineKGQA.rag.finetune_llm.FinetuneLLM.generate_finetune_data_paraphrased_questions","title":"<code>generate_finetune_data_paraphrased_questions(number_of_questions=10, identifier_file_name='paraphrased')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>number_of_questions</code> <code>int</code> <p>The number of questions</p> <code>10</code> <code>identifier_file_name</code> <code>str</code> <p>The identifier file name</p> <code>'paraphrased'</code> Source code in <code>TimelineKGQA/rag/finetune_llm.py</code> <pre><code>def generate_finetune_data_paraphrased_questions(\n    self, number_of_questions: int = 10, identifier_file_name: str = \"paraphrased\"\n):\n    \"\"\"\n    Args:\n        number_of_questions (int): The number of questions\n        identifier_file_name (str): The identifier file name\n\n    \"\"\"\n    paraphrased_finetune_query = f\"\"\"\n    SELECT * FROM unified_kg_icews_actor_questions\n    WHERE question_type = 'timeline_recovery'\n          or question_type = 'temporal_constrainted_retrieval'\n    ORDER BY events\n    LIMIT {number_of_questions} * 3;\n    \"\"\"\n\n    questions_df = pd.read_sql(paraphrased_finetune_query, self.engine)\n    fine_tune_data = []\n    evaluation_data = []\n\n    for index, row in tqdm(\n        questions_df.iterrows(),\n        total=questions_df.shape[0],\n        desc=\"Generating finetune data\",\n    ):\n        question = row[\"question\"]\n        answer = row[\"answer\"]\n\n        fine_tune_data.append(\n            {\n                \"messages\": [\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are a QA robot expert in temporal related questions.\",\n                    },\n                    {\"role\": \"user\", \"content\": question},\n                    {\"role\": \"assistant\", \"content\": answer},\n                ]\n            }\n        )\n\n        # paraphrased question\n        paraphrased_question = self.paraphrased_question(question)\n        if paraphrased_question:\n            evaluation_data.append(\n                {\n                    \"question\": question,\n                    \"paraphrased_question\": paraphrased_question,\n                    \"answer\": answer,\n                }\n            )\n\n    # dump it into a jsonl file\n    with open(\n        FINE_TUNE_LOGS_DIR / f\"finetune_data_{identifier_file_name}.jsonl\", \"w\"\n    ) as f:\n        for line in fine_tune_data:\n            f.write(json.dumps(line) + \"\\n\")\n\n    with open(\n        FINE_TUNE_LOGS_DIR / f\"evaluation_data_{identifier_file_name}.jsonl\", \"w\"\n    ) as f:\n        for line in evaluation_data:\n            f.write(json.dumps(line) + \"\\n\")\n\n    self.create_fine_tune_task(\n        train_filename=(\n            FINE_TUNE_LOGS_DIR / f\"finetune_data_{identifier_file_name}.jsonl\"\n        ).as_posix(),\n        eval_filename=(\n            FINE_TUNE_LOGS_DIR / f\"evaluation_data_{identifier_file_name}.jsonl\"\n        ).as_posix(),\n        identifier_file_name=identifier_file_name,\n    )\n</code></pre>"},{"location":"Code/rag/finetune_llm/#TimelineKGQA.rag.finetune_llm.FinetuneLLM.generate_finetune_data_simple_vs_medium","title":"<code>generate_finetune_data_simple_vs_medium(number_of_questions=30, identifier_file_name='simple_vs_medium')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>number_of_questions</code> <code>int</code> <p>The number of questions</p> <code>30</code> <code>identifier_file_name</code> <code>str</code> <p>The identifier file name</p> <code>'simple_vs_medium'</code> Source code in <code>TimelineKGQA/rag/finetune_llm.py</code> <pre><code>def generate_finetune_data_simple_vs_medium(\n    self,\n    number_of_questions: int = 30,\n    identifier_file_name: str = \"simple_vs_medium\",\n):\n    \"\"\"\n    Args:\n        number_of_questions (int): The number of questions\n        identifier_file_name (str): The identifier file name\n\n    \"\"\"\n    simple_vs_medium_query = f\"\"\"\n    SELECT * FROM unified_kg_icews_actor_questions\n    WHERE question_level = 'medium'\n    ORDER BY events\n    LIMIT {number_of_questions} * 3;\n    \"\"\"\n    questions_df = pd.read_sql(\n        simple_vs_medium_query,\n        self.engine,\n    )\n    fine_tune_data = []\n    evaluation_data = []\n    for index, row in tqdm(\n        questions_df.iterrows(),\n        total=questions_df.shape[0],\n        desc=\"Generating finetune data\",\n    ):\n        question = row[\"question\"]\n        answer = row[\"answer\"]\n\n        evaluation_data.append(\n            {\n                \"question\": question,\n                \"answer\": answer,\n            }\n        )\n\n        events = row[\"events\"]\n        for event in events:\n            logger.info(event)\n            # query the db for simple question with this event\n            simple_query = (\n                \"\"\"\n                SELECT * FROM unified_kg_icews_actor_questions\n                WHERE events = '{\"\"\"\n                + event.replace(\"'\", \"''\")\n                + \"\"\"}'\n            AND question_level = 'simple'\n            LIMIT 3;\n            \"\"\"\n            )\n            logger.info(simple_query)\n            simple_question_df = pd.read_sql(\n                simple_query,\n                self.engine,\n            )\n            logger.info(simple_question_df)\n            if simple_question_df.empty:\n                continue\n\n            for _, simple_row in simple_question_df.iterrows():\n                simple_question = simple_row[\"question\"]\n                simple_answer = simple_row[\"answer\"]\n                fine_tune_data.append(\n                    {\n                        \"messages\": [\n                            {\n                                \"role\": \"system\",\n                                \"content\": \"You are a QA robot expert in temporal related questions.\",\n                            },\n                            {\"role\": \"user\", \"content\": simple_question},\n                            {\"role\": \"assistant\", \"content\": simple_answer},\n                        ]\n                    }\n                )\n\n    # dump it into a jsonl file\n    with open(\n        FINE_TUNE_LOGS_DIR / f\"finetune_data_{identifier_file_name}.jsonl\", \"w\"\n    ) as f:\n        for line in fine_tune_data:\n            f.write(json.dumps(line) + \"\\n\")\n\n    with open(\n        FINE_TUNE_LOGS_DIR / f\"evaluation_data_{identifier_file_name}.jsonl\", \"w\"\n    ) as f:\n        for line in evaluation_data:\n            f.write(json.dumps(line) + \"\\n\")\n\n    self.create_fine_tune_task(\n        train_filename=(\n            FINE_TUNE_LOGS_DIR / f\"finetune_data_{identifier_file_name}.jsonl\"\n        ).as_posix(),\n        eval_filename=(\n            FINE_TUNE_LOGS_DIR / f\"evaluation_data_{identifier_file_name}.jsonl\"\n        ).as_posix(),\n        identifier_file_name=identifier_file_name,\n    )\n</code></pre>"},{"location":"Code/rag/finetune_llm/#TimelineKGQA.rag.finetune_llm.FinetuneLLM.paraphrased_question","title":"<code>paraphrased_question(question)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The paraphrased question</p> Source code in <code>TimelineKGQA/rag/finetune_llm.py</code> <pre><code>def paraphrased_question(self, question: str):\n    \"\"\"\n    Args:\n        question (str): The question\n\n    Returns:\n        str: The paraphrased question\n\n    \"\"\"\n\n    try:\n        prompt = f\"\"\"\n        Please paraphrase the following question with the same meaning but another way to ask:\n        {question}\n        Return it in json format with the key \"paraphrased_question\"\n        \"\"\"\n        response = client.chat.completions.create(\n            model=self.paraphrased_model,\n            messages=[\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            temperature=0.0,\n            response_format={\"type\": \"json_object\"},\n        )\n        logger.info(response.choices[0].message.content)\n        paraphrased_question_str = response.choices[0].message.content\n        paraphrased_question = json.loads(paraphrased_question_str).get(\n            \"paraphrased_question\", \"\"\n        )\n        return paraphrased_question\n    except Exception as e:\n        logger.exception(e)\n        return \"\"\n</code></pre>"},{"location":"Code/rag/gpt/","title":"Gpt","text":""},{"location":"Code/rag/gpt/#TimelineKGQA.rag.gpt.RAGRank","title":"<code>RAGRank</code>","text":"Source code in <code>TimelineKGQA/rag/gpt.py</code> <pre><code>class RAGRank:\n    def __init__(self, table_name, host, port, user, password, db_name=\"tkgqa\"):\n        self.engine = create_engine(\n            f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{db_name}\"\n        )\n        self.table_name = table_name\n        self.load_event_data()\n\n    def load_event_data(self):\n        with timer(logger, \"Load Event Data\"):\n            self.event_df = pd.read_sql(\n                f\"SELECT * FROM {self.table_name};\", self.engine\n            )\n            self._process_embeddings(\n                [\"embedding\", \"subject_embedding\", \"object_embedding\"]\n            )\n\n    def _process_embeddings(self, columns):\n        for col in columns:\n            if col in self.event_df.columns:\n                self.event_df[col] = self.event_df[col].apply(\n                    lambda x: list(map(float, x[1:-1].split(\",\")))\n                )\n\n    def add_embedding_column(self):\n        with self.engine.connect() as cursor:\n            for col in [\n                \"embedding\",\n                \"subject_embedding\",\n                \"predicate_embedding\",\n                \"object_embedding\",\n                \"start_time_embedding\",\n                \"end_time_embedding\",\n            ]:\n                if not cursor.execute(\n                    text(\n                        f\"SELECT column_name FROM information_schema.columns WHERE table_name = '{self.table_name}' AND column_name = '{col}';\"\n                    )\n                ).fetchone():\n                    cursor.execute(\n                        text(f\"ALTER TABLE {self.table_name} ADD COLUMN {col} vector;\")\n                    )\n            cursor.commit()\n\n    def embed_facts(self):\n        df = pd.read_sql(\n            f\"SELECT * FROM {self.table_name} WHERE embedding IS NULL;\", self.engine\n        )\n        if df.empty:\n            return\n        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Embedding Facts\"):\n            content = f\"{row['subject']} {row['predicate']} {row['object']} {row['start_time']} {row['end_time']}\"\n            embedding = embedding_content(content)\n            with self.engine.connect() as cursor:\n                cursor.execute(\n                    text(\n                        f\"UPDATE {self.table_name} SET embedding = array{embedding}::vector WHERE id = {row['id']};\"\n                    )\n                )\n                cursor.commit()\n\n    def embed_kg(self):\n        df = pd.read_sql(f\"SELECT * FROM {self.table_name};\", self.engine)\n        if df.empty:\n            return\n        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Embedding KG\"):\n            subject_embedding = embedding_content(row[\"subject\"])\n            object_embedding = embedding_content(row[\"object\"])\n            with self.engine.connect() as cursor:\n                cursor.execute(\n                    text(\n                        f\"UPDATE {self.table_name} SET subject_embedding = array{subject_embedding}::vector, object_embedding = array{object_embedding}::vector WHERE id = {row['id']};\"\n                    )\n                )\n                cursor.commit()\n\n    def benchmark_naive_rag(self, semantic_parse: bool = False):\n        self.event_df[\"fact\"] = (\n            self.event_df[\"subject\"]\n            + \"|\"\n            + self.event_df[\"predicate\"]\n            + \"|\"\n            + self.event_df[\"object\"]\n            + \"|\"\n            + self.event_df[\"start_time\"]\n            + \"|\"\n            + self.event_df[\"end_time\"]\n        )\n        questions_df = self._load_questions()\n        similarities = self._calculate_similarities(questions_df)\n\n        if semantic_parse:\n            mask_result_matrix = self.semantic_parse(questions_df)\n            similarities = similarities + mask_result_matrix\n\n        top_30_values, top_30_indices = torch.topk(similarities, 30, dim=1)\n\n        ranks = self._evaluate_rankings(questions_df, top_30_indices)\n        self._save_and_log_results(ranks, \"naive\", semantic_parse)\n\n    def benchmark_graph_rag(self, semantic_parse=False):\n        self.event_df[\"fact\"] = (\n            self.event_df[\"subject\"]\n            + \"|\"\n            + self.event_df[\"predicate\"]\n            + \"|\"\n            + self.event_df[\"object\"]\n            + \"|\"\n            + self.event_df[\"start_time\"]\n            + \"|\"\n            + self.event_df[\"end_time\"]\n        )\n        questions_df = self._load_questions()\n        similarities = self._calculate_similarities(questions_df)\n        if semantic_parse:\n            mask_result_matrix = self.semantic_parse(questions_df)\n            similarities = similarities + mask_result_matrix\n\n        top_30_values, top_30_indices = torch.topk(similarities, 30, dim=1)\n        ranks = self._evaluate_rankings(questions_df, top_30_indices)\n        self._save_and_log_results(ranks, \"graph\", semantic_parse)\n\n    def _load_questions(self):\n        questions_df = pd.read_sql(\n            f\"SELECT * FROM {self.table_name}_questions WHERE embedding IS NOT NULL;\",\n            self.engine,\n        )\n        questions_df[\"embedding\"] = questions_df[\"embedding\"].apply(\n            lambda x: list(map(float, x[1:-1].split(\",\")))\n        )\n        return questions_df\n\n    def _calculate_similarities(self, questions_df):\n        q_emb = np.array(questions_df[\"embedding\"].tolist(), dtype=\"float64\")\n        # s_emb = np.array(self.event_df[\"subject_embedding\"].tolist(), dtype=\"float64\")\n        # o_emb = np.array(self.event_df[\"object_embedding\"].tolist(), dtype=\"float64\")\n        e_emb = np.array(self.event_df[\"embedding\"].tolist(), dtype=\"float64\")\n        # return torch.tensor(\n        #     np.dot(q_emb, s_emb.T) + np.dot(q_emb, o_emb.T) + np.dot(q_emb, e_emb.T)\n        # )\n        return torch.tensor(np.dot(q_emb, e_emb.T))\n\n    def _evaluate_rankings(self, questions_df, top_30_indices):\n        ranks = []\n        for index, row in tqdm(\n            questions_df.iterrows(), total=len(questions_df), desc=\"Evaluating Rankings\"\n        ):\n            top_30_events = top_30_indices[index].tolist()\n            facts = self.event_df.iloc[top_30_events][\"fact\"].tolist()\n            ids = self.event_df.iloc[top_30_events][\"id\"].tolist()\n            relevant_facts = row[\"events\"]\n            rank = [1 if fact in relevant_facts else 0 for fact in facts]\n            ranks.append(\n                {\n                    \"question\": row[\"question\"],\n                    \"rank\": {\n                        \"rank\": rank,\n                        \"labels\": {\"complex\": 3, \"medium\": 2, \"simple\": 1}[\n                            row[\"question_level\"]\n                        ],\n                    },\n                    \"top_30_events\": ids,\n                }\n            )\n        return pd.DataFrame(ranks)\n\n    def _save_and_log_results(self, ranks_df, prefix, semantic_parse):\n        ranks_df.to_csv(LOGS_DIR / f\"{prefix}_ranks.csv\")\n        self.log_metrics(ranks_df, \"all\", semantic_parse)\n\n    def log_metrics(self, ranks_df, question_level, semantic_parse):\n        metrics = [\"mrr\", \"hit_1\", \"hit_3\", \"hit_5\", \"hit_10\"]\n        levels = [\"all\", \"simple\", \"medium\", \"complex\"]\n\n        for level in levels:\n            filtered_ranks = (\n                ranks_df[\"rank\"].tolist()\n                if level == \"all\"\n                else [\n                    item\n                    for item in ranks_df[\"rank\"].tolist()\n                    if item[\"labels\"] == {\"simple\": 1, \"medium\": 2, \"complex\": 3}[level]\n                ]\n            )\n\n            for metric in metrics:\n                value = (\n                    mean_reciprocal_rank(filtered_ranks)\n                    if metric == \"mrr\"\n                    else hit_n(filtered_ranks, int(metric.split(\"_\")[1]))\n                )\n                logger.info(\n                    f\"{metric.upper()}: {value}, Question Level: {level.capitalize()}, Semantic Parse: {semantic_parse}\"\n                )\n\n    def vis_question_answer_similarity(self, pk=None):\n        question_df = self._get_question_data(pk)\n        if isinstance(question_df, str):\n            return question_df, None\n        event_info = \"\\n\".join([event for event in question_df[\"events\"]])\n\n        fact_data = self._get_fact_data(question_df)\n        if isinstance(fact_data, str):\n            return fact_data, None\n        top3_facts_str, top3_value, ground_truths_rank_and_value = (\n            self._get_top_wrong_facts(question_df, fact_data)\n        )\n        # calculate the rank\n        fig = self._create_visualization(question_df, fact_data)\n\n        info_text = f\"\"\"Question: {question_df['question']}\nLevel: {question_df['question_level']}\nNumber of facts: {len(fact_data)}\nGround Truth facts: \\n{event_info}\nGround truth facts rank and similarity: {ground_truths_rank_and_value}\n-----\nTop 3 facts: \\n{top3_facts_str}\nTop 3 simlarity: {top3_value.tolist()}\n\"\"\"\n        return info_text, fig\n\n        def process_question_answer_similarity(self, pk=None):\n            question_df = self._get_question_data(pk)\n            if isinstance(question_df, str):\n                return question_df, None\n            event_info = \"\\n\".join([event for event in question_df[\"events\"]])\n\n            fact_data = self._get_fact_data(question_df)\n            if isinstance(fact_data, str):\n                return fact_data, None\n            top3_facts_str, top3_value, ground_truths_rank_and_value = (\n                self._get_top_wrong_facts(question_df, fact_data)\n            )\n            # calculate the rank\n            fig = self._create_visualization(question_df, fact_data)\n\n            info_text = f\"\"\"\n            - Question: {question_df['question']}\n            - Level: {question_df['question_level']}\n            - Number of facts: {len(fact_data)}\n            - Ground Truth facts: \\n{event_info}\n            - Ground truth facts rank and similarity: {ground_truths_rank_and_value}\n            -----\n            - Top 3 facts: \\n{top3_facts_str}\n            - Top 3 simlarity: {top3_value.tolist()}\n            \"\"\"\n            return info_text, fig.to_html()\n\n    def _get_top_wrong_facts(self, question_df: pd.DataFrame, fact_data):\n        \"\"\"\n        We want to know the rank of the correct fact in the list of facts.\n        We also want to know the top 3 facts that are wrong, if Hits@3 is 0.\n        \"\"\"\n        logger.info(question_df)\n        question_embedding_str = question_df[\"embedding\"]\n        question_embedding_array = np.fromstring(\n            question_embedding_str[1:-1], sep=\",\", dtype=\"float64\"\n        ).reshape(1, -1)\n        events_embedding_array = np.array(\n            self.event_df[\"embedding\"].tolist(), dtype=\"float64\"\n        )\n\n        similarities = torch.mm(\n            torch.tensor(question_embedding_array, dtype=torch.float32),\n            torch.tensor(events_embedding_array, dtype=torch.float32).T,\n        )\n\n        # Get the top 3 ids\n        top3_values, top3_indices = torch.topk(similarities, 3, dim=1)\n        top3_indices = top3_indices[0].tolist()\n        top3_facts_df = self.event_df.iloc[top3_indices]\n\n        top3_facts_str = \"\\n\".join(\n            [\n                f\"{row['subject']} {row['predicate']} {row['object']} {row['start_time']} {row['end_time']}\"\n                for _, row in top3_facts_df.iterrows()\n            ]\n        )\n        top3_ids = top3_facts_df[\"id\"].tolist()\n\n        # Locate the indices of ground truth facts\n        ground_truth_fact_ids = [fact[-1] for fact in fact_data]\n        ground_truth_fact_indices = self.event_df[\n            self.event_df[\"id\"].isin(ground_truth_fact_ids)\n        ].index.tolist()\n        logger.info(f\"Ground truth fact indices: {ground_truth_fact_indices}\")\n\n        ground_truths_rank_and_value = []\n        if ground_truth_fact_indices:\n            # Get the rank of the correct fact within the similarity matrix based on the index\n            for i, index in enumerate(ground_truth_fact_indices):\n                logger.info(\n                    f\"Similarity value of the correct fact {i}: {similarities[0][index].item()}\"\n                )\n                rank = (similarities[0] &gt;= similarities[0][index]).sum().item()\n                logger.info(f\"Rank of the correct fact {i}: {rank}\")\n                ground_truths_rank_and_value.append(\n                    (rank, similarities[0][index].item())\n                )\n        else:\n            logger.warning(\"No ground truth facts found in the event_df\")\n\n        logger.info(f\"Top 3 facts:\\n{top3_facts_str}\")\n\n        return top3_facts_str, top3_values, ground_truths_rank_and_value\n\n    def _get_question_data(self, pk):\n        query = f\"SELECT * FROM {self.table_name}_questions WHERE embedding IS NOT NULL\"\n        query += f\" AND id = {int(pk)}\" if pk else \" ORDER BY RANDOM() LIMIT 1\"\n        question_df = pd.read_sql(query, self.engine)\n\n        if question_df.empty:\n            return \"No question found or no questions with embeddings\"\n        return question_df.iloc[0]\n\n    def _get_fact_data(self, question_df):\n        fact_data = []\n        for fact in question_df[\"events\"]:\n            try:\n                logger.info(fact)\n                subject, predicate, object, start_time, end_time = fact.split(\"|\")\n                fact_df = self.event_df[\n                    (self.event_df[\"subject\"] == subject)\n                    &amp; (self.event_df[\"predicate\"] == predicate)\n                    &amp; (self.event_df[\"object\"] == object)\n                    &amp; (self.event_df[\"start_time\"] == start_time)\n                    &amp; (self.event_df[\"end_time\"] == end_time)\n                ]\n                if not fact_df.empty:\n                    fact_data.append(\n                        (\n                            fact,\n                            fact_df[\"embedding\"].values[0],\n                            fact_df[\"subject_embedding\"].values[0],\n                            fact_df[\"object_embedding\"].values[0],\n                            # add indice of the fact\n                            fact_df[\"id\"].values[0],\n                        )\n                    )\n            except Exception as e:\n                logger.error(f\"Error processing fact: {fact}, {e}\")\n        if not fact_data:\n            return f\"Question: {question_df['question']}\\nLevel: {question_df['question_level']}\\nNo matching facts found in the event database.\"\n        return fact_data\n\n    def _create_visualization(self, question_df, fact_data):\n        question = question_df[\"question\"]\n        question_embedding = list(map(float, question_df[\"embedding\"][1:-1].split(\",\")))\n\n        fig = plt.figure(figsize=(24, 12 * len(fact_data)))\n        gs = fig.add_gridspec(3 * len(fact_data), 1, hspace=0.4)\n\n        plt.rcParams.update(\n            {\"font.size\": 16, \"axes.titlesize\": 20, \"axes.labelsize\": 18}\n        )\n\n        for i, (fact, fact_emb, subj_emb, obj_emb, fact_id) in enumerate(fact_data):\n            # ax_text = fig.add_subplot(gs[3 * i, 0])\n            # ax_text.axis('off')\n            # ax_text.text(0, 0.5, f\"Fact {i + 1}: {fact.replace('|', ' | ')}\", fontsize=18, wrap=True)\n\n            ax_matrix = fig.add_subplot(gs[3 * i + 1 : 3 * i + 3, 0])\n            similarities = self.calculate_similarities(\n                question, question_embedding, fact_emb, subj_emb, obj_emb\n            ).T\n            sns.heatmap(\n                similarities,\n                annot=True,\n                fmt=\".2f\",\n                cmap=\"YlOrRd\",\n                cbar=False,\n                ax=ax_matrix,\n                annot_kws={\"size\": 14},\n            )\n            compoents = fact.split(\"|\")\n            subject = compoents[0]\n            object_content = compoents[2]\n            ax_matrix.set_yticklabels(\n                [\"Fact\", subject, object_content], rotation=45, va=\"center\"\n            )\n            ax_matrix.set_xticklabels(\n                [\"Question\"] + self.word_tokenize(question), rotation=45, ha=\"right\"\n            )\n            ax_matrix.set_title(\n                f\"{question} \\n Similarity Matrix for {fact}\", fontsize=16, pad=20\n            )\n\n            # Remove top and left spines\n            ax_matrix.spines[\"top\"].set_visible(False)\n            ax_matrix.spines[\"right\"].set_visible(False)\n            ax_matrix.spines[\"left\"].set_visible(False)\n\n        return fig\n\n    def calculate_similarities(\n        self,\n        question,\n        question_embedding,\n        fact_embedding,\n        subj_embedding,\n        obj_embedding,\n    ):\n        question_words = self.word_tokenize(question)\n        similarities = [\n            self._cosine_similarity(\n                question_embedding, [fact_embedding, subj_embedding, obj_embedding]\n            )\n        ]\n        for word in question_words:\n            word_embedding = self.get_word_embedding(word)\n            similarities.append(\n                self._cosine_similarity(\n                    word_embedding, [fact_embedding, subj_embedding, obj_embedding]\n                )\n            )\n        return np.array(similarities)\n\n    def _cosine_similarity(self, embedding1, embeddings2):\n        return [cosine_similarity([embedding1], [emb])[0][0] for emb in embeddings2]\n\n    # def word_tokenize(self, text):\n    #     enc = tiktoken.encoding_for_model(\"gpt-4\")\n    #     token_ids = enc.encode(text)\n    #     return [enc.decode([token_id]) for token_id in token_ids]\n\n    def word_tokenize(self, text):\n        # Split on whitespace and punctuation, keeping the punctuation as separate tokens\n        tokens = re.findall(r\"\\b\\w+\\b|[^\\w\\s]\", text)\n        return tokens\n\n    def get_word_embedding(self, word):\n        return embedding_content(word)\n\n    def semantic_parse(self, questions_df: pd.DataFrame):\n        \"\"\"\n        Filter the facts based on the entity\n        if it is candidate, mark as 1\n        if it is not candidate, mark as 0\n        Then use this to mask the similarity matrix\n        not candidate one set as -inf\n        Then do the top 30\n        Return will be a len(questions_df) x len(events_df) matrix, with 1 and 0\n        \"\"\"\n\n        def extract_entities(events: List[str]):\n            \"\"\"\n            Extract the entities from the event\n            Args:\n                events: The event string\n            Returns:\n                The entities in the event\n            \"\"\"\n            the_entities = []\n            for event in events:\n                try:\n                    elements = event.split(\"|\")\n                    the_entities.append(elements[0])\n                    the_entities.append(elements[2])\n                except Exception as e:\n                    logger.debug(e)\n\n            return the_entities\n\n        questions_df[\"entities\"] = questions_df[\"events\"].apply(\n            lambda x: extract_entities(x)\n        )\n        # get all value to be -2\n        result_matrix = np.zeros(\n            (len(questions_df), len(self.event_df)), dtype=\"float64\"\n        )\n        result_matrix = result_matrix - 2\n        for index, row in tqdm(\n            questions_df.iterrows(), total=questions_df.shape[0], desc=\"Semantic Parse\"\n        ):\n            entities = row[\"entities\"]\n            for entity in entities:\n                result_matrix[index] = np.where(\n                    self.event_df[\"subject\"] == entity, 1, result_matrix[index]\n                )\n                result_matrix[index] = np.where(\n                    self.event_df[\"object\"] == entity, 1, result_matrix[index]\n                )\n        return result_matrix\n</code></pre>"},{"location":"Code/rag/gpt/#TimelineKGQA.rag.gpt.RAGRank.semantic_parse","title":"<code>semantic_parse(questions_df)</code>","text":"<p>Filter the facts based on the entity if it is candidate, mark as 1 if it is not candidate, mark as 0 Then use this to mask the similarity matrix not candidate one set as -inf Then do the top 30 Return will be a len(questions_df) x len(events_df) matrix, with 1 and 0</p> Source code in <code>TimelineKGQA/rag/gpt.py</code> <pre><code>def semantic_parse(self, questions_df: pd.DataFrame):\n    \"\"\"\n    Filter the facts based on the entity\n    if it is candidate, mark as 1\n    if it is not candidate, mark as 0\n    Then use this to mask the similarity matrix\n    not candidate one set as -inf\n    Then do the top 30\n    Return will be a len(questions_df) x len(events_df) matrix, with 1 and 0\n    \"\"\"\n\n    def extract_entities(events: List[str]):\n        \"\"\"\n        Extract the entities from the event\n        Args:\n            events: The event string\n        Returns:\n            The entities in the event\n        \"\"\"\n        the_entities = []\n        for event in events:\n            try:\n                elements = event.split(\"|\")\n                the_entities.append(elements[0])\n                the_entities.append(elements[2])\n            except Exception as e:\n                logger.debug(e)\n\n        return the_entities\n\n    questions_df[\"entities\"] = questions_df[\"events\"].apply(\n        lambda x: extract_entities(x)\n    )\n    # get all value to be -2\n    result_matrix = np.zeros(\n        (len(questions_df), len(self.event_df)), dtype=\"float64\"\n    )\n    result_matrix = result_matrix - 2\n    for index, row in tqdm(\n        questions_df.iterrows(), total=questions_df.shape[0], desc=\"Semantic Parse\"\n    ):\n        entities = row[\"entities\"]\n        for entity in entities:\n            result_matrix[index] = np.where(\n                self.event_df[\"subject\"] == entity, 1, result_matrix[index]\n            )\n            result_matrix[index] = np.where(\n                self.event_df[\"object\"] == entity, 1, result_matrix[index]\n            )\n    return result_matrix\n</code></pre>"},{"location":"Code/rag/gpt/#TimelineKGQA.rag.gpt.launch_gradio_app","title":"<code>launch_gradio_app(rag)</code>","text":"<p>Input can be a question ID or left blank for random question. Then it should pull out the question and its associated facts. Also, the rank of the associated facts with naive similarity. Also pull out all facts better than the associated facts.</p> Source code in <code>TimelineKGQA/rag/gpt.py</code> <pre><code>def launch_gradio_app(rag):\n    \"\"\"\n    Input can be a question ID or left blank for random question.\n    Then it should pull out the question and its associated facts.\n    Also, the rank of the associated facts with naive similarity.\n    Also pull out all facts better than the associated facts.\n\n    \"\"\"\n    iface = gr.Interface(\n        fn=rag.vis_question_answer_similarity,\n        inputs=gr.Textbox(label=\"Enter Question ID (leave blank for random)\"),\n        outputs=[gr.Textbox(label=\"Question Info\"), gr.Plot()],\n        title=\"Question-Answer Similarity Visualization\",\n        description=\"Visualize the similarity between a question and its associated facts.\",\n        allow_flagging=\"never\",\n    )\n    iface.launch()\n</code></pre>"},{"location":"Code/rag/metrics/","title":"Metrics","text":""},{"location":"Code/rag/metrics/#TimelineKGQA.rag.metrics.hit_n","title":"<code>hit_n(rs, n=1)</code>","text":"<p>Calculate Hit@N. Args: rs (list of lists): List of results for each query. Each result is a list of binary values                     (1 if the item is relevant, 0 otherwise). n (int): The maximum rank to consider a hit.</p> Source code in <code>TimelineKGQA/rag/metrics.py</code> <pre><code>def hit_n(rs, n=1):\n    \"\"\"\n    Calculate Hit@N.\n    Args:\n    rs (list of lists): List of results for each query. Each result is a list of binary values\n                        (1 if the item is relevant, 0 otherwise).\n    n (int): The maximum rank to consider a hit.\n\n    \"\"\"\n\n    def hit_at_n(r):\n        \"\"\"Calculate the hit@n of a single result list.\n\n        If n = 1, then it is equivalent to precision@1.\n\n        simple, medium, complex must all hit at n\n        \"\"\"\n        rank = r[\"rank\"]\n        labels = r[\"labels\"]\n        rank = rank[: labels * n]\n        if sum(rank) == labels:\n            return 1\n        return 0\n\n    return sum(hit_at_n(r) for r in rs) / len(rs)\n</code></pre>"},{"location":"Code/rag/metrics/#TimelineKGQA.rag.metrics.mean_reciprocal_rank","title":"<code>mean_reciprocal_rank(rs)</code>","text":"<p>Calculate Mean Reciprocal Rank (MRR).</p> <p>rs (list of lists): List of results for each query. Each result is a list of binary values                     (1 if the item is relevant, 0 otherwise).</p> <p>Returns: float: Mean Reciprocal Rank (MRR) score.</p> Source code in <code>TimelineKGQA/rag/metrics.py</code> <pre><code>def mean_reciprocal_rank(rs):\n    \"\"\"\n    Calculate Mean Reciprocal Rank (MRR).\n\n    Args:\n    rs (list of lists): List of results for each query. Each result is a list of binary values\n                        (1 if the item is relevant, 0 otherwise).\n\n    Returns:\n    float: Mean Reciprocal Rank (MRR) score.\n    \"\"\"\n\n    def reciprocal_rank(r):\n        \"\"\"\n        Calculate the reciprocal rank of a single result list.\n        \"\"\"\n        rank = r[\"rank\"]\n        labels = r[\"labels\"]\n        # if all rank = 0, then rr = 0\n        if sum(rank) == 0:\n            return 0\n        rr = 0\n        for i, val in enumerate(rank):\n            if val:\n                rr += int(i / labels)\n\n        if sum(rank) &lt; labels:\n            # punishment for not all labels are in the top k\n            # we will assume then rest are all rank 31\n            rr += int(31 / labels) * (labels - sum(rank))\n        rr = 1 / (rr + 1)\n        # print(rr, r[\"rank\"], r[\"labels\"])\n        return rr\n\n    return sum(reciprocal_rank(r) for r in rs) / len(rs)\n</code></pre>"},{"location":"Code/rag/question2sql/","title":"Question2sql","text":""},{"location":"Code/rag/question2sql/#TimelineKGQA.rag.question2sql.Question2SQL","title":"<code>Question2SQL</code>","text":"<p>Get the question and the table schema and return the SQL query</p> Source code in <code>TimelineKGQA/rag/question2sql.py</code> <pre><code>class Question2SQL:\n    \"\"\"\n    Get the question and the table schema and return the SQL query\n    \"\"\"\n\n    def __init__(\n        self,\n        table_name: str,\n        host: str,\n        port: int,\n        user: str,\n        password: str,\n        db_name: str,\n        text2sql_table_name: str = None,\n    ):\n        \"\"\"\n        Args:\n            table_name: The name of the table\n            host: The host of the database\n            port: The port of the database\n            user: The user of the database\n            password: The password of the database\n            db_name: The name of the database\n\n\n\n        \"\"\"\n        self.table_name = table_name\n        self.host = host\n        self.port = port\n        self.user = user\n        self.password = password\n        self.db_name = db_name\n\n        self.engine = create_engine(\n            f\"postgresql://{user}:{password}@{host}:{port}/{db_name}\"\n        )\n\n        if text2sql_table_name is None:\n            self.text2sql_table_name = f\"{table_name}_text2sql\"\n        else:\n            self.text2sql_table_name = f\"{table_name}_{text2sql_table_name}\"\n        logger.info(f\"Text2SQL Table Name: {self.text2sql_table_name}\")\n\n        # create a table if not exists to store the text2sql questions and results\n        with self.engine.connect() as connection:\n            connection.execute(\n                text(\n                    f\"\"\"\n                    CREATE TABLE IF NOT EXISTS {self.text2sql_table_name} (\n                        id SERIAL PRIMARY KEY,\n                        question TEXT,\n                        question_level TEXT,\n                        prompt TEXT,\n                        prompt_semantic TEXT,\n                        sql_query TEXT,\n                        sql_query_semantic TEXT,\n                        correct BOOLEAN,\n                        correct_semantic BOOLEAN\n                    );\n                    \"\"\"\n                )\n            )\n            connection.commit()\n\n    def benchmark(\n        self,\n        semantic_parse: bool = False,\n        model_name: str = \"gpt-3.5-turbo\",\n    ):\n        \"\"\"\n\n        Args:\n            semantic_parse: If True, use the semantic parse to generate the prompt\n            model_name: The model name\n        \"\"\"\n        questions_df = pd.read_sql(\n            f\"SELECT * FROM {self.table_name}_questions\",\n            self.engine,\n        )\n\n        logger.info(f\"Number of questions: {len(questions_df)}\")\n\n        table_schema = self.get_table_schema()\n        logger.info(f\"Table Schema: {table_schema}\")\n\n        prompt_df = pd.read_sql(\n            f\"SELECT * FROM {self.text2sql_table_name}\",\n            self.engine,\n        )\n\n        for index, row in tqdm(questions_df.iterrows(), total=len(questions_df)):\n            question = row[\"question\"]\n            if question in prompt_df[\"question\"].values:\n                continue\n\n            events = row[\"events\"]\n            prompt_semantic = (\n                self.process_question_to_prompt_with_semantic_parse_few_shot(\n                    question, events, table_schema\n                )\n            )\n\n            prompt = self.process_question_to_prompt(question, table_schema)\n\n            query = text(\n                f\"\"\"\n                              INSERT INTO {self.text2sql_table_name} (question, question_level, prompt, prompt_semantic)\n                              VALUES (:question, :question_level, :prompt, :prompt_semantic)\n                          \"\"\"\n            )\n\n            with self.engine.connect() as connection:\n                connection.execute(\n                    query,\n                    {\n                        \"question\": question,\n                        \"question_level\": row[\"question_level\"],\n                        \"prompt\": prompt,\n                        \"prompt_semantic\": prompt_semantic,\n                    },\n                )\n\n                connection.commit()\n\n        prompt_df = pd.read_sql(\n            f\"SELECT * FROM {self.text2sql_table_name}\",\n            self.engine,\n        )\n\n        with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n            futures = []\n            for index, row in prompt_df.iterrows():\n                question = row[\"question\"]\n\n                progress_check_key = (\n                    \"sql_query\" if not semantic_parse else \"sql_query_semantic\"\n                )\n\n                if (\n                    row[progress_check_key] is not None\n                    and row[progress_check_key] != \"\"\n                ):\n                    continue\n                if semantic_parse:\n                    prompt = row[\"prompt_semantic\"]\n                else:\n                    prompt = row[\"prompt\"]\n                futures.append(\n                    executor.submit(\n                        self.text2sql_generation,\n                        prompt=prompt,\n                        question=question,\n                        model_name=model_name,\n                        semantic_parse=semantic_parse,\n                    )\n                )\n\n        self.verify_results(semantic_parse=semantic_parse)\n\n    def process_question_to_prompt(self, question: str, table_schema: str):\n        \"\"\"\n        Process the question to the prompt\n\n        Args:\n            question: The question\n            table_schema: The table schema\n\n        Returns:\n            The prompt\n        \"\"\"\n        prompt = f\"\"\"question: {question}\n        The related knowledge to answer this question is in table {table_schema},\n        the table name is {self.table_name},\n        Generate the sql query to retrieve the relevant information from the table to answer the question.\n        Return all columns for the rows that satisfy the condition.\n        Return the SQL query  in json format with the key \"sql_query\"\n        \"\"\"\n        return prompt\n\n    def process_question_to_prompt_with_semantic_parse(\n        self, question: str, events: List, table_schema: str\n    ):\n        # TODO: and question mark, should we do this? and How?\n        \"\"\"\n        Process the question to the prompt\n\n        Args:\n            question: The question\n            events: The events\n            table_schema: The table schema\n\n        Returns:\n            The prompt\n        \"\"\"\n\n        related_entities = []\n\n        for event in events:\n            items = event.split(\"|\")\n            if len(items) != 5:\n                continue\n            subject, predicate, tail_object, start_time, end_time = event.split(\"|\")\n            if subject in question:\n                related_entities.append(subject)\n            if tail_object in question:\n                related_entities.append(tail_object)\n        related_entities = \",\".join(related_entities)\n        prompt = f\"\"\"question: {question}\n        The related knowledge to answer this question is in table {table_schema},\n        the table name is {self.table_name},        \n        entities can be used as where clause: {related_entities}\n        Generate the sql query to retrieve the relevant information from the table to answer the question.\n        Return all columns for the rows that satisfy the condition.\n        Return the SQL query in json format with the key \"sql_query\"\n        \"\"\"\n        return prompt\n\n    def process_question_to_prompt_with_semantic_parse_few_shot(\n        self, question: str, events: List, table_schema: str\n    ):\n        # TODO: and question mark, should we do this? and How?\n        \"\"\"\n        Process the question to the prompt\n\n        Args:\n            question: The question\n            events: The events\n            table_schema: The table schema\n\n        Returns:\n            The prompt\n        \"\"\"\n\n        related_entities = []\n\n        for event in events:\n            items = event.split(\"|\")\n            if len(items) != 5:\n                continue\n            subject, predicate, tail_object, start_time, end_time = event.split(\"|\")\n            if subject in question:\n                related_entities.append(subject)\n            if tail_object in question:\n                related_entities.append(tail_object)\n        related_entities = \",\".join(related_entities)\n        prompt = f\"\"\"question: {question}\n        The related knowledge to answer this question is in table {table_schema},\n        the table name is {self.table_name},        \n        entities can be used as where clause: {related_entities}\n        Generate the sql query to retrieve the relevant information from the table to answer the question.\n        To achieve that, you will need to find a record where the subject and/or object matches the entities in the question.\n        For example, for question\n        From when to when, Assef Shawkat Affiliation To Military (Syria), at the same time, Lisa Ibrahim Affiliation To Brunei, at the same time, Salman Khurshid Affiliation To Elite (India)?\n        The answer is\n        SELECT * FROM unified_kg_icews_actor WHERE (subject = 'Assef Shawkat' AND object = 'Military (Syria)') OR (subject = 'Lisa Ibrahim' AND object = 'Brunei') OR (subject = 'Salman Khurshid' AND object = 'Elite (India)')\n        Return all columns for the rows that satisfy the condition.\n        Return the SQL query in json format with the key \"sql_query\"\n        \"\"\"\n        return prompt\n\n    def text2sql_generation(\n        self,\n        prompt: str,\n        question: str,\n        model_name: str = \"gpt-3.5-turbo\",\n        semantic_parse: bool = False,\n    ):\n        \"\"\"\n        This function will call text2sql_gpt and update the sql_query in the database\n        Args:\n            prompt: The prompt\n            question: The question\n            model_name: The model name\n            semantic_parse: If True, use the semantic parse to generate the prompt\n        \"\"\"\n        sql_query = self.text2sql_gpt(prompt=prompt, model_name=model_name)\n        self.update_sql_query(\n            question=question, sql_query=sql_query, semantic_parse=semantic_parse\n        )\n\n    @backoff.on_exception(\n        backoff.constant, RateLimitError, raise_on_giveup=True, interval=20\n    )\n    def text2sql_gpt(self, prompt: str, model_name: str = \"gpt-3.5-turbo\"):\n        \"\"\"\n        Get the question and the table schema and return the SQL query\n\n        Args:\n            prompt(str): The prompt\n            model_name: The model name\n        \"\"\"\n        try:\n            response = client.chat.completions.create(\n                model=model_name,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"\"\"You are an expert about text to SQL in PostgreSQL database\n                                      \"\"\",\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": prompt,\n                    },\n                ],\n                response_format={\"type\": \"json_object\"},\n                temperature=0,\n            )\n            logger.debug(f\"Response: {response.choices[0].message.content}\")\n            sql_query = response.choices[0].message.content\n            query_json = json.loads(sql_query)\n            sql_query = query_json[\"sql_query\"]\n            return sql_query\n        except Exception as e:\n            logger.error(f\"An error occurred: {e}\")\n            raise e\n\n    def update_sql_query(\n        self, question: str, sql_query: str, semantic_parse: bool = False\n    ):\n        if semantic_parse:\n            query = text(\n                f\"\"\"\n                      UPDATE {self.text2sql_table_name}\n                      SET sql_query_semantic = :sql_query\n                      WHERE question = :question\n                  \"\"\"\n            )\n        else:\n            query = text(\n                f\"\"\"\n                          UPDATE {self.text2sql_table_name}\n                          SET sql_query = :sql_query\n                          WHERE question = :question\n                      \"\"\"\n            )\n\n        with self.engine.connect() as connection:\n            connection.execute(\n                query,\n                {\n                    \"question\": question,\n                    \"sql_query\": sql_query,\n                },\n            )\n            connection.commit()\n\n    def verify_results(self, semantic_parse: bool = False):\n        if semantic_parse:\n            prompts_df = pd.read_sql(\n                f\"SELECT * FROM {self.text2sql_table_name} WHERE sql_query_semantic IS NOT NULL\",\n                self.engine,\n            )\n        else:\n            prompts_df = pd.read_sql(\n                f\"SELECT * FROM {self.text2sql_table_name} WHERE sql_query IS NOT NULL\",\n                self.engine,\n            )\n        questions_df = pd.read_sql(\n            f\"SELECT * FROM {self.table_name}_questions\",\n            self.engine,\n        )\n\n        with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n            futures = []\n            for index, row in prompts_df.iterrows():\n                progress_key = \"correct\" if not semantic_parse else \"correct_semantic\"\n                if row[progress_key] is not None:\n                    continue\n                futures.append(\n                    executor.submit(\n                        self.verify_one_result,\n                        row=row,\n                        questions_df=questions_df,\n                        semantic_parse=semantic_parse,\n                    )\n                )\n\n    def verify_one_result(self, row, questions_df, semantic_parse: bool = False):\n        connection = self.engine.connect()\n        question = row[\"question\"]\n        if semantic_parse:\n            sql_query = row[\"sql_query_semantic\"]\n        else:\n            sql_query = row[\"sql_query\"]\n        progress_key = \"correct\" if not semantic_parse else \"correct_semantic\"\n        if row[progress_key] is not None:\n            return\n        query = text(sql_query)\n        try:\n            df = pd.read_sql(query, self.engine)\n            events = []\n            for _, event in df.iterrows():\n                subject = event[\"subject\"]\n                predicate = event[\"predicate\"]\n                tail_object = event[\"object\"]\n                start_time = event[\"start_time\"]\n                end_time = event[\"end_time\"]\n\n                events.append(\n                    f\"{subject}|{predicate}|{tail_object}|{start_time}|{end_time}\"\n                )\n            # locate the ground truth for the question\n            ground_truth = questions_df[questions_df[\"question\"] == question]\n            ground_truth_events = ground_truth[\"events\"].values.tolist()\n            # decompose the nested list\n            ground_truth_events = [\n                item for sublist in ground_truth_events for item in sublist\n            ]\n            logger.info(f\"Question: {question}\")\n            logger.info(f\"SQL Query: {sql_query}\")\n            logger.info(f\"Events: {events}\")\n            logger.info(f\"Ground Truth Events: {ground_truth_events}\")\n            if set(events) == set(ground_truth_events):\n                correct = True\n            else:\n                correct = False\n        except Exception as e:\n            logger.exception(e)\n            correct = False\n\n        if semantic_parse:\n            query = text(\n                f\"\"\"\n                      UPDATE {self.text2sql_table_name}\n                      SET correct_semantic = :correct\n                      WHERE question = :question\n                  \"\"\"\n            )\n        else:\n            query = text(\n                f\"\"\"\n                              UPDATE {self.text2sql_table_name}\n                              SET correct = :correct\n                              WHERE question = :question\n                          \"\"\"\n            )\n\n        connection.execute(\n            query,\n            {\n                \"question\": question,\n                \"correct\": correct,\n            },\n        )\n        connection.commit()\n        connection.close()\n\n    def get_table_schema(self):\n        query = f\"\"\"\n        SELECT \n            column_name, \n            data_type, \n            character_maximum_length, \n            is_nullable, \n            column_default\n        FROM \n            information_schema.columns\n        WHERE \n            table_name = '{self.table_name}';\n        \"\"\"\n        df = pd.read_sql(query, self.engine)\n        return df.to_markdown(index=False)\n</code></pre>"},{"location":"Code/rag/question2sql/#TimelineKGQA.rag.question2sql.Question2SQL.__init__","title":"<code>__init__(table_name, host, port, user, password, db_name, text2sql_table_name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The name of the table</p> required <code>host</code> <code>str</code> <p>The host of the database</p> required <code>port</code> <code>int</code> <p>The port of the database</p> required <code>user</code> <code>str</code> <p>The user of the database</p> required <code>password</code> <code>str</code> <p>The password of the database</p> required <code>db_name</code> <code>str</code> <p>The name of the database</p> required Source code in <code>TimelineKGQA/rag/question2sql.py</code> <pre><code>def __init__(\n    self,\n    table_name: str,\n    host: str,\n    port: int,\n    user: str,\n    password: str,\n    db_name: str,\n    text2sql_table_name: str = None,\n):\n    \"\"\"\n    Args:\n        table_name: The name of the table\n        host: The host of the database\n        port: The port of the database\n        user: The user of the database\n        password: The password of the database\n        db_name: The name of the database\n\n\n\n    \"\"\"\n    self.table_name = table_name\n    self.host = host\n    self.port = port\n    self.user = user\n    self.password = password\n    self.db_name = db_name\n\n    self.engine = create_engine(\n        f\"postgresql://{user}:{password}@{host}:{port}/{db_name}\"\n    )\n\n    if text2sql_table_name is None:\n        self.text2sql_table_name = f\"{table_name}_text2sql\"\n    else:\n        self.text2sql_table_name = f\"{table_name}_{text2sql_table_name}\"\n    logger.info(f\"Text2SQL Table Name: {self.text2sql_table_name}\")\n\n    # create a table if not exists to store the text2sql questions and results\n    with self.engine.connect() as connection:\n        connection.execute(\n            text(\n                f\"\"\"\n                CREATE TABLE IF NOT EXISTS {self.text2sql_table_name} (\n                    id SERIAL PRIMARY KEY,\n                    question TEXT,\n                    question_level TEXT,\n                    prompt TEXT,\n                    prompt_semantic TEXT,\n                    sql_query TEXT,\n                    sql_query_semantic TEXT,\n                    correct BOOLEAN,\n                    correct_semantic BOOLEAN\n                );\n                \"\"\"\n            )\n        )\n        connection.commit()\n</code></pre>"},{"location":"Code/rag/question2sql/#TimelineKGQA.rag.question2sql.Question2SQL.benchmark","title":"<code>benchmark(semantic_parse=False, model_name='gpt-3.5-turbo')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>semantic_parse</code> <code>bool</code> <p>If True, use the semantic parse to generate the prompt</p> <code>False</code> <code>model_name</code> <code>str</code> <p>The model name</p> <code>'gpt-3.5-turbo'</code> Source code in <code>TimelineKGQA/rag/question2sql.py</code> <pre><code>def benchmark(\n    self,\n    semantic_parse: bool = False,\n    model_name: str = \"gpt-3.5-turbo\",\n):\n    \"\"\"\n\n    Args:\n        semantic_parse: If True, use the semantic parse to generate the prompt\n        model_name: The model name\n    \"\"\"\n    questions_df = pd.read_sql(\n        f\"SELECT * FROM {self.table_name}_questions\",\n        self.engine,\n    )\n\n    logger.info(f\"Number of questions: {len(questions_df)}\")\n\n    table_schema = self.get_table_schema()\n    logger.info(f\"Table Schema: {table_schema}\")\n\n    prompt_df = pd.read_sql(\n        f\"SELECT * FROM {self.text2sql_table_name}\",\n        self.engine,\n    )\n\n    for index, row in tqdm(questions_df.iterrows(), total=len(questions_df)):\n        question = row[\"question\"]\n        if question in prompt_df[\"question\"].values:\n            continue\n\n        events = row[\"events\"]\n        prompt_semantic = (\n            self.process_question_to_prompt_with_semantic_parse_few_shot(\n                question, events, table_schema\n            )\n        )\n\n        prompt = self.process_question_to_prompt(question, table_schema)\n\n        query = text(\n            f\"\"\"\n                          INSERT INTO {self.text2sql_table_name} (question, question_level, prompt, prompt_semantic)\n                          VALUES (:question, :question_level, :prompt, :prompt_semantic)\n                      \"\"\"\n        )\n\n        with self.engine.connect() as connection:\n            connection.execute(\n                query,\n                {\n                    \"question\": question,\n                    \"question_level\": row[\"question_level\"],\n                    \"prompt\": prompt,\n                    \"prompt_semantic\": prompt_semantic,\n                },\n            )\n\n            connection.commit()\n\n    prompt_df = pd.read_sql(\n        f\"SELECT * FROM {self.text2sql_table_name}\",\n        self.engine,\n    )\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = []\n        for index, row in prompt_df.iterrows():\n            question = row[\"question\"]\n\n            progress_check_key = (\n                \"sql_query\" if not semantic_parse else \"sql_query_semantic\"\n            )\n\n            if (\n                row[progress_check_key] is not None\n                and row[progress_check_key] != \"\"\n            ):\n                continue\n            if semantic_parse:\n                prompt = row[\"prompt_semantic\"]\n            else:\n                prompt = row[\"prompt\"]\n            futures.append(\n                executor.submit(\n                    self.text2sql_generation,\n                    prompt=prompt,\n                    question=question,\n                    model_name=model_name,\n                    semantic_parse=semantic_parse,\n                )\n            )\n\n    self.verify_results(semantic_parse=semantic_parse)\n</code></pre>"},{"location":"Code/rag/question2sql/#TimelineKGQA.rag.question2sql.Question2SQL.process_question_to_prompt","title":"<code>process_question_to_prompt(question, table_schema)</code>","text":"<p>Process the question to the prompt</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question</p> required <code>table_schema</code> <code>str</code> <p>The table schema</p> required <p>Returns:</p> Type Description <p>The prompt</p> Source code in <code>TimelineKGQA/rag/question2sql.py</code> <pre><code>def process_question_to_prompt(self, question: str, table_schema: str):\n    \"\"\"\n    Process the question to the prompt\n\n    Args:\n        question: The question\n        table_schema: The table schema\n\n    Returns:\n        The prompt\n    \"\"\"\n    prompt = f\"\"\"question: {question}\n    The related knowledge to answer this question is in table {table_schema},\n    the table name is {self.table_name},\n    Generate the sql query to retrieve the relevant information from the table to answer the question.\n    Return all columns for the rows that satisfy the condition.\n    Return the SQL query  in json format with the key \"sql_query\"\n    \"\"\"\n    return prompt\n</code></pre>"},{"location":"Code/rag/question2sql/#TimelineKGQA.rag.question2sql.Question2SQL.process_question_to_prompt_with_semantic_parse","title":"<code>process_question_to_prompt_with_semantic_parse(question, events, table_schema)</code>","text":"<p>Process the question to the prompt</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question</p> required <code>events</code> <code>List</code> <p>The events</p> required <code>table_schema</code> <code>str</code> <p>The table schema</p> required <p>Returns:</p> Type Description <p>The prompt</p> Source code in <code>TimelineKGQA/rag/question2sql.py</code> <pre><code>def process_question_to_prompt_with_semantic_parse(\n    self, question: str, events: List, table_schema: str\n):\n    # TODO: and question mark, should we do this? and How?\n    \"\"\"\n    Process the question to the prompt\n\n    Args:\n        question: The question\n        events: The events\n        table_schema: The table schema\n\n    Returns:\n        The prompt\n    \"\"\"\n\n    related_entities = []\n\n    for event in events:\n        items = event.split(\"|\")\n        if len(items) != 5:\n            continue\n        subject, predicate, tail_object, start_time, end_time = event.split(\"|\")\n        if subject in question:\n            related_entities.append(subject)\n        if tail_object in question:\n            related_entities.append(tail_object)\n    related_entities = \",\".join(related_entities)\n    prompt = f\"\"\"question: {question}\n    The related knowledge to answer this question is in table {table_schema},\n    the table name is {self.table_name},        \n    entities can be used as where clause: {related_entities}\n    Generate the sql query to retrieve the relevant information from the table to answer the question.\n    Return all columns for the rows that satisfy the condition.\n    Return the SQL query in json format with the key \"sql_query\"\n    \"\"\"\n    return prompt\n</code></pre>"},{"location":"Code/rag/question2sql/#TimelineKGQA.rag.question2sql.Question2SQL.process_question_to_prompt_with_semantic_parse_few_shot","title":"<code>process_question_to_prompt_with_semantic_parse_few_shot(question, events, table_schema)</code>","text":"<p>Process the question to the prompt</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question</p> required <code>events</code> <code>List</code> <p>The events</p> required <code>table_schema</code> <code>str</code> <p>The table schema</p> required <p>Returns:</p> Type Description <p>The prompt</p> Source code in <code>TimelineKGQA/rag/question2sql.py</code> <pre><code>def process_question_to_prompt_with_semantic_parse_few_shot(\n    self, question: str, events: List, table_schema: str\n):\n    # TODO: and question mark, should we do this? and How?\n    \"\"\"\n    Process the question to the prompt\n\n    Args:\n        question: The question\n        events: The events\n        table_schema: The table schema\n\n    Returns:\n        The prompt\n    \"\"\"\n\n    related_entities = []\n\n    for event in events:\n        items = event.split(\"|\")\n        if len(items) != 5:\n            continue\n        subject, predicate, tail_object, start_time, end_time = event.split(\"|\")\n        if subject in question:\n            related_entities.append(subject)\n        if tail_object in question:\n            related_entities.append(tail_object)\n    related_entities = \",\".join(related_entities)\n    prompt = f\"\"\"question: {question}\n    The related knowledge to answer this question is in table {table_schema},\n    the table name is {self.table_name},        \n    entities can be used as where clause: {related_entities}\n    Generate the sql query to retrieve the relevant information from the table to answer the question.\n    To achieve that, you will need to find a record where the subject and/or object matches the entities in the question.\n    For example, for question\n    From when to when, Assef Shawkat Affiliation To Military (Syria), at the same time, Lisa Ibrahim Affiliation To Brunei, at the same time, Salman Khurshid Affiliation To Elite (India)?\n    The answer is\n    SELECT * FROM unified_kg_icews_actor WHERE (subject = 'Assef Shawkat' AND object = 'Military (Syria)') OR (subject = 'Lisa Ibrahim' AND object = 'Brunei') OR (subject = 'Salman Khurshid' AND object = 'Elite (India)')\n    Return all columns for the rows that satisfy the condition.\n    Return the SQL query in json format with the key \"sql_query\"\n    \"\"\"\n    return prompt\n</code></pre>"},{"location":"Code/rag/question2sql/#TimelineKGQA.rag.question2sql.Question2SQL.text2sql_generation","title":"<code>text2sql_generation(prompt, question, model_name='gpt-3.5-turbo', semantic_parse=False)</code>","text":"<p>This function will call text2sql_gpt and update the sql_query in the database Args:     prompt: The prompt     question: The question     model_name: The model name     semantic_parse: If True, use the semantic parse to generate the prompt</p> Source code in <code>TimelineKGQA/rag/question2sql.py</code> <pre><code>def text2sql_generation(\n    self,\n    prompt: str,\n    question: str,\n    model_name: str = \"gpt-3.5-turbo\",\n    semantic_parse: bool = False,\n):\n    \"\"\"\n    This function will call text2sql_gpt and update the sql_query in the database\n    Args:\n        prompt: The prompt\n        question: The question\n        model_name: The model name\n        semantic_parse: If True, use the semantic parse to generate the prompt\n    \"\"\"\n    sql_query = self.text2sql_gpt(prompt=prompt, model_name=model_name)\n    self.update_sql_query(\n        question=question, sql_query=sql_query, semantic_parse=semantic_parse\n    )\n</code></pre>"},{"location":"Code/rag/question2sql/#TimelineKGQA.rag.question2sql.Question2SQL.text2sql_gpt","title":"<code>text2sql_gpt(prompt, model_name='gpt-3.5-turbo')</code>","text":"<p>Get the question and the table schema and return the SQL query</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt</p> required <code>model_name</code> <code>str</code> <p>The model name</p> <code>'gpt-3.5-turbo'</code> Source code in <code>TimelineKGQA/rag/question2sql.py</code> <pre><code>@backoff.on_exception(\n    backoff.constant, RateLimitError, raise_on_giveup=True, interval=20\n)\ndef text2sql_gpt(self, prompt: str, model_name: str = \"gpt-3.5-turbo\"):\n    \"\"\"\n    Get the question and the table schema and return the SQL query\n\n    Args:\n        prompt(str): The prompt\n        model_name: The model name\n    \"\"\"\n    try:\n        response = client.chat.completions.create(\n            model=model_name,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"You are an expert about text to SQL in PostgreSQL database\n                                  \"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": prompt,\n                },\n            ],\n            response_format={\"type\": \"json_object\"},\n            temperature=0,\n        )\n        logger.debug(f\"Response: {response.choices[0].message.content}\")\n        sql_query = response.choices[0].message.content\n        query_json = json.loads(sql_query)\n        sql_query = query_json[\"sql_query\"]\n        return sql_query\n    except Exception as e:\n        logger.error(f\"An error occurred: {e}\")\n        raise e\n</code></pre>"},{"location":"Code/reasoning/","title":"Index","text":""},{"location":"Code/reasoning/openai_reasoning/","title":"Openai reasoning","text":""},{"location":"Code/reasoning/openai_reasoning/#TimelineKGQA.reasoning.openai_reasoning.reasoning_temporal_questions","title":"<code>reasoning_temporal_questions(prompt)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The question to reason.</p> required Source code in <code>TimelineKGQA/reasoning/openai_reasoning.py</code> <pre><code>def reasoning_temporal_questions(prompt: str):\n    \"\"\"\n\n    Args:\n        prompt (str): The question to reason.\n    \"\"\"\n    try:\n        logger.info(f\"Reasoning the question: {prompt}\")\n        res = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"You are an expert on reasoning temporal questions.\n                                  Give us the answer to the following question.\n                                  \"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": prompt,\n                },\n            ],\n        )\n        return res.choices[0].message.content\n    except Exception as e:\n        logger.error(e)\n        return \"\"\n</code></pre>"},{"location":"Code/reasoning/reasoning/","title":"Reasoning","text":""},{"location":"Code/reasoning/reasoning/#TimelineKGQA.reasoning.reasoning.TemporalReasoningEvaluator","title":"<code>TemporalReasoningEvaluator</code>","text":"Source code in <code>TimelineKGQA/reasoning/reasoning.py</code> <pre><code>class TemporalReasoningEvaluator:\n    def __init__(\n        self,\n        table_name: str,\n        host: str,\n        port: int,\n        user: str,\n        password: str,\n        db_name: str = \"tkgqa\",\n    ):\n        \"\"\"\n\n        Args:\n            table_name (str): The table name to be evaluated.\n            host (str): The host of the database.\n            port (int): The port of the database.\n            user (str): The user of the database.\n            password (str): The password of the database.\n            db_name (str): The database name. Default is \"tkgqa\".\n        \"\"\"\n\n        self.host = host\n        self.port = port\n        self.user = user\n        self.password = password\n        self.db_name = db_name\n        self.engine = create_engine(\n            f\"postgresql://{user}:{password}@{host}:{port}/{db_name}\"\n        )\n        self.table_name = table_name\n        with timer(logger, \"Loading the table\"):\n            self.qa_df = pd.read_sql_table(table_name, self.engine)\n            logger.info(\n                f\"Loaded {len(self.qa_df)} records from the table {table_name}.\"\n            )\n\n        self.max_workers = cpu_count\n\n    def evaluate(self):\n        \"\"\"\n        Evaluate the temporal reasoning. via the LLM\n        Given the question, the LLM will generate the answer.\n        \"\"\"\n        random_index = random.randint(0, len(self.qa_df))\n\n        for index, row in tqdm.tqdm(self.qa_df.iterrows(), total=len(self.qa_df)):\n            if index != random_index:\n                continue\n            question = row[\"question\"]\n            events = row[\"events\"]\n            logger.info(f\"Question: {question}\")\n            logger.info(f\"Events: {events}\")\n            logger.info(f\"Answer: {row['answer']}\")\n            reasoning_prompt = (\n                f\"Given the events: {events}, reason the following question: {question}\"\n            )\n\n            res = reasoning_temporal_questions(reasoning_prompt)\n\n            logger.info(f\"Reasoning result: {res}\")\n            break\n</code></pre>"},{"location":"Code/reasoning/reasoning/#TimelineKGQA.reasoning.reasoning.TemporalReasoningEvaluator.__init__","title":"<code>__init__(table_name, host, port, user, password, db_name='tkgqa')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>The table name to be evaluated.</p> required <code>host</code> <code>str</code> <p>The host of the database.</p> required <code>port</code> <code>int</code> <p>The port of the database.</p> required <code>user</code> <code>str</code> <p>The user of the database.</p> required <code>password</code> <code>str</code> <p>The password of the database.</p> required <code>db_name</code> <code>str</code> <p>The database name. Default is \"tkgqa\".</p> <code>'tkgqa'</code> Source code in <code>TimelineKGQA/reasoning/reasoning.py</code> <pre><code>def __init__(\n    self,\n    table_name: str,\n    host: str,\n    port: int,\n    user: str,\n    password: str,\n    db_name: str = \"tkgqa\",\n):\n    \"\"\"\n\n    Args:\n        table_name (str): The table name to be evaluated.\n        host (str): The host of the database.\n        port (int): The port of the database.\n        user (str): The user of the database.\n        password (str): The password of the database.\n        db_name (str): The database name. Default is \"tkgqa\".\n    \"\"\"\n\n    self.host = host\n    self.port = port\n    self.user = user\n    self.password = password\n    self.db_name = db_name\n    self.engine = create_engine(\n        f\"postgresql://{user}:{password}@{host}:{port}/{db_name}\"\n    )\n    self.table_name = table_name\n    with timer(logger, \"Loading the table\"):\n        self.qa_df = pd.read_sql_table(table_name, self.engine)\n        logger.info(\n            f\"Loaded {len(self.qa_df)} records from the table {table_name}.\"\n        )\n\n    self.max_workers = cpu_count\n</code></pre>"},{"location":"Code/reasoning/reasoning/#TimelineKGQA.reasoning.reasoning.TemporalReasoningEvaluator.evaluate","title":"<code>evaluate()</code>","text":"<p>Evaluate the temporal reasoning. via the LLM Given the question, the LLM will generate the answer.</p> Source code in <code>TimelineKGQA/reasoning/reasoning.py</code> <pre><code>def evaluate(self):\n    \"\"\"\n    Evaluate the temporal reasoning. via the LLM\n    Given the question, the LLM will generate the answer.\n    \"\"\"\n    random_index = random.randint(0, len(self.qa_df))\n\n    for index, row in tqdm.tqdm(self.qa_df.iterrows(), total=len(self.qa_df)):\n        if index != random_index:\n            continue\n        question = row[\"question\"]\n        events = row[\"events\"]\n        logger.info(f\"Question: {question}\")\n        logger.info(f\"Events: {events}\")\n        logger.info(f\"Answer: {row['answer']}\")\n        reasoning_prompt = (\n            f\"Given the events: {events}, reason the following question: {question}\"\n        )\n\n        res = reasoning_temporal_questions(reasoning_prompt)\n\n        logger.info(f\"Reasoning result: {res}\")\n        break\n</code></pre>"},{"location":"Code/simulation/","title":"Index","text":""},{"location":"Code/simulation/allen_temporal_simulator/","title":"Allen temporal simulator","text":""},{"location":"Code/simulation/allen_temporal_simulator/#TimelineKGQA.simulation.allen_temporal_simulator.AllenTemporalSimulator","title":"<code>AllenTemporalSimulator</code>","text":"Source code in <code>TimelineKGQA/simulation/allen_temporal_simulator.py</code> <pre><code>class AllenTemporalSimulator:\n    def __init__(self, n, m, max_time=10, mode=\"timeranges\"):\n        self.n = n  # number of random points to generate\n        self.m = m  # number of points to generate around each random point\n        self.max_time = max_time  # maximum time value for all axes\n        self.mode = mode  # mode to generate points, either \"timeranges\" or \"timepoint\"\n        self.color_shape_map = {\n            \"before\": (\"#FF0000\", \"circle\"),  # bright red, circle\n            \"after\": (\"#FF4500\", \"square\"),  # orange-red, square\n            \"meets\": (\"#1E90FF\", \"circle-open\"),  # dodger blue, circle-open\n            \"met_by\": (\"#4169E1\", \"square-open\"),  # royal blue, square-open\n            \"overlaps\": (\"#32CD32\", \"cross\"),  # lime green, cross\n            \"overlapped_by\": (\"#228B22\", \"x\"),  # forest green, x\n            \"starts\": (\"#800080\", \"diamond\"),  # purple, diamond\n            \"started_by\": (\"#9932CC\", \"diamond-open\"),  # dark orchid, diamond-open\n            \"during\": (\"#FFA500\", \"circle\"),  # orange, circle\n            \"contains\": (\"#FF8C00\", \"square\"),  # dark orange, square\n            \"finishes\": (\"#FFFF00\", \"circle-open\"),  # yellow, circle-open\n            \"finished_by\": (\"#FFD700\", \"square-open\"),  # gold, square-open\n            \"equals\": (\"#00FFFF\", \"diamond\"),  # cyan, diamond\n        }\n        self.output_dir = DATA_DIR / \"simulations\"\n        self.output_dir.mkdir(exist_ok=True, parents=True)\n\n    def generate_surface(self):\n        \"\"\"\n        This is the 3D surface that represents all information temporal information\n        \"\"\"\n        x = np.linspace(0, self.max_time, 100)\n        y = np.linspace(0, self.max_time, 100)\n        X, Y = np.meshgrid(x, y)\n        Z = np.maximum(Y - X, 0)  # Z starts from 0 at the origin and increases\n        # remove all points where X &gt; Y\n        Z[X &gt; Y] = np.nan\n        return X, Y, Z\n\n    def generate_nearby_time_ranges(self, point):\n        start, end = point\n        nearby_points = []\n        for _ in range(self.m):\n            # random select a relationship to generate a nearby point\n            temporal_relation = np.random.choice(list(self.color_shape_map.keys()))\n            if temporal_relation == \"meets\":\n                new_start = end\n                new_end = np.random.uniform(new_start + 0.1, self.max_time)\n            elif temporal_relation == \"met_by\":\n                new_end = start\n                new_start = np.random.uniform(0, new_end - 0.1)\n            elif temporal_relation == \"starts\":\n                new_start = start\n                new_end = np.random.uniform(start + 0.1, self.max_time)\n            elif temporal_relation == \"started_by\":\n                new_end = end\n                new_start = np.random.uniform(0, new_end - 0.1)\n            elif temporal_relation == \"finishes\":\n                new_end = end\n                new_start = np.random.uniform(0, new_end - 0.1)\n            elif temporal_relation == \"finished_by\":\n                new_start = start\n                new_end = np.random.uniform(start + 0.1, self.max_time)\n            elif temporal_relation == \"equals\":\n                new_start = start\n                new_end = end\n            else:\n                new_start = np.random.uniform(\n                    max(0, start - 100), min(self.max_time, start + 100)\n                )\n                new_end = np.random.uniform(\n                    max(new_start + 0.1, end - 100), min(self.max_time, end + 100)\n                )\n            nearby_points.append((new_start, new_end))\n        return nearby_points\n\n    def generate_nearby_timepoints(self, point):\n        start, end = point\n        nearby_points = []\n        for _ in range(self.m):\n            # random select a relationship to generate a nearby point\n            temporal_relation = np.random.choice([\"before\", \"equals\", \"after\"])\n            if temporal_relation == \"before\":\n                new_start = np.random.uniform(0, start)\n                new_end = new_start\n            elif temporal_relation == \"after\":\n                new_start = np.random.uniform(end, self.max_time)\n                new_end = new_start\n            elif temporal_relation == \"equals\":\n                new_start = start\n                new_end = end\n            nearby_points.append((new_start, new_end))\n        return nearby_points\n\n    def generate_nearby_timepoints_for_timerange(self, time_range):\n        start, end = time_range\n        nearby_points = []\n        for _ in range(self.m):\n            # random select a relationship to generate a nearby point\n            temporal_relation = np.random.choice(\n                [\"before\", \"finishes\", \"contains\", \"started_by\", \"after\"]\n            )\n            if temporal_relation == \"before\":\n                new_start = np.random.uniform(0, start)\n                new_end = new_start\n            elif temporal_relation == \"finishes\":\n                new_start = start\n                new_end = new_start\n            elif temporal_relation == \"contains\":\n                new_start = np.random.uniform(start, end)\n                new_end = new_start\n            elif temporal_relation == \"started_by\":\n                new_start = end\n                new_end = new_start\n            elif temporal_relation == \"after\":\n                new_start = np.random.uniform(end, self.max_time)\n                new_end = new_start\n            else:\n                return\n            nearby_points.append((new_start, new_end))\n        return nearby_points\n\n    def determine_relation(self, interval1, interval2):\n        \"\"\"\n        Determine the relationship between two temporal intervals\n\n        \"\"\"\n        start1, end1 = interval1\n        start2, end2 = interval2\n\n        if end1 &lt; start2:\n            return \"before\"  # interval1 is before interval2\n        elif start1 &gt; end2:\n            return \"after\"  # interval1 is after interval2\n        elif end1 == start2:  # interval1 meets interval2\n            return \"meets\"\n        elif start1 == end2:  # interval1 is met by interval2\n            return \"met_by\"\n        elif start1 &lt; start2 &lt; end1 &lt; end2:  # interval1 overlaps interval2\n            return \"overlaps\"\n        elif start2 &lt; start1 &lt; end2 &lt; end1:  # interval1 is overlapped by interval2\n            return \"overlapped_by\"\n        elif start1 == start2 and end1 &lt; end2:  # interval1 starts interval2\n            return \"starts\"\n        elif start1 == start2 and end1 &gt; end2:  # interval1 is started by interval2\n            return \"started_by\"\n        elif start2 &lt; start1 &lt; end1 &lt; end2:  # interval1 is during interval2\n            return \"during\"\n        elif start1 &lt; start2 &lt; end2 &lt; end1:  # interval1 contains interval2\n            return \"contains\"\n        elif start1 &lt; start2 &lt; end2 == end1:  # interval1 finishes interval2\n            return \"finishes\"\n        elif start2 &lt; start1 &lt; end1 == end2:  # interval1 is finished by interval2\n            return \"finished_by\"\n        elif start1 == start2 and end1 == end2:  # interval1 equals interval2\n            return \"equals\"\n\n    def determine_relation_timepoint(self, point1, point2):\n        start1, end1 = point1\n        start2, end2 = point2\n        if end1 &lt; start2:\n            return \"before\"\n        elif start1 &gt; end2:\n            return \"after\"\n        elif start1 == start2 and end1 == end2:\n            return \"equals\"\n\n    def determine_relation_timepointrange(self, point_1, time_range_2):\n        \"\"\"\n        Point 1 will be a time point, and point 2 will be a time range\n        So there will be 5 relationships: before, starts, during, finishes, after\n\n        \"\"\"\n        start1, end1 = point_1  # point1 is a time point, so start1 = end1\n        start2, end2 = time_range_2\n\n        if start1 &lt; start2:\n            return \"before\"  # point1 is before range2\n        elif start1 == start2:\n            return \"starts\"  # point1 starts range2\n        elif start1 &gt; start2 and start1 &lt; end2:\n            return \"during\"\n        elif start1 == end2:\n            return \"finishes\"\n        elif start1 &gt; start2:\n            return \"after\"\n\n    def determine_relation_timerangepoint(self, point1, point2):\n        \"\"\"\n        Point 1 will be a time range, and point 2 will be a time point\n        So there will be 5 relationships: before, meets, contain, finished_by, after\n        \"\"\"\n\n        start1, end1 = point1\n        start2, end2 = point2  # point2 is a time point, so end2 = start2\n\n        if end1 &lt; start2:\n            return \"before\"  # range1 is before point2\n        elif end1 == start2:\n            return \"finished_by\"  # range 1 meets point2\n        elif start1 &lt; start2 and end1 &gt; start2:\n            return \"contains\"  # range 1 contains point2\n        elif start1 == start2:\n            return \"met_by\"  # range 1 finished_by point2\n        elif start1 &gt; start2:\n            return \"after\"  # range 1 is after point2\n\n    def visualize(self, center_point=None):\n        \"\"\"\n        First generate the 3D surface for the visualization\n        \"\"\"\n        X, Y, Z = self.generate_surface()\n\n        fig = go.Figure()\n\n        if self.mode == \"surface\":\n            # show the surface track\n            fig.add_trace(\n                go.Surface(\n                    x=X,\n                    y=Y,\n                    z=Z,\n                    # give it the grey color\n                    colorscale=[[0, \"rgb(220,220,220)\"], [1, \"rgb(220,220,220)\"]],\n                    showscale=False,\n                    opacity=0.5,\n                    showlegend=False,\n                )\n            )\n\n        traces = {\n            relation: go.Scatter3d(\n                x=[],\n                y=[],\n                z=[],\n                mode=\"markers\",\n                marker=dict(size=5, color=color, symbol=shape),\n                name=relation,\n            )\n            for relation, (color, shape) in self.color_shape_map.items()\n        }\n\n        \"\"\"\n        Here is for relationship between temporal intervals\n        \"\"\"\n        if self.mode == \"timeranges\":\n            # define a center point\n            if not center_point:\n                center_point = (25, 75)\n            start, end = center_point\n            fig.add_trace(\n                go.Scatter3d(\n                    x=[start],\n                    y=[end],\n                    z=[end - start],\n                    mode=\"markers\",\n                    marker=dict(size=20, color=\"black\", symbol=\"cross\"),\n                    showlegend=False,\n                )\n            )\n\n            nearby_points = self.generate_nearby_time_ranges(center_point)\n            for nearby_point in nearby_points:\n                n_start, n_end = nearby_point\n                relation = self.determine_relation(center_point, nearby_point)\n                traces[relation].x = traces[relation].x + (n_start,)\n                traces[relation].y = traces[relation].y + (n_end,)\n                traces[relation].z = traces[relation].z + (n_end - n_start,)\n\n            # Add all traces to the figure\n            for trace in traces.values():\n                fig.add_trace(trace)\n\n        \"\"\"\n        Add a time point to time point relationship\n\n        Which means the z is 0, x = y, and only three relationships: before, equals, after\n        \"\"\"\n\n        if self.mode == \"timepoints\":\n\n            # define a center point for timepoint\n            if not center_point:\n                center_point = (50, 50)\n            else:\n                assert (\n                    len(center_point) == 2\n                ), \"Center point must be a tuple of 2 values\"\n                assert (\n                    center_point[0] == center_point[1]\n                ), \"Center point must be a time point\"\n            start, end = center_point\n            fig.add_trace(\n                go.Scatter3d(\n                    x=[start],\n                    y=[end],\n                    z=[0],\n                    mode=\"markers\",\n                    marker=dict(size=20, color=\"black\", symbol=\"cross\"),\n                    showlegend=False,\n                )\n            )\n\n            nearby_points = self.generate_nearby_timepoints(center_point)\n\n            for nearby_point in nearby_points:\n                n_start, n_end = nearby_point\n                relation = self.determine_relation_timepoint(center_point, nearby_point)\n                traces[relation].x = traces[relation].x + (n_start,)\n                traces[relation].y = traces[relation].y + (n_end,)\n                traces[relation].z = traces[relation].z + (0,)\n            # Add all traces to the figure\n            for trace in traces.values():\n                fig.add_trace(trace)\n\n        if self.mode == \"timepointrange\":\n            if not center_point:\n                center_point = (50, 50)\n            else:\n                assert (\n                    len(center_point) == 2\n                ), \"Center point must be a tuple of 2 values\"\n                assert (\n                    center_point[0] != center_point[1]\n                ), \"Center point must be a time range\"\n            start, end = center_point\n            fig.add_trace(\n                go.Scatter3d(\n                    x=[start],\n                    y=[end],\n                    z=[0],\n                    mode=\"markers\",\n                    marker=dict(size=20, color=\"black\", symbol=\"cross\"),\n                    showlegend=False,\n                )\n            )\n\n            nearby_points = self.generate_nearby_time_ranges(center_point)\n            for nearby_point in nearby_points:\n                n_start, n_end = nearby_point\n                relation = self.determine_relation_timepointrange(\n                    center_point, nearby_point\n                )\n                if relation is None:\n                    continue\n                traces[relation].x = traces[relation].x + (n_start,)\n                traces[relation].y = traces[relation].y + (n_end,)\n                traces[relation].z = traces[relation].z + (n_end - n_start,)\n            # Add all traces to the figure\n            for trace in traces.values():\n                fig.add_trace(trace)\n\n        if self.mode == \"timerangepoint\":\n            if not center_point:\n                center_point = (25, 75)\n            else:\n                assert (\n                    len(center_point) == 2\n                ), \"Center point must be a tuple of 2 values\"\n                assert (\n                    center_point[0] != center_point[1]\n                ), \"Center point must be a time range\"\n            start, end = center_point\n            fig.add_trace(\n                go.Scatter3d(\n                    x=[start],\n                    y=[end],\n                    z=[end - start],\n                    mode=\"markers\",\n                    marker=dict(size=20, color=\"black\", symbol=\"cross\"),\n                    showlegend=False,\n                )\n            )\n\n            nearby_points = self.generate_nearby_timepoints_for_timerange(center_point)\n            for nearby_point in nearby_points:\n                n_start, n_end = nearby_point\n                relation = self.determine_relation_timerangepoint(\n                    center_point, nearby_point\n                )\n                if relation is None:\n                    continue\n                traces[relation].x = traces[relation].x + (n_start,)\n                traces[relation].y = traces[relation].y + (n_end,)\n                traces[relation].z = traces[relation].z + (0,)\n            # Add all traces to the figure\n            for trace in traces.values():\n                fig.add_trace(trace)\n\n        # Update layout\n        fig.update_layout(\n            scene=dict(\n                xaxis_title=\"Start Time\",\n                yaxis_title=\"End Time\",\n                zaxis_title=\"Duration\",\n                xaxis=dict(range=[self.max_time, 0], autorange=\"reversed\"),\n                yaxis=dict(range=[self.max_time, 0], autorange=\"reversed\"),\n                zaxis=dict(range=[0, self.max_time * 2]),\n                aspectmode=\"cube\",\n                aspectratio=dict(x=1, y=1, z=1),\n            ),\n            title=f\"Allen's 13 Temporal Relations Visualization for {self.mode}\",\n            legend_title=\"Temporal Relations\",\n            legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01),\n        )\n\n        # Show plot\n        fig.show()\n        # to html\n        fig.write_html(self.output_dir / f\"{self.mode}_allen_temporal_relations.html\")\n</code></pre>"},{"location":"Code/simulation/allen_temporal_simulator/#TimelineKGQA.simulation.allen_temporal_simulator.AllenTemporalSimulator.determine_relation","title":"<code>determine_relation(interval1, interval2)</code>","text":"<p>Determine the relationship between two temporal intervals</p> Source code in <code>TimelineKGQA/simulation/allen_temporal_simulator.py</code> <pre><code>def determine_relation(self, interval1, interval2):\n    \"\"\"\n    Determine the relationship between two temporal intervals\n\n    \"\"\"\n    start1, end1 = interval1\n    start2, end2 = interval2\n\n    if end1 &lt; start2:\n        return \"before\"  # interval1 is before interval2\n    elif start1 &gt; end2:\n        return \"after\"  # interval1 is after interval2\n    elif end1 == start2:  # interval1 meets interval2\n        return \"meets\"\n    elif start1 == end2:  # interval1 is met by interval2\n        return \"met_by\"\n    elif start1 &lt; start2 &lt; end1 &lt; end2:  # interval1 overlaps interval2\n        return \"overlaps\"\n    elif start2 &lt; start1 &lt; end2 &lt; end1:  # interval1 is overlapped by interval2\n        return \"overlapped_by\"\n    elif start1 == start2 and end1 &lt; end2:  # interval1 starts interval2\n        return \"starts\"\n    elif start1 == start2 and end1 &gt; end2:  # interval1 is started by interval2\n        return \"started_by\"\n    elif start2 &lt; start1 &lt; end1 &lt; end2:  # interval1 is during interval2\n        return \"during\"\n    elif start1 &lt; start2 &lt; end2 &lt; end1:  # interval1 contains interval2\n        return \"contains\"\n    elif start1 &lt; start2 &lt; end2 == end1:  # interval1 finishes interval2\n        return \"finishes\"\n    elif start2 &lt; start1 &lt; end1 == end2:  # interval1 is finished by interval2\n        return \"finished_by\"\n    elif start1 == start2 and end1 == end2:  # interval1 equals interval2\n        return \"equals\"\n</code></pre>"},{"location":"Code/simulation/allen_temporal_simulator/#TimelineKGQA.simulation.allen_temporal_simulator.AllenTemporalSimulator.determine_relation_timepointrange","title":"<code>determine_relation_timepointrange(point_1, time_range_2)</code>","text":"<p>Point 1 will be a time point, and point 2 will be a time range So there will be 5 relationships: before, starts, during, finishes, after</p> Source code in <code>TimelineKGQA/simulation/allen_temporal_simulator.py</code> <pre><code>def determine_relation_timepointrange(self, point_1, time_range_2):\n    \"\"\"\n    Point 1 will be a time point, and point 2 will be a time range\n    So there will be 5 relationships: before, starts, during, finishes, after\n\n    \"\"\"\n    start1, end1 = point_1  # point1 is a time point, so start1 = end1\n    start2, end2 = time_range_2\n\n    if start1 &lt; start2:\n        return \"before\"  # point1 is before range2\n    elif start1 == start2:\n        return \"starts\"  # point1 starts range2\n    elif start1 &gt; start2 and start1 &lt; end2:\n        return \"during\"\n    elif start1 == end2:\n        return \"finishes\"\n    elif start1 &gt; start2:\n        return \"after\"\n</code></pre>"},{"location":"Code/simulation/allen_temporal_simulator/#TimelineKGQA.simulation.allen_temporal_simulator.AllenTemporalSimulator.determine_relation_timerangepoint","title":"<code>determine_relation_timerangepoint(point1, point2)</code>","text":"<p>Point 1 will be a time range, and point 2 will be a time point So there will be 5 relationships: before, meets, contain, finished_by, after</p> Source code in <code>TimelineKGQA/simulation/allen_temporal_simulator.py</code> <pre><code>def determine_relation_timerangepoint(self, point1, point2):\n    \"\"\"\n    Point 1 will be a time range, and point 2 will be a time point\n    So there will be 5 relationships: before, meets, contain, finished_by, after\n    \"\"\"\n\n    start1, end1 = point1\n    start2, end2 = point2  # point2 is a time point, so end2 = start2\n\n    if end1 &lt; start2:\n        return \"before\"  # range1 is before point2\n    elif end1 == start2:\n        return \"finished_by\"  # range 1 meets point2\n    elif start1 &lt; start2 and end1 &gt; start2:\n        return \"contains\"  # range 1 contains point2\n    elif start1 == start2:\n        return \"met_by\"  # range 1 finished_by point2\n    elif start1 &gt; start2:\n        return \"after\"  # range 1 is after point2\n</code></pre>"},{"location":"Code/simulation/allen_temporal_simulator/#TimelineKGQA.simulation.allen_temporal_simulator.AllenTemporalSimulator.generate_surface","title":"<code>generate_surface()</code>","text":"<p>This is the 3D surface that represents all information temporal information</p> Source code in <code>TimelineKGQA/simulation/allen_temporal_simulator.py</code> <pre><code>def generate_surface(self):\n    \"\"\"\n    This is the 3D surface that represents all information temporal information\n    \"\"\"\n    x = np.linspace(0, self.max_time, 100)\n    y = np.linspace(0, self.max_time, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.maximum(Y - X, 0)  # Z starts from 0 at the origin and increases\n    # remove all points where X &gt; Y\n    Z[X &gt; Y] = np.nan\n    return X, Y, Z\n</code></pre>"},{"location":"Code/simulation/allen_temporal_simulator/#TimelineKGQA.simulation.allen_temporal_simulator.AllenTemporalSimulator.visualize","title":"<code>visualize(center_point=None)</code>","text":"<p>First generate the 3D surface for the visualization</p> Source code in <code>TimelineKGQA/simulation/allen_temporal_simulator.py</code> <pre><code>def visualize(self, center_point=None):\n    \"\"\"\n    First generate the 3D surface for the visualization\n    \"\"\"\n    X, Y, Z = self.generate_surface()\n\n    fig = go.Figure()\n\n    if self.mode == \"surface\":\n        # show the surface track\n        fig.add_trace(\n            go.Surface(\n                x=X,\n                y=Y,\n                z=Z,\n                # give it the grey color\n                colorscale=[[0, \"rgb(220,220,220)\"], [1, \"rgb(220,220,220)\"]],\n                showscale=False,\n                opacity=0.5,\n                showlegend=False,\n            )\n        )\n\n    traces = {\n        relation: go.Scatter3d(\n            x=[],\n            y=[],\n            z=[],\n            mode=\"markers\",\n            marker=dict(size=5, color=color, symbol=shape),\n            name=relation,\n        )\n        for relation, (color, shape) in self.color_shape_map.items()\n    }\n\n    \"\"\"\n    Here is for relationship between temporal intervals\n    \"\"\"\n    if self.mode == \"timeranges\":\n        # define a center point\n        if not center_point:\n            center_point = (25, 75)\n        start, end = center_point\n        fig.add_trace(\n            go.Scatter3d(\n                x=[start],\n                y=[end],\n                z=[end - start],\n                mode=\"markers\",\n                marker=dict(size=20, color=\"black\", symbol=\"cross\"),\n                showlegend=False,\n            )\n        )\n\n        nearby_points = self.generate_nearby_time_ranges(center_point)\n        for nearby_point in nearby_points:\n            n_start, n_end = nearby_point\n            relation = self.determine_relation(center_point, nearby_point)\n            traces[relation].x = traces[relation].x + (n_start,)\n            traces[relation].y = traces[relation].y + (n_end,)\n            traces[relation].z = traces[relation].z + (n_end - n_start,)\n\n        # Add all traces to the figure\n        for trace in traces.values():\n            fig.add_trace(trace)\n\n    \"\"\"\n    Add a time point to time point relationship\n\n    Which means the z is 0, x = y, and only three relationships: before, equals, after\n    \"\"\"\n\n    if self.mode == \"timepoints\":\n\n        # define a center point for timepoint\n        if not center_point:\n            center_point = (50, 50)\n        else:\n            assert (\n                len(center_point) == 2\n            ), \"Center point must be a tuple of 2 values\"\n            assert (\n                center_point[0] == center_point[1]\n            ), \"Center point must be a time point\"\n        start, end = center_point\n        fig.add_trace(\n            go.Scatter3d(\n                x=[start],\n                y=[end],\n                z=[0],\n                mode=\"markers\",\n                marker=dict(size=20, color=\"black\", symbol=\"cross\"),\n                showlegend=False,\n            )\n        )\n\n        nearby_points = self.generate_nearby_timepoints(center_point)\n\n        for nearby_point in nearby_points:\n            n_start, n_end = nearby_point\n            relation = self.determine_relation_timepoint(center_point, nearby_point)\n            traces[relation].x = traces[relation].x + (n_start,)\n            traces[relation].y = traces[relation].y + (n_end,)\n            traces[relation].z = traces[relation].z + (0,)\n        # Add all traces to the figure\n        for trace in traces.values():\n            fig.add_trace(trace)\n\n    if self.mode == \"timepointrange\":\n        if not center_point:\n            center_point = (50, 50)\n        else:\n            assert (\n                len(center_point) == 2\n            ), \"Center point must be a tuple of 2 values\"\n            assert (\n                center_point[0] != center_point[1]\n            ), \"Center point must be a time range\"\n        start, end = center_point\n        fig.add_trace(\n            go.Scatter3d(\n                x=[start],\n                y=[end],\n                z=[0],\n                mode=\"markers\",\n                marker=dict(size=20, color=\"black\", symbol=\"cross\"),\n                showlegend=False,\n            )\n        )\n\n        nearby_points = self.generate_nearby_time_ranges(center_point)\n        for nearby_point in nearby_points:\n            n_start, n_end = nearby_point\n            relation = self.determine_relation_timepointrange(\n                center_point, nearby_point\n            )\n            if relation is None:\n                continue\n            traces[relation].x = traces[relation].x + (n_start,)\n            traces[relation].y = traces[relation].y + (n_end,)\n            traces[relation].z = traces[relation].z + (n_end - n_start,)\n        # Add all traces to the figure\n        for trace in traces.values():\n            fig.add_trace(trace)\n\n    if self.mode == \"timerangepoint\":\n        if not center_point:\n            center_point = (25, 75)\n        else:\n            assert (\n                len(center_point) == 2\n            ), \"Center point must be a tuple of 2 values\"\n            assert (\n                center_point[0] != center_point[1]\n            ), \"Center point must be a time range\"\n        start, end = center_point\n        fig.add_trace(\n            go.Scatter3d(\n                x=[start],\n                y=[end],\n                z=[end - start],\n                mode=\"markers\",\n                marker=dict(size=20, color=\"black\", symbol=\"cross\"),\n                showlegend=False,\n            )\n        )\n\n        nearby_points = self.generate_nearby_timepoints_for_timerange(center_point)\n        for nearby_point in nearby_points:\n            n_start, n_end = nearby_point\n            relation = self.determine_relation_timerangepoint(\n                center_point, nearby_point\n            )\n            if relation is None:\n                continue\n            traces[relation].x = traces[relation].x + (n_start,)\n            traces[relation].y = traces[relation].y + (n_end,)\n            traces[relation].z = traces[relation].z + (0,)\n        # Add all traces to the figure\n        for trace in traces.values():\n            fig.add_trace(trace)\n\n    # Update layout\n    fig.update_layout(\n        scene=dict(\n            xaxis_title=\"Start Time\",\n            yaxis_title=\"End Time\",\n            zaxis_title=\"Duration\",\n            xaxis=dict(range=[self.max_time, 0], autorange=\"reversed\"),\n            yaxis=dict(range=[self.max_time, 0], autorange=\"reversed\"),\n            zaxis=dict(range=[0, self.max_time * 2]),\n            aspectmode=\"cube\",\n            aspectratio=dict(x=1, y=1, z=1),\n        ),\n        title=f\"Allen's 13 Temporal Relations Visualization for {self.mode}\",\n        legend_title=\"Temporal Relations\",\n        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01),\n    )\n\n    # Show plot\n    fig.show()\n    # to html\n    fig.write_html(self.output_dir / f\"{self.mode}_allen_temporal_relations.html\")\n</code></pre>"}]}